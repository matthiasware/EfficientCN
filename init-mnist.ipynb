{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07aa17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "#\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ac9c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = datasets.MNIST(root='./data', train=True, download=True, transform=T.ToTensor())\n",
    "ds_valid = datasets.MNIST(root=\"./data\", train=False, download=True, transform=T.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9449851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ds_train.data[0], cmap='gray')\n",
    "plt.title('%i' % ds_train.targets[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50988be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = torch.utils.data.DataLoader(ds_train, \n",
    "                                          batch_size=256, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=4)\n",
    "dl_valid = torch.utils.data.DataLoader(ds_valid, \n",
    "                                          batch_size=16, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a9dfd",
   "metadata": {},
   "source": [
    "# Baseline CNN for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c90ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,                  \n",
    "            ),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)       \n",
    "        output = self.out(x)\n",
    "        return output    # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277cc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cdf256",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e005f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for idx, (x, y_true) in enumerate(dl_train):\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y_true)\n",
    "\n",
    "        # clear gradients for this training step   \n",
    "        optimizer.zero_grad()           \n",
    "            \n",
    "        # backpropagation, compute gradients \n",
    "        loss.backward()    \n",
    "        # apply gradients             \n",
    "        optimizer.step()\n",
    "        \n",
    "        if idx % 1000 == 0:\n",
    "            print(\"Epoch[{}/{}] - step {} loss: {:.4f}\".format(epoch, num_epochs, idx, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y_true in dl_valid:\n",
    "        y_pred = model(x)\n",
    "        y_pred = torch.max(y_pred, 1)[1]\n",
    "        correct += (y_pred == y_true).sum().item()\n",
    "        total += y_true.shape[0]\n",
    "    acc = correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33819d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc)\n",
    "print(total - correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c175ff9",
   "metadata": {},
   "source": [
    "# Eff-Caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb078de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    \"\"\"\n",
    "        Create a primary capsule layer with the methodology described in 'Efficient-CapsNet: Capsule Network with Self-Attention Routing'. \n",
    "        Properties of each capsule s_n are exatracted using a 2D depthwise convolution.\n",
    "\n",
    "        ...\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        F: int depthwise conv number of features\n",
    "        K: int depthwise conv kernel dimension \n",
    "        N: int number of primary capsules\n",
    "        D: int primary capsules dimension (number of properties)\n",
    "        s: int depthwise conv strides\n",
    "    \"\"\"\n",
    "    def __init__(self, F, K, N, D, s=1):\n",
    "        super().__init__()\n",
    "        self.F = F\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "        self.D = D\n",
    "        self.s = s\n",
    "        #\n",
    "        self.dw_conv2d = nn.Conv2d(F, F, kernel_size=K, stride=s, groups=F, padding=\"valid\")\n",
    "        #\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "         X in (B,C,H,W) = (B,F,K,K)\n",
    "         -> (B, N, D)\n",
    "        \"\"\"\n",
    "        # (B,C,H,W) -> (B,C,H,W)\n",
    "        x = self.dw_conv2d(x)\n",
    "\n",
    "        # (B,C,H,W) -> (B, N, D)\n",
    "        x = x.view((-1, self.N, self.D))\n",
    "        \n",
    "        #\n",
    "        return x\n",
    "\n",
    "class Squash(nn.Module):\n",
    "    def __init__(self, eps=10e-21):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "         in:  (b, n, d)\n",
    "         out: (b, n, d)\n",
    "        \"\"\"\n",
    "        xn = torch.norm(x, dim=2, keepdim=True)\n",
    "        return (1 - 1/(torch.exp(xn) + self.eps)) * (x / (xn + self.eps))\n",
    "\n",
    "class FCCaps(nn.Module):\n",
    "    def __init__(self, n_l, n_h, d_l, d_h):\n",
    "        super().__init__()\n",
    "        self.n_l = n_l\n",
    "        self.d_l = d_l\n",
    "        self.n_h = n_h\n",
    "        self.d_h = d_h\n",
    "        #\n",
    "        self.W = torch.nn.Parameter(torch.rand(n_l, n_h, d_l, d_h), requires_grad=True)\n",
    "        self.B = torch.nn.Parameter(torch.rand(n_l, n_h), requires_grad=True)\n",
    "        self.squash = Squash()\n",
    "\n",
    "    def forward(self, U_l):\n",
    "        \"\"\"\n",
    "        einsum convenventions:\n",
    "          n_l = i | h\n",
    "          d_l = j\n",
    "          n_h = k\n",
    "          d_h = l\n",
    "        \n",
    "        Data tensors:\n",
    "            IN:  U_l\n",
    "            OUT: U_h\n",
    "            DIMS: \n",
    "                U_l (n_l, d_l)\n",
    "                U_h (n_h, d_h)\n",
    "                W   (n_l, n_h, d_l, d_h)\n",
    "                B   (n_l, n_h)\n",
    "                A   (n_l, n_l, n_h)\n",
    "                C   (n_l, n_h)\n",
    "        \"\"\"\n",
    "        U_hat = torch.einsum('...ij,ikjl->...ikl', U_l, self.W)\n",
    "        A = torch.einsum(\"...ikl, ...hkl -> ...hik\", U_hat, U_hat)\n",
    "        A = A / torch.sqrt(torch.Tensor([d_l]))\n",
    "        A_sum = torch.einsum(\"...hij->...hj\",A)\n",
    "        C = torch.softmax(A_sum,dim=-1)\n",
    "        CB = C + self.B\n",
    "        U_h = torch.einsum('...ikl,...ik->...kl', U_hat, CB)\n",
    "        return self.squash(U_h)\n",
    "\n",
    "\n",
    "class EfficientCapsNets(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def call(self, x):\n",
    "        pass\n",
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d90430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Part\n",
    "# add he normal initializer\n",
    "cn = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, kernel_size=(5, 5), padding=\"valid\"),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.Conv2d(32, 64, kernel_size=(3, 3), padding=\"valid\"),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), padding=\"valid\"),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.Conv2d(64, 128, kernel_size=(3, 3), stride=2, padding=\"valid\"),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm2d(128),\n",
    ")\n",
    "x_h = cn(x)\n",
    "print(x_h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58e90da",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_l = 16\n",
    "d_l = 8\n",
    "#\n",
    "n_h = 10\n",
    "d_h = 16\n",
    "#\n",
    "pc = PrimaryCaps(F=128, K=9, N=n_l, D=d_l)\n",
    "#\n",
    "U_0 = pc(x_h)\n",
    "print(U_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04871754",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn = FCCaps(n_l, n_h, d_l, d_h)\n",
    "#\n",
    "U_1 = fcn(U_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276c0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_flat = torch.flatten(U_1, start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = nn.Sequential(\n",
    "    nn.Linear(16*10, 512),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(512, 1024),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(1024, 28*28),\n",
    "    nn.Sigmoid(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd30330",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rec = generator(U_flat)\n",
    "x_rec = x_rec.view((-1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5723cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_view = x_rec.reshape((-1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26f2475",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_view.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c690881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_view[5].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6077ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c80e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, x_rec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.mse_loss(x, x_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a926a8",
   "metadata": {},
   "source": [
    "# TensorFlow Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4274cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Length(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of each capsule n of a layer l.\n",
    "    ...\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    call(inputs)\n",
    "        compute the length of each capsule\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Compute the length of each capsule\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs: tensor\n",
    "           tensor with shape [None, num_capsules (N), dim_capsules (D)]\n",
    "        \"\"\"\n",
    "        return tf.sqrt(tf.reduce_sum(tf.square(inputs), - 1) + tf.keras.backend.epsilon())\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Length, self).get_config()\n",
    "        return config\n",
    "\n",
    "class Mask(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Mask operation described in 'Dynamic routinig between capsules'.\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    call(inputs, double_mask)\n",
    "        mask a capsule layer\n",
    "        set double_mask for multimnist dataset\n",
    "    \"\"\"\n",
    "    def call(self, inputs, double_mask=None, **kwargs):\n",
    "        if type(inputs) is list:\n",
    "            if double_mask:\n",
    "                inputs, mask1, mask2 = inputs\n",
    "            else:\n",
    "                inputs, mask = inputs\n",
    "        else:  \n",
    "            x = tf.sqrt(tf.reduce_sum(tf.square(inputs), -1))\n",
    "            if double_mask:\n",
    "                mask1 = tf.keras.backend.one_hot(tf.argsort(x,direction='DESCENDING',axis=-1)[...,0],num_classes=x.get_shape().as_list()[1])\n",
    "                mask2 = tf.keras.backend.one_hot(tf.argsort(x,direction='DESCENDING',axis=-1)[...,1],num_classes=x.get_shape().as_list()[1])\n",
    "            else:\n",
    "                mask = tf.keras.backend.one_hot(indices=tf.argmax(x, 1), num_classes=x.get_shape().as_list()[1])\n",
    "\n",
    "        if double_mask:\n",
    "            masked1 = tf.keras.backend.batch_flatten(inputs * tf.expand_dims(mask1, -1))\n",
    "            masked2 = tf.keras.backend.batch_flatten(inputs * tf.expand_dims(mask2, -1))\n",
    "            return masked1, masked2\n",
    "        else:\n",
    "            masked = tf.keras.backend.batch_flatten(inputs * tf.expand_dims(mask, -1))\n",
    "            return masked\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:  \n",
    "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
    "        else:  # generation step\n",
    "            return tuple([None, input_shape[1] * input_shape[2]])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Mask, self).get_config()\n",
    "        return config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
