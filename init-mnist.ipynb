{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07aa17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "import torch.nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ac9c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = datasets.MNIST(root='./data', train=True, download=True, transform=T.ToTensor())\n",
    "ds_valid = datasets.MNIST(root=\"./data\", train=False, download=True, transform=T.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9449851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ds_train.data[0], cmap='gray')\n",
    "plt.title('%i' % ds_train.targets[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50988be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = torch.utils.data.DataLoader(ds_train, \n",
    "                                          batch_size=256, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=4)\n",
    "dl_valid = torch.utils.data.DataLoader(ds_valid, \n",
    "                                          batch_size=16, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a9dfd",
   "metadata": {},
   "source": [
    "# Baseline CNN for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c90ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,                  \n",
    "            ),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)       \n",
    "        output = self.out(x)\n",
    "        return output    # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277cc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cdf256",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e005f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for idx, (x, y_true) in enumerate(dl_train):\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y_true)\n",
    "\n",
    "        # clear gradients for this training step   \n",
    "        optimizer.zero_grad()           \n",
    "            \n",
    "        # backpropagation, compute gradients \n",
    "        loss.backward()    \n",
    "        # apply gradients             \n",
    "        optimizer.step()\n",
    "        \n",
    "        if idx % 1000 == 0:\n",
    "            print(\"Epoch[{}/{}] - step {} loss: {:.4f}\".format(epoch, num_epochs, idx, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y_true in dl_valid:\n",
    "        y_pred = model(x)\n",
    "        y_pred = torch.max(y_pred, 1)[1]\n",
    "        correct += (y_pred == y_true).sum().item()\n",
    "        total += y_true.shape[0]\n",
    "    acc = correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33819d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc)\n",
    "print(total - correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c175ff9",
   "metadata": {},
   "source": [
    "# Eff-Caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb078de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    \"\"\"\n",
    "        Create a primary capsule layer with the methodology described in 'Efficient-CapsNet: Capsule Network with Self-Attention Routing'. \n",
    "        Properties of each capsule s_n are exatracted using a 2D depthwise convolution.\n",
    "\n",
    "        ...\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        F: int depthwise conv number of features\n",
    "        K: int depthwise conv kernel dimension \n",
    "        N: int number of primary capsules\n",
    "        D: int primary capsules dimension (number of properties)\n",
    "        s: int depthwise conv strides\n",
    "    \"\"\"\n",
    "    def __init__(self, F, K, N, D, s=1):\n",
    "        super().__init__()\n",
    "        self.F = F\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "        self.D = D\n",
    "        self.s = s\n",
    "        #\n",
    "        self.dw_conv2d = nn.Conv2d(F, F, kernel_size=K, stride=s, groups=F, padding=\"valid\")\n",
    "        #\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "         X in (B,C,H,W) = (B,F,K,K)\n",
    "         -> (B, N, D)\n",
    "        \"\"\"\n",
    "        # (B,C,H,W) -> (B,C,H,W)\n",
    "        x = self.dw_conv2d(x)\n",
    "\n",
    "        # (B,C,H,W) -> (B, N, D)\n",
    "        x = x.view((-1, self.N, self.D))\n",
    "        \n",
    "        #\n",
    "        return x\n",
    "\n",
    "class Squash(nn.Module):\n",
    "    def __init__(self, eps=10e-21):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class FCCaps(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forware(self, x):\n",
    "        raise NoteImplementedError()\n",
    "\n",
    "class EfficientCapsNets(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def call(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d90430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Part\n",
    "# add he normal initializer\n",
    "cn = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, kernel_size=(5, 5), padding=\"valid\"),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.Conv2d(32, 64, kernel_size=(3, 3), padding=\"valid\"),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), padding=\"valid\"),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.Conv2d(64, 128, kernel_size=(3, 3), stride=2, padding=\"valid\"),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm2d(128),\n",
    ")\n",
    "x_h = cn(x)\n",
    "print(x_h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18307385",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = PrimaryCaps(F=128, K=9, N=16, D=8)\n",
    "#\n",
    "z = pc(x_h)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63828ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.view((-1, 16, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b02479",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.view((-1, 16, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d = 4, 3\n",
    "eps = 10e-21\n",
    "s = torch.rand((n, d))\n",
    "s_norm = torch.linalg.vector_norm(s, dim=1)\n",
    "#\n",
    "print(s.shape)\n",
    "print(s_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a926a8",
   "metadata": {},
   "source": [
    "# TensorFlow Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a148f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squash(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Squash activation used in 'Efficient-CapsNet: Capsule Network with Self-Attention Routing'.\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    eps: int\n",
    "        fuzz factor used in numeric expression\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    call(s)\n",
    "        compute the activation from input capsules\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eps=10e-21, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "\n",
    "    def call(self, s):\n",
    "        n = tf.norm(s,axis=-1,keepdims=True)\n",
    "        return (1 - 1/(tf.math.exp(n)+self.eps))*(s/(n+self.eps))\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config}\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512c15ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Create a primary capsule layer with the methodology described in 'Efficient-CapsNet: Capsule Network with Self-Attention Routing'. \n",
    "    Properties of each capsule s_n are exatracted using a 2D depthwise convolution.\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    F: int\n",
    "        depthwise conv number of features\n",
    "    K: int\n",
    "        depthwise conv kernel dimension\n",
    "    N: int\n",
    "        number of primary capsules\n",
    "    D: int\n",
    "        primary capsules dimension (number of properties)\n",
    "    s: int\n",
    "        depthwise conv strides\n",
    "    Methods\n",
    "    -------\n",
    "    call(inputs)\n",
    "        compute the primary capsule layer\n",
    "    \"\"\"\n",
    "    def __init__(self, F, K, N, D, s=1, **kwargs):\n",
    "        super(PrimaryCaps, self).__init__(**kwargs)\n",
    "        self.F = F\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "        self.D = D\n",
    "        self.s = s\n",
    "        \n",
    "    def build(self, input_shape):    \n",
    "        self.DW_Conv2D = tf.keras.layers.Conv2D(self.F, self.K, self.s,\n",
    "                                             activation='linear', groups=self.F, padding='valid')\n",
    "\n",
    "        self.built = True\n",
    "    \n",
    "    def call(self, inputs):      \n",
    "        x = self.DW_Conv2D(inputs)      \n",
    "        x = tf.keras.layers.Reshape((self.N, self.D))(x)\n",
    "        x = Squash()(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4274cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCCaps(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Fully-connected caps layer. It exploites the routing mechanism, explained in 'Efficient-CapsNet: Capsule Network with Self-Attention Routing', \n",
    "    to create a parent layer of capsules. \n",
    "    \n",
    "    ...\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    N: int\n",
    "        number of primary capsules\n",
    "    D: int\n",
    "        primary capsules dimension (number of properties)\n",
    "    kernel_initilizer: str\n",
    "        matrix W initialization strategy\n",
    " \n",
    "    Methods\n",
    "    -------\n",
    "    call(inputs)\n",
    "        compute the primary capsule layer\n",
    "    \"\"\"\n",
    "    def __init__(self, N, D, kernel_initializer='he_normal', **kwargs):\n",
    "        super(FCCaps, self).__init__(**kwargs)\n",
    "        self.N = N\n",
    "        self.D = D\n",
    "        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        input_N = input_shape[-2]\n",
    "        input_D = input_shape[-1]\n",
    "\n",
    "        self.W = self.add_weight(shape=[self.N, input_N, input_D, self.D],initializer=self.kernel_initializer,name='W')\n",
    "        self.b = self.add_weight(shape=[self.N, input_N,1], initializer=tf.zeros_initializer(), name='b')\n",
    "        self.built = True\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        \n",
    "        # j=n\n",
    "        # i=d\n",
    "        # k=n\n",
    "        # z=d\n",
    "        # inputs(n, d)\n",
    "        # W(n,n,d,d)\n",
    "        # u(n,n,d)\n",
    "        u = tf.einsum('...ji,kjiz->...kjz',inputs,self.W)         # u shape=(None,N,H*W*input_N,D)\n",
    "             \n",
    "        c = tf.einsum('...ij,...kj->...i', u, u)[...,None]        # b shape=(None,N,H*W*input_N,1) -> (None,j,i,1)\n",
    "        c = c/tf.sqrt(tf.cast(self.D, tf.float32))\n",
    "        c = tf.nn.softmax(c, axis=1)                              # c shape=(None,N,H*W*input_N,1) -> (None,j,i,1)\n",
    "        c = c + self.b\n",
    "        s = tf.reduce_sum(tf.multiply(u, c),axis=-2)             # s shape=(None,N,D)\n",
    "        v = Squash()(s)       # v shape=(None,N,D)\n",
    "        \n",
    "        return v\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.C, self.L)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'N': self.N,\n",
    "            'D': self.D\n",
    "        }\n",
    "        base_config = super(FCCaps, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "\n",
    "class Length(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of each capsule n of a layer l.\n",
    "    ...\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    call(inputs)\n",
    "        compute the length of each capsule\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Compute the length of each capsule\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs: tensor\n",
    "           tensor with shape [None, num_capsules (N), dim_capsules (D)]\n",
    "        \"\"\"\n",
    "        return tf.sqrt(tf.reduce_sum(tf.square(inputs), - 1) + tf.keras.backend.epsilon())\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Length, self).get_config()\n",
    "        return config\n",
    "\n",
    "class Mask(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Mask operation described in 'Dynamic routinig between capsules'.\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    call(inputs, double_mask)\n",
    "        mask a capsule layer\n",
    "        set double_mask for multimnist dataset\n",
    "    \"\"\"\n",
    "    def call(self, inputs, double_mask=None, **kwargs):\n",
    "        if type(inputs) is list:\n",
    "            if double_mask:\n",
    "                inputs, mask1, mask2 = inputs\n",
    "            else:\n",
    "                inputs, mask = inputs\n",
    "        else:  \n",
    "            x = tf.sqrt(tf.reduce_sum(tf.square(inputs), -1))\n",
    "            if double_mask:\n",
    "                mask1 = tf.keras.backend.one_hot(tf.argsort(x,direction='DESCENDING',axis=-1)[...,0],num_classes=x.get_shape().as_list()[1])\n",
    "                mask2 = tf.keras.backend.one_hot(tf.argsort(x,direction='DESCENDING',axis=-1)[...,1],num_classes=x.get_shape().as_list()[1])\n",
    "            else:\n",
    "                mask = tf.keras.backend.one_hot(indices=tf.argmax(x, 1), num_classes=x.get_shape().as_list()[1])\n",
    "\n",
    "        if double_mask:\n",
    "            masked1 = tf.keras.backend.batch_flatten(inputs * tf.expand_dims(mask1, -1))\n",
    "            masked2 = tf.keras.backend.batch_flatten(inputs * tf.expand_dims(mask2, -1))\n",
    "            return masked1, masked2\n",
    "        else:\n",
    "            masked = tf.keras.backend.batch_flatten(inputs * tf.expand_dims(mask, -1))\n",
    "            return masked\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:  \n",
    "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
    "        else:  # generation step\n",
    "            return tuple([None, input_shape[1] * input_shape[2]])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Mask, self).get_config()\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d251f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
