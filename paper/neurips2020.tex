\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
     \usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Title in Progress}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
  Yes we will do this.
\end{abstract}

\section{Introduction}

Since their first applicable appearance in \cite{DBLP:conf/nips/SabourFH17}, CapsNets (CNs) were highly praised for overcoming many shortcomings of traditional Convolutional Neural Networks (CNNs). Interestingly, and although CNs have gained popularity in a small community of devoting Adapts that do not fail to preach the imminent takeover of CNs, real world practitioner tend to stick to traditional CNN architectures or the newer Transformer based architectures in their applications. This might be explained with the fact, that so far CNs do not scale up to large scale tasks like ImageNet classification.

\subsection{Promises of CNs}
Following properties of CNs can be found in literature
\begin{itemize}
	\item Viewpoint invariance \cite{DBLP:conf/icann/HintonKW11}
	\item The dynamic routing extracs more menaingfull features compared to CNNs \cite{DBLP:conf/nips/SabourFH17}
	\item CNs require less (exponentially less) data compared to their CNN counterparts \cite{DBLP:conf/nips/SabourFH17} \cite{DBLP:conf/miccai/Jimenez-Sanchez18}
	\item CNs learn a part-whole relationship (distributed representation) \cite{DBLP:conf/nips/SabourFH17} and offer a dynamic parse tree \cite{DBLP:journals/corr/Peer2018}
	\item CN features offer better "transferability" than the standard "CNN". \cite{DBLP:journals/corr/Ancheng2018}
	\item CNs provide interpretable and equivariant representations \cite{DBLP:conf/nips/LenssenFL18}
	\item CNs classify small data sets without data augmentation [see survey paper]
	\item CNs are robust to white box adversarial attacks \cite{DBLP:conf/iclr/HintonSF18}
	\item CNs are more robust to class imbalances \cite{DBLP:conf/miccai/Jimenez-Sanchez18}
	\item Object capsule presences are highly informative of the object class \cite{DBLP:conf/nips/KosiorekSTH19}  \cite{DBLP:journals/corr/Arjun2020}
	\item CNs allow Geometric Reasoning \cite{DBLP:conf/nips/KosiorekSTH19}
	\item CNs extrapolate better to affine transformations \cite{cvpr/GuT20}
	\item CNs allow more meaningful embeddings and output capsules are less coupled compared to neurons in standard CNNs \cite{corr/Lin2018}
\end{itemize}
In short, CNs promise to be robust to affine transformations and viewpoint variance and in result offer better generalization capabilities with less parameters and require less data with less augmentation than a "comparable" CNN.

\subsection{Inconsistencies with CNS}
On the other hand, CN failed to take off in real applications, which can be attributed to the following observations:
When reading through literature, several inconsistencies have been found:
\begin{itemize}
	\item Routing is computationally expensive, although this has been addressed in various publications. This slows down training up to non-feasibility.
	\item CNs fail on complex datasets with high iter and intra-class variance and background clutter like like CIFAR10, STL10, ImageNet.
	\item So far they have been parameter intensive, although like the routing problem, this has been addressed in different ways.
	\item In serveral articles, the routing has been identified as crucial component in CNs, whereas in others it could be discarded without loss or even gain in performance.
	\item The same goes for the reconstruction loss, although this was kind of demystified in \cite{DBLP:conf/cvpr/GuT021} for the original CN.
	\item There is no precise definition of a CN, as there are many different implementations and variations with different goals in mind.
	\item Many implementations use awful baselines for bench-marking their methods. Furthermore there is no systematic standard to test a  proposed method.
	\item Many papers miss ablation studies, extensive experiments or proofs!
	\item Many publications seem to "improve" upon a given standard CN by changing the architecture or the training procedure, yet often it remains unclear if all the original promises still hold as they always seem to be taken as granted. This is huge problem. 
	\item It was shown that the original CN implementation is not in general superior to CNNs when it comes to robustness to affine transformations, the segmetation of digits and semantic compactness. It came out, that thse properties arise from the individual building blocks of CNN and can be incorporated in CNNs \cite{DBLP:conf/cvpr/GuT021}. However this study was conducted on the original CN. Modern implementations of CN have not been considered.
\end{itemize}
\subsection{Interesting Developments}
\begin{itemize}
	\item \cite{icann/HintonKW11} Hinton paper 1
	\item \cite{nips/SabourFH17} Hinton paper 2
	\item \cite{iclr/HintonSF18} Hinton paper 3
	\item \cite{nips/KosiorekSTH19} Hinton paper 4
	\item In \cite{DBLP:conf/iclr/VenkataramanBS20} it was shown how to get provable equivariance guarantees.
	\item The authors in \cite{DBLP:journals/corr/Nair2021} were able to show, that the original capsnet \cite{DBLP:conf/nips/SabourFH17} was superior on MNIST like datasets but simply failed on CIFAR10 and SVHN as the feature embeddings simply encoded color and intensities rather than geometric information. They concluded that CNs are highly susceptible to noise.
	\item The work in \cite{DBLP:journals/corr/Arjun2020} came to the conclusion that CNs capture all facets of object classes compactly. The reconstruction played in crucial role with that, whereas the routing was neglectible.
	\item Efficient-CapsNets \cite{DBLP:journals/corr/Mazzia2021} proof that CNs can be implsing Residual Connections to make them deeperemented in an parameter friendly way, but don't demonstrate how to scale to larger datasets.
	\item DenseCaps \cite{DBLP:journals/soco/SunWYX21} aims to make CNs lighter. They introduce Tensor capsules and dense capsules. They failed to reach SOTA but gained performance on CIFAR10. It also fails to address one of the promises of CNs in general.
	\item In \cite{DBLP:conf/cvpr/GuT021} it shown that the original CN implementation is not in general superior to CNNs when it comes to robustness to affine transformations, the segmetation of digits and semantic compactness. It came out, that thse properties arise from the individual building blocks of CNN and can be incorporated in CNNs. However this study was conducted on the original CN. Modern implementations of CN have not been considered.
	\item Transformer Capsules \cite{mobiny2021transcaps} try to scale CapsNets to ImageNet (tiny) and CIFAR10 but fail short to rach SOTA and fail short to define what CpasNets actually are
	\item The authors of \cite{DBLP:conf/icann/GugglbergerPR21} show how to make CNs deeper.
	\item \cite{cvpr/RajasegaranJJJS19} uses a 3D convolution based routing algorithm, skip connections and a class independent decoder. They do not go beyond CIFAR10.
	\item The work in \cite{nips/HahnPK19} introduces Self-Routing and show that dynamic routing might not be required to achieve view-point generalization and harmful when scaled up. Furthermore, in a "fair" comparison they show that CNs are more robust to FGSM attacks than their CNN counterparts.
	\item \cite{aaai/EdrakiRS20} ... not really sure what they do, but it looks cool, as they do semi-supervised classification and generative modeling on complex data, which might be relevant 
	\item G-CapsNet \cite{corr/Chen2018} incorporates the routing procedure into the optimization problem and make it trainable. However the authors remark their lack of knowledge about how to scale CNs
	\item \cite{corr/Zhao2019} introduces a Min-Max normalization instead of the softmax and show improvement on CIFAR10.
	\item The results in \cite{corr/ONeill2018} indicate that CNs converge faster and learn from fewer samples when using a contrastive loss and attributes the scaling issue to the inherently slow routing algorithm.
	\item The work in \cite{cvpr/GuT20} reveals, that the routing algorithm contributes neither to generalization, nor to the affine robustness of CNs. They propose Aff-CapsNets to improve the AffNIST SOTA from 79 to 93 acc. This work should be studied in detail!
	\item \cite{corr/Phong2020} introduces Depthwise Separate Convolutions (like Efficient-CapsNet ???) to reduce parameter count and sensitivity to background noise.
	\item \cite{corr/Fuchs2020} uses a Wasserstein inspired rounting algorithm and show really good performance on CIFAR10 and CIFAR100 compared with other CNs but still lack behind SOTA architectures
	\item \cite{corr/Hoogi2019} propose to feed self attention layers into the capsules layers to suppress background noise and outperformed baseline CNs and CNNs (ResNet e.g.)
	\item \cite{aaai/RibeiroLK20} gives the routing algorithm an Bayesian touch and delivers good results.
	\item There is work on CN-GANS that should deal with the background noise problem. \cite{corr/Marusaki2020}
	\item The work in \cite{corr/Smith2020} explores if CNs are sufficient to create explicit representations of objects and pose. The conclusion was mixed and they suggest to further explore the role of the generator.
	\item \cite{DBLP:conf/nips/LenssenFL18} introduce guaranteed equivariance properties but fail to scale up to complex data.
	\item \cite{acml/PaikKK19} analyzes a series of routing algorithms and found that routing tend to polarize the link strength of a small number of connections while suppressing all other connections which is undesireable when learning whole-part relationships. Furthermore, the routing algorithms produce results that are often worse than random routing.
	\item \cite{corr/Mandal2019} addresses the problem of scalability to large classes by using feture specific capsules rather than class specific capsules. They use exotic datasets. which is crap! They argue that one issue with current CNs is, that more classes generate a more difficult routing problem, as it gets more difficult to find agreement. Therefore they make CNs independent from the number of classes. The baselines are weak.
	\item \cite{corr/Zhao2019} replace the dynamic routing during inference to gain speed. During training nothing changes.
	\item \cite{corr/Ren2019} Introduce a Top-2 Classification challenge. CNs perform bettern than the CNN counterpart. They reduced overall CN paramter by sharing parameters. What is more important. They show how the feature maps can be visualized (ProbAM) to better understand how CNs attend and focus.
	\item \cite{corr/Peer2018} shows that the dynamic routing by agreement algorithm does not ensure an emergence of a parse tree, as the coupling coefficients are close to a uniform distribution. They introduce a new algorithm called dynamic-deep-routing to tackle this issue.
	\item \cite{iclr/TsaiSGS20} Intruduces a new routing method, reduces overall parameters and is on-par with ResNet18 when it comes to CIFAR10/100. This should be read!
	\item StarCaps \cite{nips/AhmedT19} reduces computitional complexity of CNs by utilizing attention modules augmented by a differentiable binary routers. They scale this implementation to ImageNet and achieve 60\% Top-1 acc, surpassing baseline CNs but falling behind CNNs.
	\item \cite{corr/Xi2017} attribute the scaling issue to the 2d reconstruction loss, that would probably not work in 3d data like CIFAR10. Kind of shady conclusion.
	\item \cite{iclr/Wang018} formalize the routing strategy as an optimization problem the minimizes the combination of a clustering loss and the KL between current coupling distribution and its last states. They then simplify the algorithm. Rather theoretical work.
	\item \cite{corr/Anand2020} show how CNs can enforce parse-tree structures while CNNs don't. They show that the entropy of routing
	coefficients in the dynamic routing algorithm controls this ability. Without routing CNs behave like CNNs.
	\item In \cite{corr/Byerly2020} Dont understand, but they kind of use capsules.
	\item In \cite{ijon/ByerlyKD21} the authors argue, that no routing is needed when using  Homogeneous Vector Capsules which use element-wise matrix multiplication. They reduce parameters, training time and the reconstruction loss and establish a new SOTA on MNIST. They also show their performance on CIFAR10/100. Unclear which properties of the original CN still hold.
	\item AR CapsNet \cite{iccvw/ChoiSIK19} replaces dynamic routing and the squashing function with "dynamic attention routing" and "capsule activation", keeping spatial information.
	\item SegCaps \cite{corr/LaLonde2018} use CNs for segmentation tasks on 512x512 images. Compared with UNet they achieve higher acc with less parameters. They introduce transformation-matrix sharing.
	\item In \cite{nips/RibeiroLK20} the authors focus on enhancing CN by replacing local routing with variational inference of part-object connections. Furthermore they consider global context. The authors state that other implementations are not careful when it comes to property-conserving promises and demonstrate that their method performs well on pose-aware tasks!
	\item In \cite{aaai/Gu21}, the authors replace the routing algorithm by a multi-head attention-based Graph Pooling approach in order to get interpret able salience maps. They claim to achieve better classification accuracy then standard CNs while preserving affine robustness and use less parameters.
\end{itemize}
\subsection{Questions with CNs}
\begin{itemize}
	\item What is a CN? There are many different implementations. What are the inherent similarities that define a CN and separate them from other architectures?
	\item Why has nobody scaled CNs up to ImageNet?
	\item Can CNs hold their promises?
	\item Can CNs be used for Self-Supervised learning? Unsupervised learning seems to be solved, given enough data.
	\item Can we somehow overcome this background noise problem?
\end{itemize}
\subsection{Datasets}
The followind datasets are used to show one or more properties of CNs

\begin{tabular}{ll}
	Dataset     & Description / Usage \\
	\hline
	MNIST       & General classification ability, semantic compactness \\
	affNIST     & Train on MNIST and show generalization ability on a affine transformed MNIST verwsion \\
	CMNIST      & Color MNIST to make it more difficult \\
	MultiMNIST  & A MNIST version with multiple digits per image, often overlapping to test the segmentation ability \\
	BMNIST      & MNIST with background crap \\
	SVHN        & not sure why this is used \\
	FMNIST      & Fashion MNIST is slightly harder than MNIST \\
	smallNORB   & To test the robustness to changes in viewpoint \\
	CIFAR10/100 & Harder classification task, Introduces background clutter and lots of intra/inner-class variation \\
	STL10       & Larger imagesize than CIFAR10 with the same goals in mind \\
	TinyImageNet& A 64x64 version img ImageNet \\
	ImageNet    & The endboss of image models \\
	CPSPRITES   & The perfect dataset to test background noise dependencies \\
	GTSRB		& traffic sign dataset \\
\end{tabular}

\subsection{Research Plan}
The term "Capsule Network" is extremely vague, but the general consensuns seems to be
\begin{itemize}
	\item The atomic information unit is based on Vectors | Matrices | Tensors
	\item There is a kind of routing algorithms
	\item There is a kind of transformation module in which the primary capsules are transformed execute votes nonÂ´-shared transformation matrices 
	\item Reconstruction loss
	\item Squashing function
	\item Marginal Loss
\end{itemize}
Furthermore whenever the term capsule network comes up, it is kind of associated with one or more of the promised benefits.
So in the end we want an architecture / method that is quite generic when it comes to image tasks but works with less parameters, less augmentation, is more robust to affine transformation, gives more interpretable or semantically compact features and is robust to adversarial attacks. The baseline should be SOTA in the same parameter regime. \\

It also seems to be that "CapsNets" seem to be superior, whenever spatial information, as well as complex part-whole relationships are important! 

The goals should therefore be:
\begin{itemize}
	\item Optmize CNs or identify the important components and make them scale to larger tasks.
	\item Show that the optimized CN is in some ways superior to a comparable CNN.
	\item answer some of the questions above.
\end{itemize}
So in order to make CNs fly they first have to scale to real world data sets and then they must hold their promises.

\subsection{SOTA in Classification}
Architectures to consider
\begin{itemize}
	\item STAR-CAPS \cite{nips/AhmedT19} 2019, but they don't provide code!
	\item CFR-CapsNet \cite{access/WangLYC21} 2021, CFR-CapsNet, no code -.-
	\item DR \cite{nips/SabourFH17}
	\item ER \cite{iclr/HintonSF18}
	\item InverterdDP \cite{iclr/TsaiSGS20}
	\item Efficient-CapsNets (ECN) \cite{DBLP:journals/corr/Mazzia2021}
	\item HVC \cite{ijon/ByerlyKD21}
\end{itemize}

\begin{tabular}{lll}
	\hline
	\multicolumn{3}{c}{MNIST}\\
	\hline
	Model     & Parameters & Acc \\
	\hline
	Star-Caps &  143k & 99.49 \\
	Star-Caps &  281k & 99.57 \\
	EM & 310K & 99.56 \\
	DR & 6.8M & 99.75 \\
	ECN & 161K & 99.74 \\
	HVC & -   & 99.87 \\
	\hline
\end{tabular}

\begin{tabular}{lll}
	\hline
	\multicolumn{3}{c}{smallNORB}\\
	\hline
	Model     & Parameters & Acc \\
	\hline
	\hline
\end{tabular}

\begin{tabular}{lll}
	\hline
	\multicolumn{3}{c}{CIFAR10}\\
	\hline
	Model     & Parameters & Acc \\
	\hline
	Star-Caps &  80K & 91.32 \\
	CFR-CapsNet & 3.6M & 93.15 \\
	ResNet20 & 270K & 91.25 \\
	ResNet110 & 1.7M & 93.57 \\
	DR & 7.99M & 84.08 \\
	EM & 0.45M  & 82.19 \\
	InvertedDP & 0.45M & 85.17 \\
	DR + ResNetBB & 12.45M & 92.65 \\
	ER + ResNetBB & 1.71M  & 92.15 \\
	InvertedDP + ResNetBB & 1.83M & 95.14 \\
	HVC & - & 89.23 \\
	LaNet (SOTA, 11.2021) & 44.1M & 99.03 \\
	ResNet50 & 25M & 98.31 \\
	\hline
\end{tabular}

\begin{tabular}{lll}
	\hline
	\multicolumn{3}{c}{CIFAR100}\\
	\hline
	Model     & Parameters & Acc \\
	\hline
	Star-Caps &  - & 67.66 \\
	CFR-CapsNet & 5.56M & 71.18 \\
	ResNet38  & - & 68.54 \\
	ResNet110 & - & 71.21 \\
	DR & 31.59M & 56.96 \\
	EM & 0.5M  & 37.73 \\
	InvertedDP & 1.46M & 57.32 \\
	DR + ResNetBB & 36.04 & 71.70 \\
	ER + ResNetBB & 1.76M  & 58.08 \\
	InvertedDP + ResNetBB & 2.8M & 78.02 \\
	HVC & - & 64.15 \\
	E2E-3M (SOTA 11.2021) & - & 90.27 \\
	ResNet50 & 25M & 86.9 \\
	\hline
\end{tabular}

\begin{tabular}{lll}
	\hline
	\multicolumn{3}{c}{STL10}\\
	\hline
	Model     & Parameters & Acc \\
	\hline
	EnAET &  - & 95.48 \\
	\hline
\end{tabular}

\begin{tabular}{llll}
	\hline
	\multicolumn{4}{c}{ImageNet}\\
	\hline
	Model     & Parameters & Top1 & Top5 \\
	\hline
	Star-Caps &  - & 60.07 & 85.66 \\
	VIT-H448 (SOTA 11.2021) & - & 87.8 & - \\
	ViL-Medium-D & 39.7M & 83.3 & - \\
	ResNet50 & - & 83.3 & - \\
	MUXNet-xs & 1.8M & 66.7 & 86.8 \\ 
	
	\hline
\end{tabular}

\begin{tabular}{llll}
	\hline
	\multicolumn{4}{c}{TinyImageNet}\\
	\hline
	Model     & Parameters & Top1 & Top5 \\
	PreActResNet-18-3 & - & 70.24  & - \\
	UPANets & - & 67.67  & - \\
	DenseNet + Residual Networks & - & 60  & - \\
	\hline
	
	
	\hline
\end{tabular}

\subsubsection{MNIST}
\subsubsection{CIFAR10}
\subsubsection{CIFAR100}
  

\section{Experiments on AffNIST}

Data on Howard:
\begin{itemize}
	\item Run on AffNIST \url{/mnt/experiments/effcn/affnist/effcn_affnist_2021_12_05_00_37_49_0x82d}, train-mn: 0.9992, valid-mn: 0.9933, valid-an: 0.8530
\end{itemize}

\section{Issues with EfficientCN}

\begin{itemize}
	\item ECN uses a large CNN backbone which does most of the work
	\item ECN uses a huge Discrimator: 1.3m params on mnist vs 0.3m params for the Capsule part!
	\item ECN uses fully connected attention. Since the atomic information unit is a vector, the Weights is a Tensor of 3. degree, that grows with the number of classes!
\end{itemize}

\section{Issues with CNs in General}

\begin{itemize}
	\item CNs pre-allocated capsules to a discrete set of possible parts. The more objects, the more capsules are required! Can be solved by universal capsules but this breaks Symmetry assumption and Routing will become impossible
	\item ...
\end{itemize}

\section{Issues with Convolutions}
The good:
\begin{itemize}
	\item strong bias for visual input
	\item spatial agnostic
	\item parameter efficient via weight sharing
	\item translation invariance
\end{itemize}

The bad:
\begin{itemize}
	\item Hard to capture long range relationshipts other than stacking them up
	\item small receptive field
	\item Fixed kernels after training!
	\item Place coding in feature map
\end{itemize}

\section{Notes}

\begin{itemize}
	\item Dedicated capsule must have access to the whole receptive field. This is expensive. Furthermore if multiple instances of the respective object part are present, it cannot be expressed. However dynamic routing is possible with dedicated capsules.
	\item Universal capsules on different locations have the advantage of being able to express any part of an object. With multiple layers of universal capsules, we get the problem of symmetry, as it is unclear to which capsule in an upper layer the information is routed.
	\item Do a combination of both?
\end{itemize}

\begin{ack}
for my goldfish fritzgerald
\end{ack}

\bibliographystyle{apalike}
\bibliography{references}

\end{document}