\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{verbatim}		% Comments


\newtheorem{theorem}{Theorem}

\title{                                                                                                                                          
	Why CapsNets are hard to get right! \\
	\vspace{0.5cm}
	\large
	Challenging  the dynamic parse-tree assumption  in Capsule Neural Networks
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\Read Carefully and think about impact on the paper
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
  Write the paper like a blog post!
  Why haven't CapsNet taken off?
  What are the issues?
  Challenge the dynamic parse-tree assumption in the Capsule Neural Network.
  Show that there exists a discrepancy between the semantics of CapsNets and its technical implementation.
  Explain why the CapsNet does not scale to larger tasks.
  Sow that the CapsNet model sucks and proof that analytically!
\end{abstract}

\begin{comment}
	TODO
	- Rework results
	- Reread the CapsNet paper
	- Think about motivation for parse trees, then motivate experiements accordingly
	- Add better less exhaustive introduction
	- Rework related work
	
	IDEA:
	- Title in parse-tree form
\end{comment}

\section{Introduction}

The Capsule Neural Network (CapsNet)~\cite{nips/SabourFH17} was vowed to succeed the Convolutional Neural Network (ConvNet)~\cite{neco/LeCunBDHHHJ89} as the predominant method for computer vision tasks, due to its ability to overcome some of the ConvNet`s limitations. %TODO: schlechter satz, wer sage das?%
These limitations include
a lack of robustness to affine transformations and novel viewpoints on the input data, 
the susceptibility to adversarial attacks,
exponential inefficiencies
and a general lack of interpretability in the networks decision making process. %TODO: add references%

The CapsNet was specifically designed to address some of these limitations and demonstrated SOTA classification accuracy on MNIST with less parameters and stronger robustness to affine transformations, compared to their ConvNet`s baseline, and sparked a flood of follow-up research.

% GOOD CAPSNETS RESULT
Results demonstrating advantages of CapsNets over baseline ConvNet`s include:
a better transferability of CapsNet features~\cite{corr/Ancheng2018},
the ability to generalize to novel viewpoints and robustness to white box adversarial attacks~\cite{iclr/HintonSF18},
a looser coupling of the output features~\cite{corr/Lin2018}.
the strong robustness to class imbalances~\cite{miccai/Jimenez-Sanchez18} and
the ability to better extrapolate to affine transformations~\cite{cvpr/GuT20}.

Furthermore it has been shown that CapsNet deliver interpretable and equivariant object representations~\cite{nips/LenssenFL18}, meaningful embeddings~\cite{corr/Lin2018} and that object capsule presences are highly informative of the object class and allow for geometric reasoning~\cite{nips/KosiorekSTH19}.

The CapsNet idea also resonated well with the popular science community~\cite{book/hawkins2021},~\cite{book/Hiesinger2021} and it was even featured in the ACM Turing Lecture~\cite{comacm/Bengio2021} to be a future direction to the field of deep learning and AI.

% DISCREPANCIES
Despite these promising results and positive vibes, we found several discrepancies regarding the whole CapsNet field.

% DISCREPANCY: CapsNets do not achieve SOTA
To this date, CapsNets don't scale. Although the CapsNet was introduced in the realm of computer vision, to the extend of our knowledge, the best performing CapsNet version~\cite{nips/AhmedT19} achieves only $60.07\%$ top-1 image classification accuracy on ImageNet~\cite{data/imagenet}, far behind the SOTA Transformer based approaches~\cite{corr/Wortsman2022} and  ConvNet~\cite{cvpr/Pham2021} with $90.88\%$ and  $90.02\%$ accuracy respectively.
This raises the question: Why don't CapsNets scale, despite the immense research effort?

% DISCREPANCY: Disjoint Metrics
Furthermore, many CapsNet approaches don't report on ImageNet, but prefer to benchmark on much smaller datasets like MNIST or CIFAR10 or niche datasets like SmallSNORB. %TODO add references%
Especially SOTA CapsNet results are only claimed on these. %TODO add references%
As other non-CapsNet approaches usually do not report on these, this results in disjoint metrics, making general claims hard.
This issue has recently been acknowledged in~\cite{cvpr/GuT021}, where it was demonstrated, that a stronger ConvNet baseline still outperforms the CapsNet on any evaluated task, reaching SOTA relatively cheaply.
There seems to be bubbly trend within the CapsNet community to refer and compare only to peers within their group.

% DISCREPANCY: CapsNets: A technical mess!
% TODO for more details i must descripe the CapsNet before. Otherwise it does not make sense to list all the diferences
Another discrepancy lies within the vagueness and broad usage of the term itself.
The original work~\cite{nips/SabourFH17} provides a semantic, biologically inspired motivation , from which their technical model implementation is deduced. Retrospectively, this implementation was later entitled "CapsNet".
From there things complicate, as follow-up work, despite significant technical differences is also termed CapsNet, raising the questions: What are CapsNets actually? And What aren't?

% DISCREPANCY: CapsNets: Negative results!
% TODO for more detalis i must describe the CapsNet before. Otherwise it does not make sense to list all the negative stuff
Moreover, negative results regarding CapsNets emerged, questioning any of the promised benefits and technical progress altogether.
~\cite{acml/PaikKK19} showed that increasing depth did not improve accuracy and routing algorithms do not provide any benefit when it comes to accuracy in image classification.
~\cite{corr/michels2019} showed that CapsNet can be as easily fooled as ConvNets.
~\cite{ieee/Gu2020} revealed, that removing dynamic routing improves transformation robustness.
The work in ~\cite{cvpr/GuT021} showed that the individual parts of the CapsNets have contradictory effects on the performance in different tasks. Furthermore they showed, that with the right baseline, the CapsNet is not generally superior to ConvNets.
~\cite{iclr/Gu2021} designed a first specific attack on CapsNets and
\cite{prl/PeerSR21} show that RBA is limited in depth!

%Lay out the paper
In this paper we argue that the misconception regarding the CapsNets lies in the vagueness of the definition.
In its core the original CapsNet provides a semantic definition, namely parsing objects into part-whole hierachies or parse-trees.
From this the technical implementation is derived.


CapsNets are hard to grasp, as there exists a semantic component.
The heart of CapsNets = Semantic component, Parse-Tree idea.
The syntactic component.
Ideas close to the synatax face the issue of performing well.
Ideas that go beyond the idea, better performance, but then the metrics do not apply! inadequate baselines.
No one focused on the dynamic parse-tree.
We focus on the original implementation, but will give insights on follow-ups.
Benefits come from their semantic component, assumption. Never shown to come true
Issues with the technical implementation.
To the best of our knowledge, the Parse-Tree issue has not been addressed so far. Elephant in the room!

What we do?
\begin{itemize}
	\item TOPIC: Parse-Tree. What it is and why it is important.
	\item What are CapsNets? Semantics and Syntax: This makes them hard
	\item The concept of CapsNets: Parse-Trees
	\item The implementation of CapsNets
	\item Follow Up work and Misconceptions about CapsNets
	\item Technical Limitations of the approach
	\item Semantic limitations of the approach
	\item Focus on the original implementation, but will give insights on follow-ups.
\end{itemize}

What we don't:
\begin{itemize}
	\item Survey
	\item Cannot proof that CapsNet do not work
	\item No claim of exhaustiveness
	\item Don't discredit other peoples work!
\end{itemize}

Why this work?
\begin{itemize}
	\item The field is confusing as hell, and we disentangle.
	\item In exchange with other researches, several misconceptions!
	\item Explain technical and conceptual issues with CapsNets.
	\item Inspire new research by demonstrating issues with CapsNets
	\item We cannot proof anything, however, as we feel, there is an elephant in the room that should be addressed
\end{itemize}

\section{Related Work and Follow-Ups}

\subsubsection*{[A Short History of Progress]}
~\cite{cogsci/Hinton79} Some reference to the hierachy of parts.
~\cite{icann/HintonKW11} Transforming Autoencoders
~\cite{nips/SabourFH17} CapsNet.
~\cite{iclr/HintonSF18} EM Routing
~\cite{nips/KosiorekSTH19} SCAE
~\cite{nips/HahnPK19} Self-Routing Capsule Networks
~\cite{nips/AhmedT19} Star-Caps: No clue what to do with them.
~\cite{iclr/TsaiSGS2} Inverted Dot-Product Attention Routing.
~\cite{corr/Hinton2021} GLOM
~\cite{icml/SabourTYHF21} Flow Capsules: Learning PrimeCaps
~\cite{iclr/Wang018} framed the routing in ~\cite{nips/SabourFH17} as an optimization problems.
 \cite{prl/PeerSR21} show that RBA is limited in depth!
~\cite{corr/Rawlinson2018} introduced an unsupervised learning scheme for CapsNet. Furthermore they show that is is necessary to introduce sparsity alongside unsupervised training. Otherwise Capsules don't specialize. Labels create sparsity.
The work~\cite{corr/Michels19} showed that CapsNet can be fooled as easily as ConvNets.
~\cite{acml/PaikKK19} showed that increasing depth did not improve accuracy. However theey used ResNet-34 backbone. They prosume
capsules are more appropriate to represent high-level concepts than low-level concepts
such as lines and edges, because they are more informative and their part-whole
relations are less ambiguous than those of low-level concepts. They find the link polarization problem [OptimCaps and AttnCaps] suffer from the same problem. EMCaps are observed to suffer from the same problem.
~\cite{prl/PeerSR21} showed limitations of CapsNet when going deeper. They propose a Bias. However a bias term prevents the routing and emerging of a parse-tree.
The receptive field problem was also recognized in ~\cite{spl/XiangZTZX18}, the authors propose a MultiScale Architecture to reduce parameters, still this work relies on dynamic routing.
~\cite{cvpr/GuT021} emprical sutdy on the individual parts of the CapsNet. Better ConvNet baslines required. CapsNet are not more robust 
Hard to give a general statement about the state of CapsNet, as there are both technical as well as semantic developments.
It it works, it is good! Here there is a conceptual component to it, that makes it hard!
than ConvNets.
~\cite{iclr/TsaiSGS20} Inverted Dot- \cite{prl/PeerSR21} show that RBA is limited in depth!Product Attention Routing. No Activation, So what about the parse-tree? Layer-Norm will prevent sparsity.
~\cite{cvpr/RajasegaranJJJS19} Gooing deeper

GLOM ~\cite{corr/Hinton2021}

\subsubsection*{[Negative Results]}
Moreover, negative results regarding CapsNets emerged, questioning any of the promised benefits and technical progress altogether.
~\cite{acml/PaikKK19} showed that increasing depth did not improve accuracy and routing algorithms do not provide any benefit when it comes to accuracy in image classification.
~\cite{corr/michels2019} showed that CapsNet can be as easily fooled as ConvNets.
~\cite{ieee/Gu2020} revealed, that removing dynamic routing improves transformation robustness.
The work in ~\cite{cvpr/GuT021} showed that the individual parts of the CapsNets have contradictory effects on the performance in different tasks. Furthermore they showed, that with the right baseline, the CapsNet is not generally superior to ConvNets.
~\cite{iclr/Gu2021} designed a first specific attack on CapsNets and
\cite{prl/PeerSR21} show that RBA is limited in depth!


\subsubsection*{[Read Carefully and think about impact on the paper]}
~\cite{iclr/TsaiSGS20} Inverted Dot-Product Attention Routing. No Activation, So what about the parse-tree? Layer-Norm will prevent sparsity. It may be necessary to implement and test this method!
~\cite{nips/AhmedT19} Star-Caps: No clue what to do with them.
~\cite{iclr/TsaiSGS20} Inverted Dot-Product Attention Routing.
 ~\cite{cvpr/RajasegaranJJJS19} Going Deeper with Capsule Networks
~\cite{nips/LenssenFL18} provably equivariance

\subsubsection*{[Read and/or include]}
~\cite{eccv/LiGDOW18} Neural Capsule Encapsulation, seems complicated. More parameter efficient and less computational heavy.
GroupCaps ~\cite{cvpr/ChenJSWS18}, not really sure what they are doing. Probabliy too much.

\section{The Semantic CapsNet: Of Parse Trees and Dragons}

The key motivation of the CapsNet is the ability to dynamically deconstruct a scene into a part-whole hierarchy or parse-tree, where a complex object is deconstructed in smaller more simpler parts.
The process of dynamically creating such a part-whole hierarchy finds analogy in parsing a string according to some context-free-grammar.
The CapsNet is specifically designed to provide a kind of grammar for images.
More background: Human vision, geon theory, Hinton stuff.
%TODO: Add cool visualization of a parse tree

what is a parse tree?
\begin{itemize}
	\item Nodes: Stores istantiation parameters
	\item Hierachy of Nodes
	\item Each lower level node is linked to a parent node
	\item part-whole relationship
\end{itemize}
why do we want a parse tree?
\begin{itemize}
	\item Inverse graphics
	\item Picasso Problem \url{https://www.sciencedirect.com/science/article/abs/pii/S0141938222000439}
	\item Grammar of images
	\item Interpretability
	\item Debugging of neural networks
	\item Transferability
	\item Robustness
	\item allows inference, as discrete
	\item allows insights in learning
	\item conceptual learning
	\item Semantic concepts and distentangled representation
\end{itemize}

\section{The Technical CapsNet: Of Routing and Capsules}

What is a CapsNet? Technical definition
Show that it was designed to implement the parse tree.
Show how it can be scaled up.

Components
\begin{itemize}
	\item Capsule: dedicated part, vector / matrix, instantiation parameter
	\item Multiple layers of those
	\item Routing algorithm to assign lower level capsules to a parent.
\end{itemize}

What do we want for a strong parse-tree to emerge:
\begin{itemize}
	\item Sparse but strong activation of the capsules
	\item Strong coupling coefficients
	\item Dynamic assignments, Different input, different routing.
	\item For complex parse-trees, depth is required.
\end{itemize}

Implementation of a parse-tree in CapsNets:
\begin{itemize}
	\item Nodes via Capsules. = Vectors. Capsules represent dedicated objects
	\item PrimeCaps full receptive field.
	\item No dynamic allocation of memory is possible, fixed number of capsules in each layers that can be either active or inactive. Sparse activation.
	\item Activeness via norm. Probabilistic interpretation.
	\item instantiation parameters via direction. The values in the vectors are conceptually thought of being the instantiation parameters of the part, the capsule stands for!
	\item Activeness implemented via a squashing function that ensures that the length of the capsule vector in in $[0,1)$.
	\item The squashing function is required to force bipolar response and is necessary to knock capsules out. Furthermore it is required to allow the probabilistic interpretation of the capsule norm and the interpretation of a objects presence.
	\item Capsule activated via routing.
	\item Coupling coefficients calculated via a routing algorithm.
	\item Coupling Ideally strong couplings for a clear parse-tree to emerge. Strong agreement. Non-iterative algorithms this can be done via softmax temperature, iterative algorithms that is the number of iterations can be directly linked to stronger couplings.
	\item Ensure that each capsule is routed, softmax over coupling coefficients.
	\item Coupling coefficients calculated via a routing algorithm.
	\item Hierarchical: multiple layers of Capsules.
	\item Sparse but strong activation via squashing function. Dynamic allocation not possible. therefore switch.
	\item Dynamic routing via calculation of coupling coefficients via agreement.
	\item Strong coupling via softmax temperature or number of iterations.
\end{itemize}

%TODO ADD VISUALIZATION of CpasNet architecture
%TODO ADD training precedure

\subsubsection*{Model Class in this work}
\begin{itemize}
	\item Backbone, receptive field requirement!
	\item Vectorized capsules
	\item Coupling coefficients determined by routing
	\item Bias
	\item Softmax
	\item Squash
\end{itemize}

\section{Results}
\subsection{Miscellaneous and Ideas}
\begin{itemize}
	\item Build argument around technical complexity of the approaches
	\item Discuss parameter- and computational inefficiencies
	\item Investigate the Picasso problem (\textbf{THIS})
\end{itemize}
\subsection{There is not evidence that supports the parse-tree}
This should actually be the implication of the other results.

\subsection{Issues regarding the Backbone and PrimeCaps}
\textbf{Observations}:
\begin{itemize}
	\item Usually, there is a ConvNet backbone that extracts features from the image.
	\item These features are then reshaped and constitute the first layer of Capsules, the PrimeCaps
	\item Each one of these PrimeCaps must access the full receptive field.
	\item Two options for backbone: shallow or deep ConvNet.
\end{itemize}
\textbf{Case}: Shallow ConvNet backbone results in:
\begin{itemize}
	\item Less abstract PrimeCaps, which is conceptually problematic due to inverse pyramid principle. Edges occur more often! 
	\item Scaling issues, as the feature maps are way too large. Calculate receptive field implications for different image sizes, number of prime caps and parameters and dimension of prime caps.
	\item, [Show on larger image sizes] and we get either
	\begin{itemize}
		\item Capsule Explosion, -> Computationally expensive routing %TODO calc routing complexity
		\item Capsule dimensional explosion: Computationally expensive routing and breaks with the concept.
	\end{itemize}
	\item Shifts the task of building more abstract and compositional objects to the routing algorithm. And as we see later, also this is limited.
	\item Sparse activation of the PrimeCaps possible
	\item Therefore, since this approach does not scale, deeper backbones might be the solution.
	\item Show that Capsule for one sample activates for a certain part in the input and then show that it activates for another part in the transformed image.
\end{itemize}

\textbf{Case}: Deeper ConvNet backbone results in:
\begin{itemize}
	\item More abstract PrimeCaps possible
	\item All the benefits and issues with ConvNets, especially diversification.
	\item Loss of traceability. Feature to Output
	\item Still not easy to implement since pooling layers are prohibited from a conceptual perspective. Using pooling breaks with the concept.
	\item Shifts more work to the CNN. Hypothesis is actually, that all the work is done in the backbone. [Some experiments with ResNet exist and can be referenced]
	\item Experiments show, that PrimeCaps are not at all sparse, but rather dense, which breaks with the semantics. Also since all PrimeCaps are active, always, they must be routed!
\end{itemize}

Notes on ConvNets
\begin{itemize}
	\item ConvNets usually diversify and don`t consolidate. The diversification of the ConvNets might be shown analytically.
	\item Does the transition from place to rate coding really take place? And where?
	\item Entenglement literature does not suggest, that CNN per se learn interpretable features.
	The diversification of the ConvNets might be shown analytically.
\end{itemize}

Central arguments
\begin{itemize}
	\item CapsNets with shallow backbones do not scale
	\item CapsNets with deeper backbones do not profit from routing and the PrimeCaps are noisy. 
\end{itemize}

Open questions:
\begin{itemize}
	\item Can we show that we cannot attribute certain features to a PrimeCaps? E.g. Can we show that information about the color "red" might be scattered around many Capsules?
	\item Can we come up with a stronger theoretical claim?
\end{itemize}

\subsection{The Softmax routing}

\begin{itemize}
	\item SNR!
	\item Connection to Transformers, that do the opposite, which is great, as it allows each capsule to learn better!
	\item Strong coupling coefficients are not given automatically by the algorithms, but can be enforced via softmax temperature or more iterations in the routing rounds.
	\item However this result in sparse capsules and bad performance.
	\item Strong coupling coefficients do not necessarily yield a dynamic parse tree!
	\item Even inactive capsules are forced to route uniformly. This results in a bad signal to noise ratio and prevents effective learning.
\end{itemize}

\subsection*{The Squashing results in vanishing gradient}
\begin{itemize}
	\item Capsules only activate when other capsules route to them, by squashing.
	\item Squashing leads to vanishing gradient / no information flow for inactive capsules.
	\item Vanishing gradient leads to dead capsules
	\item Dead capsules rarely activate
	\item Dead capsule don't learn.
\end{itemize}

\subsection*{The squashing and Routing leads to information loss}
two scenarios:
\begin{itemize}
	\item Dense Setting: = "Uniform routing", capsules are equally squashed. This limits depth and expressiveness of parse-tree.
	\item Sparse Setting: Dead capsules stay dead, alive capsules stay alive, because of vanishing gradient. This also limits depth and expressiveness of parse-tree.
\end{itemize}
Show in both cases experimentally
\begin{itemize}
	\item The activation norm of capsules gets sparser and lower over the layers!
	\item The norm of the gradients for Capsules
	\item The loss lowers step wise, The capsules in upper layers take much longer to train.
\end{itemize}

\subsection*{Dynamics}
Common parse-trees are largely of a static nature!
Measure dynamics.
Show that there isn't much. 
If enforced, that leads to problems!
The Bias prevents dynamic routing of capsules and the routing becomes static!

\subsection*{RBA limits the depths of the CapsNet}
 
Routing, that is based on agreement limits the depth of the CN logarithmically with base agreement factor, depending on the number of PrimeCaps!
This prevents the CN from getting deeper!
There are multiple arguments for deeper CapsNets
\begin{itemize}
	\item Complex parse-trees, requires either weak agreemetn or tons of primecaps.
	\item conceptually it would make sense to have a limited number of primecaps that can be used to create an unlimited number of complex objects.
	\item Minimizing the impact of the ConvNet backbone. It was also shown that in this case, CapsNets do not really yield a benefit ~\cite{acml/PaikKK19}.
	\item Depth is required for abstract features to form.
	\item Practical trend for deeper architectures.
	\item There is also theoretical evidence, that depth is necessary.
	\item A Bias term allows depth but prevents parse-tree from forming.
\end{itemize}
We show that RBA limits the possible depth of CN.

Stronger coupling coefficients require stronger agreement. 
Strong agreement limits the depth of the Network.

\begin{theorem}
	The number of PrimeCaps rises exponentially with the number of capsule layers or
	The depth of a CapsNet is logarithmically dependent on the number of primary capsules.
	The base of the logarithm is the agreement factor.
	Agreement factor is the average number of capsules that agree.
\end{theorem}
Routing beyond that limit seems pointless!
If only one Caps remains, there is no point of agreement routing! so routing becomes random or static!

Experiments
\begin{itemize}
	\item Make CapsNet deeper and wider and vary coupling strength [hopefully by softmax or by more routing iterations]
	\item Show the number of capsules that are active on average.
	\item Calculate the agreement factor!
\end{itemize}

Opposing requirements. Make it deeper or make the coupling stronger.
Depth is a requirement for abstract features to form.

The only choice that CNs have is to further rely on CNNs all the way offering no real alternative.
It was also shown that in this case, CapsNets do not really yield a benefit ~\cite{acml/PaikKK19}.

Other papers also noticed the problem with of training deep CapsNet. They added Bias term and showed that in this case accuracy does not vanish.
Furthermore the work in \cite{prl/PeerSR21} or Efficient-CapsNet adds a Bias term to ensure a CapsNets to be layers.
From a technical perspective this works but this also undermines the dynamic routing.
Routing becomes static.

Experiments
\begin{itemize}
	\item Show that the Bias term prevents dynamic routing!
\end{itemize}

number of active capsules with agreement factor for 12 layer
\begin{itemize}
	\item 2: 4096
	\item 3: 531441
	\item 4: 16777216
\end{itemize}

\section{Conclusion}

Strong semantic attachment, evidence for the parse-tree occurrence should be demonstrated. Otherwise the metrics for comparisons should be adjusted accordingly.
With this model class, the idea of parsing a complex object into a hierarchy of its parts can not be confirmed.

We found:
\begin{itemize}
	\item No parse tree in the current implementations
	\item Technical implementation has some limitations.
\end{itemize}

We show that the methods, that aim to reach that goal limit efficient and effective training.
Furthermore adjusting those methods to a certain degree comes with the downside of preventing the parse-tree of taking form.
In the sum all those arguments lead to the conclusion that the Parse-Tree cannot be realised with CapsNets, in contrary to common believe.
Multiple reasons for the parse-tree to not work, we disregard the idea in this case.

\begin{itemize}
	\item Universal Capsule might be conceptually better
	\item Problem of multiple instances of parts in one image. e.g legs
	\item GLOM
\end{itemize}

%\bibliographystyle{apalike}
\bibliographystyle{unsrt}
\bibliography{references}
\end{document}