{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c42d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727bd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beeb573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "#\n",
    "\n",
    "from misc.plot_utils import plot_mat, imshow\n",
    "from effcn.layers import FCCaps, FCCapsWOBias, Squash\n",
    "from misc.utils import count_parameters\n",
    "from effcn.functions import margin_loss\n",
    "from datasets import AffNIST\n",
    "#\n",
    "from einops import rearrange, repeat\n",
    "from torch import einsum, nn\n",
    "#\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7fac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:1\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54f86e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = T.Compose([\n",
    "    T.RandomAffine(degrees=(-8, 8),\n",
    "                   shear=(-15, 15),\n",
    "                   scale=(0.9, 1.1)\n",
    "                  ),\n",
    "    T.Normalize((0.0641,), (0.2257))\n",
    "])\n",
    "transform_valid = T.Normalize((0.0641,), (0.2257))\n",
    "\n",
    "p_data = '/home/matthias/projects/EfficientCN/data'\n",
    "\n",
    "ds_mnist_train = AffNIST(p_root=p_data, split=\"mnist_train\", download=True, transform=transform_train, target_transform=None)\n",
    "ds_mnist_valid = AffNIST(p_root=p_data, split=\"mnist_valid\", download=True, transform=transform_valid, target_transform=None)\n",
    "ds_affnist_valid = AffNIST(p_root=p_data, split=\"affnist_valid\", download=True, transform=transform_valid, target_transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae53463",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 512\n",
    "dl_mnist_train = torch.utils.data.DataLoader(\n",
    "    ds_mnist_train, \n",
    "    batch_size=bs, \n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4)\n",
    "dl_mnist_valid= torch.utils.data.DataLoader(\n",
    "    ds_mnist_valid, \n",
    "    batch_size=bs, \n",
    "    shuffle=True, \n",
    "    pin_memory=True,\n",
    "    num_workers=4)\n",
    "dl_affnist_valid= torch.utils.data.DataLoader(\n",
    "    ds_affnist_valid, \n",
    "    batch_size=bs, \n",
    "    shuffle=True, \n",
    "    pin_memory=True,\n",
    "    num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1129f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = next(iter(dl_mnist_train))\n",
    "x_vis_train = x[:32]\n",
    "\n",
    "x, _ = next(iter(dl_mnist_valid))\n",
    "x_vis_mnist_valid = x[:32]\n",
    "\n",
    "x, _ = next(iter(dl_affnist_valid))\n",
    "x_vis_affnist_valid = x[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c9f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torchvision.utils.make_grid(x_vis_train).permute(1,2,0))\n",
    "plt.show()\n",
    "#\n",
    "plt.imshow(torchvision.utils.make_grid(x_vis_mnist_valid).permute(1,2,0))\n",
    "plt.show()\n",
    "#\n",
    "plt.imshow(torchvision.utils.make_grid(x_vis_affnist_valid).permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01718f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, fn, context_dim=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)  # use switchnorm?\n",
    "        self.norm_context = nn.LayerNorm(context_dim) if helpers.exists(\n",
    "            context_dim) else None\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if helpers.exists(self.norm_context):\n",
    "            context = kwargs[\"context\"]\n",
    "            normed_context = self.norm_context(context)\n",
    "            kwargs.update(context=normed_context)\n",
    "\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim=-1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, mult=4, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(dim, dim * mult * 2), GEGLU(),\n",
    "                                 nn.Dropout(dropout),\n",
    "                                 nn.Linear(dim * mult, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 query_dim,\n",
    "                 context_dim=None,\n",
    "                 heads=8,\n",
    "                 dim_head=64,\n",
    "                 dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = helpers.default(context_dim, query_dim)\n",
    "\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Linear(inner_dim, query_dim),\n",
    "                                    nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, x, context=None, mask=None):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        context = helpers.default(context, x)\n",
    "        k, v = self.to_kv(context).chunk(2, dim=-1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> (b h) n d\", h=h),\n",
    "                      (q, k, v))\n",
    "\n",
    "        sim = einsum(\"b i d, b j d -> b i j\", q, k) * self.scale\n",
    "\n",
    "        if helpers.exists(mask):\n",
    "            mask = rearrange(mask, \"b ... -> b (...)\")\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, \"b j -> (b h) () j\", h=h)\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        attn = sim.softmax(dim=1)\n",
    "\n",
    "        out = einsum(\"b i j, b j d -> b i d\", attn, v)\n",
    "        out = rearrange(out, \"(b h) n d -> b n (h d)\", h=h)\n",
    "\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class Perceiver(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 num_freq_bands,\n",
    "                 depth,\n",
    "                 max_freq,\n",
    "                 freq_base=2,\n",
    "                 input_channels=3,\n",
    "                 input_axis=2,\n",
    "                 num_latents=512,\n",
    "                 latent_dim=512,\n",
    "                 cross_heads=1,\n",
    "                 latent_heads=8,\n",
    "                 cross_dim_head=64,\n",
    "                 latent_dim_head=64,\n",
    "                 num_classes=1000,\n",
    "                 attn_dropout=0.,\n",
    "                 ff_dropout=0.,\n",
    "                 weight_tie_layers=False,\n",
    "                 fourier_encode_data=True,\n",
    "                 self_per_cross_attn=1):\n",
    "        super().__init__()\n",
    "        self.input_axis = input_axis\n",
    "        self.max_freq = max_freq\n",
    "        self.num_freq_bands = num_freq_bands\n",
    "        self.freq_base = freq_base\n",
    "\n",
    "        self.fourier_encode_data = fourier_encode_data\n",
    "        fourier_channels = (input_axis * (\n",
    "            (num_freq_bands * 2) + 1)) if fourier_encode_data else 0\n",
    "        input_dim = fourier_channels + input_channels\n",
    "\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))\n",
    "\n",
    "        get_cross_attn = lambda: PreNorm(latent_dim,\n",
    "                                         Attention(latent_dim,\n",
    "                                                   input_dim,\n",
    "                                                   heads=cross_heads,\n",
    "                                                   dim_head=cross_dim_head,\n",
    "                                                   dropout=attn_dropout),\n",
    "                                         context_dim=input_dim)\n",
    "        get_cross_ff = lambda: PreNorm(\n",
    "            latent_dim, FeedForward(latent_dim, dropout=ff_dropout))\n",
    "        get_latent_attn = lambda: PreNorm(\n",
    "            latent_dim,\n",
    "            Attention(latent_dim,\n",
    "                      heads=latent_heads,\n",
    "                      dim_head=latent_dim_head,\n",
    "                      dropout=attn_dropout))\n",
    "        get_latent_ff = lambda: PreNorm(\n",
    "            latent_dim, FeedForward(latent_dim, dropout=ff_dropout))\n",
    "\n",
    "        get_cross_attn, get_cross_ff, get_latent_attn, get_latent_ff = map(\n",
    "            helpers.cache_fn,\n",
    "            (get_cross_attn, get_cross_ff, get_latent_attn, get_latent_ff))\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for i in range(depth):\n",
    "            should_cache = i > 0 and weight_tie_layers\n",
    "            cache_args = {'_cache': should_cache}\n",
    "\n",
    "            self_attns = nn.ModuleList([])\n",
    "\n",
    "            for _ in range(self_per_cross_attn):\n",
    "                self_attns.append(\n",
    "                    nn.ModuleList([\n",
    "                        get_latent_attn(**cache_args),\n",
    "                        get_latent_ff(**cache_args)\n",
    "                    ]))\n",
    "\n",
    "            self.layers.append(\n",
    "                nn.ModuleList([\n",
    "                    get_cross_attn(**cache_args),\n",
    "                    get_cross_ff(**cache_args), self_attns\n",
    "                ]))\n",
    "\n",
    "        self.to_logits = nn.Sequential(nn.LayerNorm(latent_dim),\n",
    "                                       nn.Linear(latent_dim, num_classes))\n",
    "\n",
    "    def forward(self, data, mask=None):\n",
    "        b, *axis, _, device = *data.shape, data.device\n",
    "        assert len(\n",
    "            axis\n",
    "        ) == self.input_axis, 'input data must have the right number of axis'\n",
    "\n",
    "        data = self.get_embeddings(data)\n",
    "\n",
    "        x = repeat(self.latents, 'n d -> b n d', b=b)\n",
    "\n",
    "        for cross_attn, cross_ff, self_attns in self.layers:\n",
    "            x = cross_attn(x, context=data, mask=mask) + x\n",
    "            x = cross_ff(x) + x\n",
    "\n",
    "            for self_attn, self_ff in self_attns:\n",
    "                x = self_attn(x) + x\n",
    "                x = self_ff(x) + x\n",
    "\n",
    "        x = x.mean(dim=-2)\n",
    "        return self.to_logits(x)\n",
    "\n",
    "    def forward_wohead(self, data, mask=None):\n",
    "        b, *axis, _, device = *data.shape, data.device\n",
    "        assert len(\n",
    "            axis\n",
    "        ) == self.input_axis, 'input data must have the right number of axis'\n",
    "\n",
    "        data = self.get_embeddings(data)\n",
    "\n",
    "        x = repeat(self.latents, 'n d -> b n d', b=b)\n",
    "\n",
    "        for cross_attn, cross_ff, self_attns in self.layers:\n",
    "            x = cross_attn(x, context=data, mask=mask) + x\n",
    "            x = cross_ff(x) + x\n",
    "\n",
    "            for self_attn, self_ff in self_attns:\n",
    "                x = self_attn(x) + x\n",
    "                x = self_ff(x) + x\n",
    "        return x\n",
    "    \n",
    "    def get_embeddings(self, data):\n",
    "        b, *axis, _, device = *data.shape, data.device\n",
    "        if self.fourier_encode_data:\n",
    "            # calculate fourier encoded positions\n",
    "            # in the range of [-1, 1], for all axis\n",
    "\n",
    "            axis_pos = list(\n",
    "                map(\n",
    "                    lambda size: torch.linspace(\n",
    "                        -1., 1., steps=size, device=device), axis))\n",
    "            pos = torch.stack(torch.meshgrid(*axis_pos), dim=-1)\n",
    "            enc_pos = helpers.fourier_encode(pos,\n",
    "                                             self.max_freq,\n",
    "                                             self.num_freq_bands,\n",
    "                                             base=self.freq_base)\n",
    "            enc_pos = rearrange(enc_pos, '... n d -> ... (n d)')\n",
    "            enc_pos = repeat(enc_pos, '... -> b ...', b=b)\n",
    "\n",
    "            data = torch.cat((data, enc_pos), dim=-1)\n",
    "\n",
    "        # concat to channels of data and flatten axis\n",
    "\n",
    "        data = rearrange(data, 'b ... d -> b (...) d')\n",
    "\n",
    "        return  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57af20c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceiver(\n",
    "    input_channels = 1,          # number of channels for each token of the input\n",
    "    input_axis = 2,              # number of axis for input data (2 for images, 3 for video)\n",
    "    num_freq_bands = 4,          # number of freq bands, with original value (2 * K + 1)\n",
    "    max_freq = 10.,              # maximum frequency, hyperparameter depending on how fine the data is\n",
    "    depth = 1,                   # depth of net. The shape of the final attention mechanism will be:\n",
    "                                 #   depth * (cross attention -> self_per_cross_attn * self attention)\n",
    "    num_latents = 64,            # number of latents, or induced set points, or centroids. different papers giving it different names\n",
    "    latent_dim = 32,             # latent dimension\n",
    "    cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
    "    latent_heads = 1,            # number of heads for latent self attention, 8\n",
    "    cross_dim_head = 32,          # number of dimensions per cross attention head\n",
    "    latent_dim_head = 32,        # number of dimensions per latent self attention head\n",
    "    num_classes = 10,           # output number of classes\n",
    "    attn_dropout = 0.,\n",
    "    ff_dropout = 0.,\n",
    "    weight_tie_layers = False,   # whether to weight tie layers (optional, as indicated in the diagram)\n",
    "    fourier_encode_data = True,  # whether to auto-fourier encode the data, using the input_axis given. defaults to True, but can be turned off if you are fourier encoding the data yourself\n",
    "    self_per_cross_attn = 2      # number of self attention blocks per cross attention\n",
    ")\n",
    "model = model.to(device)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceiver(\n",
    "            input_channels = 1,          # number of channels for each token of the input\n",
    "            input_axis = 2,              # number of axis for input data (2 for images, 3 for video)\n",
    "            num_freq_bands = 4,          # number of freq bands, with original value (2 * K + 1)\n",
    "            max_freq = 10.,              # maximum frequency, hyperparameter depending on how fine the data is\n",
    "            depth = 2,                   # depth of net. The shape of the final attention mechanism will be:\n",
    "                                         #   depth * (cross attention -> self_per_cross_attn * self attention)\n",
    "            num_latents = 32,            # number of latents, or induced set points, or centroids. different papers giving it different names\n",
    "            latent_dim = 32,             # latent dimension\n",
    "            cross_heads = 1,             # number of heads for cross attention. paper said 1\n",
    "            latent_heads = 1,            # number of heads for latent self attention, 8\n",
    "            cross_dim_head = 32,          # number of dimensions per cross attention head\n",
    "            latent_dim_head = 32,        # number of dimensions per latent self attention head\n",
    "            num_classes = 10,           # output number of classes\n",
    "            attn_dropout = 0.,\n",
    "            ff_dropout = 0.,\n",
    "            weight_tie_layers = False,   # whether to weight tie layers (optional, as indicated in the diagram)\n",
    "            fourier_encode_data = True,  # whether to auto-fourier encode the data, using the input_axis given. defaults to True, but can be turned off if you are fourier encoding the data yourself\n",
    "            self_per_cross_attn = 2\n",
    ")\n",
    "model = model.to(device)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee6d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.rand(1, 40, 40, 1).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b5c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3, weight_decay=2e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66572460",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27eb805",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 51\n",
    "#\n",
    "for epoch_idx in range(num_epochs):\n",
    "    # ####################\n",
    "    # TRAIN\n",
    "    # ####################\n",
    "    model.train()\n",
    "    desc = \"Train [{:3}/{:3}]:\".format(epoch_idx, num_epochs)\n",
    "    pbar = tqdm(dl_mnist_train, bar_format=desc + '{bar:10}{r_bar}{bar:-10b}')\n",
    "    \n",
    "    for x,y_true in pbar:\n",
    "        x = x.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model.forward(x.permute(0,2,3,1))\n",
    "        loss = criterion(logits, y_true)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        y_pred = torch.argmax(logits, dim=1)\n",
    "        acc = (y_true == y_pred).sum() / y_true.shape[0]\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "                {'loss': loss.item(),\n",
    "                 'acc': acc.item()\n",
    "                 }\n",
    "        )\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    #\n",
    "    # ####################\n",
    "    # VALID\n",
    "    # ####################\n",
    "    if epoch_idx % 5 != 0:\n",
    "        continue\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x,y_true in dl_mnist_valid:\n",
    "        x = x.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model.forward(x.permute(0,2,3,1))\n",
    "            y_pred = torch.argmax(logits, dim=1)\n",
    "            total_correct += (y_true == y_pred).sum()\n",
    "            total += y_true.shape[0]\n",
    "    print(\"   mnist acc_valid: {:.3f}\".format(total_correct / total))\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x,y_true in dl_affnist_valid:\n",
    "        x = x.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model.forward(x.permute(0,2,3,1))\n",
    "            y_pred = torch.argmax(logits, dim=1)\n",
    "            total_correct += (y_true == y_pred).sum()\n",
    "            total += y_true.shape[0]\n",
    "    print(\"   affnist acc_valid: {:.3f}\".format(total_correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9980eb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z = model.forward_wohead(x.permute(0,2,3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8487e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ca0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.min(), z.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5939679b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
