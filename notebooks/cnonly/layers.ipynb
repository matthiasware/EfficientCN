{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2875cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b02d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765215ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "#\n",
    "\n",
    "from misc.plot_utils import plot_mat, imshow\n",
    "from effcn.layers import FCCaps, FCCapsWOBias, Squash\n",
    "from misc.utils import count_parameters\n",
    "from effcn.functions import margin_loss\n",
    "from datasets import AffNIST\n",
    "#\n",
    "from einops import rearrange, repeat\n",
    "from torch import einsum, nn\n",
    "#\n",
    "import helpers\n",
    "#\n",
    "# local imports\n",
    "from datasets import AffNIST\n",
    "from effcn.layers import Squash\n",
    "from effcn.functions import margin_loss, max_norm_masking\n",
    "from misc.utils import count_parameters\n",
    "from misc.plot_utils import plot_couplings, plot_capsules, plot_mat, plot_mat2\n",
    "from misc.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6920ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./../..\")\n",
    "\n",
    "# standard lib\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# external imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# local imports\n",
    "from datasets.csprites import ClassificationDataset\n",
    "from effcn.layers import Squash\n",
    "from effcn.functions import margin_loss, max_norm_masking\n",
    "from misc.utils import count_parameters\n",
    "from misc.plot_utils import plot_couplings, plot_capsules, plot_mat, plot_mat2\n",
    "from misc.metrics import *\n",
    "from misc.utils import normalize_transform, inverse_normalize_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b3c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b858aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:1\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b67e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tanh_embedding(h, w, t_freq = 2, t_symm = 0.5, scale=True):\n",
    "    pe = torch.zeros(4, h, w)\n",
    "    pe[0] = (1 - torch.tanh(t_freq * (torch.linspace(0, 1, w) - t_symm)).unsqueeze(1).repeat(1, h)) * 0.5\n",
    "    pe[1] = (1 - torch.tanh(t_freq * (torch.linspace(1, 0, w) - t_symm)).unsqueeze(1).repeat(1, h)) * 0.5\n",
    "    pe[2] = (1 - torch.tanh(t_freq * (torch.linspace(0, 1, h) - t_symm)).T.repeat(w, 1)) * 0.5\n",
    "    pe[3] = (1 - torch.tanh(t_freq * (torch.linspace(1, 0, h) - t_symm)).T.repeat(w, 1)) * 0.5\n",
    "    if scale:\n",
    "        pe = (pe - pe.min()) / (pe.max() - pe.min()) \n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c3fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = pos_tanh_embedding(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af790d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.shape, pe.min(), pe.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "b, h, w, c = 2, 28, 28, 3\n",
    "#\n",
    "x = torch.rand(b, c, h, w)\n",
    "#\n",
    "pe = pos_tanh_embedding(h, w)\n",
    "\n",
    "# repeat stuff\n",
    "pe = pe.unsqueeze(0).repeat(b, 1, 1, 1)\n",
    "x = torch.cat([x, pe], dim=1)\n",
    "#\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b077b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.reshape(b, 4, h*w).permute(0,2,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff71137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, pi\n",
    "from einops import rearrange, repeat\n",
    "from torch import einsum, nn\n",
    "\n",
    "def fourier_encode(x, max_freq, num_bands=4, base=2):\n",
    "    x = x.unsqueeze(-1)\n",
    "    device, dtype, orig_x = x.device, x.dtype, x\n",
    "\n",
    "    scales = torch.logspace(1.,\n",
    "                            log(max_freq / 2) / log(base),\n",
    "                            num_bands,\n",
    "                            base=base,\n",
    "                            device=device,\n",
    "                            dtype=dtype)\n",
    "    scales = scales[(*((None,) * (len(x.shape) - 1)), Ellipsis)]\n",
    "\n",
    "    x = x * scales * pi\n",
    "    x = torch.cat([x.sin(), x.cos()], dim=-1)\n",
    "    x = torch.cat((x, orig_x), dim=-1)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def pos_embedding_fourier(data, max_freq=10, num_bands=2, base=2):\n",
    "    b, *axis, _ = data.shape\n",
    "    axis_pos = list(map(lambda size: torch.linspace(-1., 1., steps=size), axis))\n",
    "    pos = torch.stack(torch.meshgrid(*axis_pos), dim=-1)\n",
    "    enc_pos = fourier_encode(\n",
    "        pos,\n",
    "        max_freq,\n",
    "        num_bands,\n",
    "        base)\n",
    "    enc_pos = rearrange(enc_pos, '... n d -> ... (n d)')\n",
    "    enc_pos = repeat(enc_pos, '... -> b ...', b=b)\n",
    "    return enc_pos\n",
    "\n",
    "def pos_fourier_embedding(h, w, max_freq=10, num_bands=2, base=2):\n",
    "    axis = (h, w)\n",
    "    axis_pos = list(map(lambda size: torch.linspace(-1., 1., steps=size), axis))\n",
    "    pos = torch.stack(torch.meshgrid(*axis_pos), dim=-1)\n",
    "    enc_pos = fourier_encode(\n",
    "        pos,\n",
    "        max_freq,\n",
    "        num_bands,\n",
    "        base)\n",
    "    enc_pos = rearrange(enc_pos, '... n d -> ... (n d)')\n",
    "    return enc_pos\n",
    "\n",
    "\n",
    "def pos_linear_embedding(h, w):\n",
    "    # linear\n",
    "    pe = torch.zeros(4, h, w)\n",
    "    pe[0] = torch.linspace(0, 1, w).unsqueeze(1).repeat(1, h)\n",
    "    pe[1] = torch.linspace(1, 0, w).unsqueeze(1).repeat(1, h)\n",
    "    pe[2] = torch.linspace(0, 1, h).T.repeat(w, 1)\n",
    "    pe[3] = torch.linspace(1, 0, h).T.repeat(w, 1)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539c4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_mask_1_out(b, n_q):\n",
    "    mask = torch.ones(b, n_q)\n",
    "    for b_idx in range(b):\n",
    "        mask[b_idx][np.random.randint(n_q)] = 0\n",
    "    return mask.bool()\n",
    "\n",
    "def rand_mask(b, d, p_masked=0.2):\n",
    "    mask = torch.FloatTensor(b, d).uniform_() > p_masked\n",
    "    if not torch.all(mask.sum(dim=1) != 0):\n",
    "        mask = rand_mask(b, d, p_masked)\n",
    "    return mask\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_q, d_kv, n_heads, d_head, dropout=0.0, scale=None, d_out=None):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_inner = d_head * n_heads\n",
    "        self.d_out = d_out\n",
    "        if d_out is None:\n",
    "            self.d_out = d_q\n",
    "        if scale is None:\n",
    "            self.scale = d_head**-0.5\n",
    "        #\n",
    "        self.to_q = nn.Linear(d_q, self.d_inner, bias=False)\n",
    "        self.to_kv = nn.Linear(d_kv, self.d_inner * 2, bias=False)\n",
    "        #\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(self.d_inner, self.d_out),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_q, x_kv, mask=None):\n",
    "        \"\"\"\n",
    "            mask (b, d_kv): False if input should be ignored\n",
    "        \"\"\"\n",
    "        h = self.n_heads\n",
    "        q = self.to_q(x_q)\n",
    "        k, v = self.to_kv(x_kv).chunk(2, dim=-1)\n",
    "        #\n",
    "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> (b h) n d\", h=h),(q, k, v))\n",
    "        sim = einsum(\"b i d, b j d -> b i j\", q, k) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = rearrange(mask, \"b ... -> b (...)\")\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, \"b j -> (b h) () j\", h=h)\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "        attn = sim.softmax(dim=1)\n",
    "        out = einsum(\"b i j, b j d -> b i d\", attn, v)\n",
    "        out = rearrange(out, \"(b h) n d -> b n (h d)\", h=h)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43381af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_template(n, n_masked=None, p_masked=None):\n",
    "    if n_masked is None:\n",
    "        assert p_masked > 0\n",
    "        n_masked = int(p_masked * n)\n",
    "    else:\n",
    "        assert n_masked > 0\n",
    "    mask_temp = torch.ones(n)\n",
    "    mask_temp[:n_masked] = 0\n",
    "    return mask_temp.bool()\n",
    "\n",
    "def batch_mask_generator(b, n, n_masked=None, p_masked=None):\n",
    "    mask_temp = mask_template(n, n_masked, p_masked)\n",
    "    mask_temp = mask_temp.unsqueeze(0).repeat(b, 1)\n",
    "    \n",
    "    def _mask_generator():\n",
    "        indices = torch.argsort(torch.rand(*mask_temp.shape), dim=-1)\n",
    "        mask = mask_temp[torch.arange(mask_temp.shape[0]).unsqueeze(-1), indices]\n",
    "        return mask \n",
    "    return _mask_generator\n",
    "\n",
    "def mask_generator(b, n, n_masked=None, p_masked=None):\n",
    "    mask_temp = mask_template(n, n_masked, p_masked)\n",
    "    mask_temp = mask_temp.unsqueeze(0).repeat(b, 1)\n",
    "    #\n",
    "    indices = torch.argsort(torch.rand(*mask_temp.shape), dim=-1)\n",
    "    mask = mask_temp[torch.arange(mask_temp.shape[0]).unsqueeze(-1), indices]\n",
    "    return mask\n",
    "\n",
    "def masked_select(x, mask):\n",
    "    b, _, d = x.shape\n",
    "    assert len(x.shape) == 1 + len(mask.shape)\n",
    "    mask = mask.unsqueeze(-1)\n",
    "    return torch.masked_select(x, mask).reshape(b, -1, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19d89da",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 2\n",
    "d_q = 3\n",
    "d_kv = 4\n",
    "n_heads = 1\n",
    "d_head = 6\n",
    "dropout = 0.0\n",
    "#\n",
    "n_q = 7\n",
    "n_kv = 8\n",
    "#\n",
    "model = Attention(\n",
    "    d_q = d_q,\n",
    "    d_kv = d_kv,\n",
    "    n_heads = n_heads,\n",
    "    d_head = d_head,\n",
    "    dropout = dropout\n",
    ")\n",
    "#\n",
    "x_q = torch.rand(b, n_q, d_q)\n",
    "x_kv = torch.rand(b, n_kv, d_kv)\n",
    "#\n",
    "y = model.forward(x_q, x_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df55f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d76fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_q = torch.rand(1, n_q, d_q)\n",
    "x_kv = torch.rand(b, n_kv, d_kv)\n",
    "#\n",
    "y = model.forward(x_q, x_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e943957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEGLU(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim=-1)\n",
    "        return x * F.gelu(gates)\n",
    "    \n",
    "class CapsuleFeedForward(nn.Module):\n",
    "    def __init__(self, n, d, dropout=0., mult=4):\n",
    "        super().__init__()\n",
    "        self.W1 = torch.nn.Parameter(\n",
    "            torch.rand(n, d, d * mult),\n",
    "            requires_grad=True)\n",
    "        self.W2 = torch.nn.Parameter(\n",
    "            torch.rand(n, d * mult, d),\n",
    "            requires_grad=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        #self.geglu = GEGLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.einsum(\"nij,bni->bnj\", self.W1, x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.geglu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.einsum(\"nji,bnj->bni\", self.W2, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea45c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 2\n",
    "n = 3\n",
    "d = 4\n",
    "mult = 2\n",
    "dropout = 0.\n",
    "#\n",
    "model = CapsuleFeedForward(n=n, d=d, dropout=dropout, mult=mult)\n",
    "#\n",
    "x = torch.rand(b, n, d)\n",
    "y = model(x)\n",
    "#\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf2fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d, mult=4, dropout=0., d_out=None):\n",
    "        super().__init__()\n",
    "        if d_out is None:\n",
    "            d_out = d\n",
    "        self.net = nn.Sequential(nn.Linear(d, d * mult),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(dropout),\n",
    "                                 nn.Linear(d * mult, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f07639f",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34467334",
   "metadata": {},
   "outputs": [],
   "source": [
    "b, h, w, c = 2, 32, 32, 3\n",
    "n_masked = 128\n",
    "#\n",
    "pe = pos_tanh_embedding(h, w)\n",
    "pe = pe.unsqueeze(0).repeat(b, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(b, c, h, w)\n",
    "x_emb = torch.cat([x, pe], dim=1)\n",
    "\n",
    "print(x_emb.shape)\n",
    "x_emb = x_emb.permute(0,2,3,1).reshape(b, h*w,-1)\n",
    "print(x_emb.shape)\n",
    "_, n, d = x_emb.shape\n",
    "#\n",
    "mask = mask_generator(b, h*w, n_masked=n_masked)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9244c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = mask.reshape(b, h, w)\n",
    "#\n",
    "plt.imshow(m[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925ce587",
   "metadata": {},
   "outputs": [],
   "source": [
    "lay_at_up = [\n",
    "    Attention(d_q = 32, d_kv = 7, n_heads = 1, d_head = 32,dropout = 0),\n",
    "    Attention(d_q = 32, d_kv = 32, n_heads = 1, d_head = 32, dropout = 0)\n",
    "]\n",
    "lay_at_down = [\n",
    "    Attention(d_q = 32, d_kv=32, n_heads = 1, d_head = 32, dropout = 0),\n",
    "    Attention(d_q = 4, d_kv=32, n_heads = 1, d_head = 32, d_out=32, dropout = 0),\n",
    "]\n",
    "lay_ff_up = [\n",
    "    CapsuleFeedForward(n=64, d=32, dropout=0, mult=4),\n",
    "    CapsuleFeedForward(n=64, d=32, dropout=0, mult=4)\n",
    "]\n",
    "lay_ff_down = [\n",
    "    CapsuleFeedForward(n=64, d=32, dropout=0, mult=4),\n",
    "    FeedForward(d=32, dropout=0, mult=4, d_out=c)\n",
    "]\n",
    "LQS = [\n",
    "    torch.rand(1, 64, 32),\n",
    "    torch.rand(1, 64, 32),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1789d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "zq = pe.permute(0,2,3,1).reshape(b, h*w,-1)\n",
    "\n",
    "z = x_emb\n",
    "print(z.shape)\n",
    "zm = masked_select(zq, ~mask)\n",
    "xm = masked_select(x.permute(0,2,3,1).reshape(-1,h*w,c), ~mask)\n",
    "#\n",
    "z = lay_at_up[0](LQS[0], z)\n",
    "z = lay_ff_up[0](z)\n",
    "z = lay_at_up[1](LQS[1], z)\n",
    "z = lay_ff_up[1](z)\n",
    "#\n",
    "z = lay_at_down[0](LQS[0], z)\n",
    "z = lay_ff_down[0](z)\n",
    "z = lay_at_down[1](zm, z)\n",
    "z = lay_ff_down[1](z)\n",
    "#\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5dbfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EchoModel(nn.Module):\n",
    "    def __init__(self, n_h, d_h, d_in, d_e, c):\n",
    "        super().__init__()\n",
    "        #\n",
    "        self.lay_at_up = nn.ModuleList([\n",
    "            Attention(d_q = d_h, d_kv = d_in, n_heads = 1, d_head = d_h, dropout = 0.1),\n",
    "            Attention(d_q = d_h, d_kv = d_h, n_heads = 1, d_head = d_h, dropout = 0.1),\n",
    "            Attention(d_q = d_h, d_kv = d_h, n_heads = 1, d_head = d_h, dropout = 0.1),\n",
    "        ])\n",
    "        self.lay_at_down = nn.ModuleList([\n",
    "            Attention(d_q = d_h, d_kv=d_h, n_heads = 1, d_head = d_h, dropout = 0.1),\n",
    "            Attention(d_q = d_h, d_kv=d_h, n_heads = 1, d_head = d_h, dropout = 0.1),\n",
    "            Attention(d_q = d_e, d_kv=d_h, n_heads = 1, d_head = d_h, d_out=d_h, dropout = 0),\n",
    "        ])\n",
    "        self.lay_ff_up = nn.ModuleList([\n",
    "            FeedForward(d=d_h, dropout=0, mult=4, d_out=d_h),\n",
    "            FeedForward(d=d_h, dropout=0, mult=4, d_out=d_h),\n",
    "            FeedForward(d=d_h, dropout=0, mult=4, d_out=d_h),\n",
    "            #CapsuleFeedForward(n=n_h, d=d_h, dropout=0.1, mult=4),\n",
    "            #CapsuleFeedForward(n=n_h, d=d_h, dropout=0.1, mult=4),\n",
    "            #CapsuleFeedForward(n=n_h, d=d_h, dropout=0.1, mult=4)\n",
    "        ])\n",
    "        self.lay_ff_down = nn.ModuleList([\n",
    "            #CapsuleFeedForward(n=n_h, d=d_h, dropout=0.1, mult=4),\n",
    "            #CapsuleFeedForward(n=n_h, d=d_h, dropout=0.1, mult=4),\n",
    "            FeedForward(d=d_h, dropout=0, mult=4, d_out=d_h),\n",
    "            FeedForward(d=d_h, dropout=0, mult=4, d_out=d_h),\n",
    "            FeedForward(d=d_h, dropout=0, mult=4, d_out=c)\n",
    "        ])\n",
    "        self.LQS = nn.ParameterList([\n",
    "            nn.Parameter(torch.rand(1, n_h, d_h), requires_grad=True),\n",
    "            nn.Parameter(torch.rand(1, n_h, d_h), requires_grad=True),\n",
    "            nn.Parameter(torch.rand(1, n_h, d_h), requires_grad=True),\n",
    "        ])\n",
    "        self.to_out = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, z, zq):\n",
    "        \"\"\"\n",
    "            In:\n",
    "                z  ... Embedding (b, n, d)\n",
    "                zq ... PE Query  (b, m, dq)\n",
    "            Out: z\n",
    "        \"\"\"\n",
    "        # UP\n",
    "        z = self.lay_at_up[0](self.LQS[0], z)\n",
    "        z = self.lay_ff_up[0](z)\n",
    "        z = self.lay_at_up[1](self.LQS[1], z)\n",
    "        z = self.lay_ff_up[1](z)\n",
    "        z = self.lay_at_up[2](self.LQS[2], z)\n",
    "        z = self.lay_ff_up[2](z)\n",
    "        \n",
    "        # DOWN\n",
    "        z = self.lay_at_down[0](self.LQS[1], z)\n",
    "        z = self.lay_ff_down[0](z)\n",
    "        z = self.lay_at_down[1](self.LQS[0], z)\n",
    "        z = self.lay_ff_down[1](z)\n",
    "        z = self.lay_at_down[2](zq, z)\n",
    "        z = self.lay_ff_down[2](z)\n",
    "        \n",
    "        # OUT\n",
    "        z = self.to_out(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e3fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "b, h, w, c = 2, 32, 32, 3\n",
    "n_masked = 256\n",
    "#\n",
    "n_h = 32\n",
    "d_h = 16\n",
    "d_in = 7\n",
    "d_e = 4\n",
    "\n",
    "# MASK GENERATOR\n",
    "gen_mask = batch_mask_generator(b, h*w, n_masked=n_masked)\n",
    "\n",
    "# POSTIONAL ENCODING\n",
    "pe = pos_tanh_embedding(h, w)\n",
    "pe = pe.unsqueeze(0).repeat(b, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00809f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT [for each step]\n",
    "mask = gen_mask()\n",
    "\n",
    "x = torch.rand(b, c, h, w)\n",
    "x_emb = torch.cat([x, pe], dim=1)\n",
    "#\n",
    "x_tar = masked_select(x.permute(0,2,3,1).reshape(b,h*w,c), ~mask)\n",
    "x_que = masked_select(pe.permute(0,2,3,1).reshape(b,h*w,-1), ~mask)\n",
    "x_inp = masked_select(x_emb.permute(0,2,3,1).reshape(b,h*w,-1), mask)\n",
    "#\n",
    "print(x_tar.shape)\n",
    "print(x_que.shape)\n",
    "print(x_inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46a143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EchoModel(n_h=n_h, d_h=d_h, d_in=d_in, d_e=d_e, c=c)\n",
    "print(\"#params: {}\".format(count_parameters(model)))\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8175c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pre = model.forward(x_inp, x_que)\n",
    "x_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_pre.min(), x_pre.max(), x_pre.shape)\n",
    "print(x_tar.min(), x_tar.max(), x_tar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421aa2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.functional.mse_loss(x_pre, x_tar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82934156",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329cbbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# black background\n",
    "p_data = '/mnt/data/csprites/single_csprites_32x32_n7_c24_a12_p6_s2_bg_1_constant_color_145152'\n",
    "\n",
    "# structured background\n",
    "#p_data = '/mnt/data/csprites/single_csprites_32x32_n7_c24_a12_p6_s2_bg_inf_random_function_145152'\n",
    "p_data = '/home/matthias/projects/data/single_csprites_32x32_n7_c24_a12_p6_s2_bg_1_constant_color_145152'\n",
    "#p_data = '/home/matthias//projects/data/single_csprites_32x32_n7_c24_a12_p6_s2_bg_inf_random_function_145152'\n",
    "\n",
    "p_ds_config = Path(p_data) / \"config.pkl\"\n",
    "with open(p_ds_config, \"rb\") as file:\n",
    "    ds_config = pickle.load(file)\n",
    "target_variable = \"shape\"\n",
    "target_idx = [idx for idx, target in enumerate(ds_config[\"classes\"]) if target == target_variable][0]\n",
    "n_classes = ds_config[\"n_classes\"][target_variable]\n",
    "#\n",
    "norm_transform = normalize_transform(ds_config[\"means\"],\n",
    "                               ds_config[\"stds\"])\n",
    "#\n",
    "target_transform = lambda x: x[target_idx]\n",
    "transform = T.Compose(\n",
    "    [T.ToTensor(),\n",
    "     #norm_transform,\n",
    "    ])\n",
    "inverse_norm_transform = inverse_normalize_transform(\n",
    "    ds_config[\"means\"],\n",
    "    ds_config[\"stds\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aadadcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "batch_size = 512\n",
    "num_workers = 4\n",
    "#\n",
    "ds_train = ClassificationDataset(\n",
    "    p_data = p_data,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform,\n",
    "    split=\"train\"\n",
    ")\n",
    "dl_train = DataLoader(\n",
    "    ds_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    ")\n",
    "# VALID\n",
    "ds_valid = ClassificationDataset(\n",
    "    p_data = p_data,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform,\n",
    "    split=\"valid\"\n",
    ")\n",
    "dl_valid = DataLoader(\n",
    "    ds_valid,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers = num_workers,\n",
    "    pin_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7619e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vis = 64\n",
    "x,y = next(iter(dl_train))\n",
    "x = x[:n_vis]\n",
    "y = y[:n_vis]\n",
    "#\n",
    "print(x.min(), x.max())\n",
    "#\n",
    "#x = inverse_norm_transform(x)\n",
    "#\n",
    "grid_img = torchvision.utils.make_grid(x, nrow=int(np.sqrt(n_vis)))\n",
    "plt.imshow(grid_img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_masked = 256\n",
    "h = w = 32\n",
    "b = batch_size\n",
    "n_h = 256\n",
    "d_h = 64\n",
    "d_e = 4\n",
    "c = 3\n",
    "d_in = d_e + c\n",
    "\n",
    "# MASK GENERATOR\n",
    "gen_mask = batch_mask_generator(b, h*w, n_masked=n_masked)\n",
    "\n",
    "# POSTIONAL ENCODING\n",
    "# TANH\n",
    "pe = pos_tanh_embedding(h, w)\n",
    "pe = pe.unsqueeze(0).repeat(b, 1, 1, 1)\n",
    "\n",
    "# FOURIER\n",
    "pe = pos_fourier_embedding(h, w, max_freq=10, num_bands=4, base=2)\n",
    "pe = pe.unsqueeze(0).repeat(b, 1, 1, 1).permute(0, 3, 1, 2)\n",
    "\n",
    "# LINEAR\n",
    "pe = pos_linear_embedding(h, w)\n",
    "pe = pe.unsqueeze(0).repeat(b, 1, 1, 1)\n",
    "\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd293b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EchoModel(n_h=n_h, d_h=d_h, d_in=d_in, d_e=d_e, c=c)\n",
    "model = model.to(device)\n",
    "print(\"#params: {}\".format(count_parameters(model)))\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fddf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.96)\n",
    "#\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae507ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################\n",
    "# overfit\n",
    "# #################\n",
    "b = 1\n",
    "\n",
    "# MASK GENERATOR\n",
    "gen_mask = batch_mask_generator(b, h*w, n_masked=n_masked)\n",
    "\n",
    "# POSTIONAL ENCODING\n",
    "pe = pos_linear_embedding(h, w)\n",
    "pe = pe.unsqueeze(0).repeat(b, 1, 1, 1)\n",
    "\n",
    "x, _ = next(iter(dl_train))\n",
    "x = x[:b]\n",
    "x = x.to(device)\n",
    "num_epochs = 10001\n",
    "pe = pe.to(device)\n",
    "#\n",
    "for epoch_idx in range(num_epochs):\n",
    "    model.train()\n",
    "    #\n",
    "    \n",
    "    mask = gen_mask()\n",
    "    mask = mask.to(device)\n",
    "    x_emb = torch.cat([x, pe], dim=1)\n",
    "    #\n",
    "    x_tar = masked_select(x.permute(0,2,3,1).reshape(b,h*w,c), ~mask)\n",
    "    x_que = masked_select(pe.permute(0,2,3,1).reshape(b,h*w,-1), ~mask)\n",
    "    x_inp = masked_select(x_emb.permute(0,2,3,1).reshape(b,h*w,-1), mask)\n",
    "    \n",
    "    optimizer.zero_grad()    \n",
    "    x_pre = model.forward(x_inp, x_que)\n",
    "    loss = loss_fn(x_pre, x_tar)\n",
    "\n",
    "    loss.backward()\n",
    "        \n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch_idx % 100 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1572b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.cpu()\n",
    "x_pre = x_pre.detach().cpu()\n",
    "x_tar = x_tar.cpu()\n",
    "x_que = x_que.cpu()\n",
    "x_emb = x_emb.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b60ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vis = 10\n",
    "n_vis = min(x.shape[0], n_vis)\n",
    "for idx in range(n_vis):\n",
    "    x_ori = x[idx].permute(1,2,0)\n",
    "    m = mask[idx]\n",
    "    x_rec = torch.clone(x_ori).reshape(h*w, -1)\n",
    "    x_rec[~m] = x_pre[idx]\n",
    "    x_rec = x_rec.reshape(h,w,c)\n",
    "    x_mask = torch.zeros(h*w)\n",
    "    x_mask[~m] = 1\n",
    "    x_mask = x_mask.reshape(h, w)\n",
    "    \n",
    "    # DIFF\n",
    "    diff = torch.abs(x_ori.reshape(h*w,-1)[~m] - x_pre[idx])\n",
    "    x_diff = torch.zeros(h * w, c)\n",
    "    x_diff[~m] = diff\n",
    "    x_diff = x_diff.reshape(h, w, c)\n",
    "    #\n",
    "    fig, axes = plt.subplots(1,4,figsize=(20, 5))\n",
    "    axes[0].imshow(x_ori)\n",
    "    axes[1].imshow(x_mask)\n",
    "    axes[2].imshow(x_rec)\n",
    "    axes[3].imshow(x_diff)\n",
    "    #axes[1].imshow()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ce303",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 11\n",
    "pe = pe.to(device)\n",
    "#\n",
    "for epoch_idx in range(num_epochs):\n",
    "    # ####################\n",
    "    # TRAIN\n",
    "    # ####################\n",
    "    model.train()\n",
    "    desc = \"Train [{:3}/{:3}]:\".format(epoch_idx, num_epochs)\n",
    "    pbar = tqdm(dl_train, bar_format=desc + '{bar:10}{r_bar}{bar:-10b}')\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    steps = 0n_vis = 8\n",
    "    for x,_ in pbar:\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # INPUT AND TARGET\n",
    "        mask = gen_mask()\n",
    "        mask = mask.to(device)\n",
    "        x_emb = torch.cat([x, pe], dim=1)\n",
    "        #\n",
    "        x_tar = masked_select(x.permute(0,2,3,1).reshape(b,h*w,c), ~mask)\n",
    "        x_que = masked_select(pe.permute(0,2,3,1).reshape(b,h*w,-1), ~mask)\n",
    "        x_inp = masked_select(x_emb.permute(0,2,3,1).reshape(b,h*w,-1), mask)\n",
    "        \n",
    "        x_pre = model.forward(x_inp, x_que)\n",
    "        \n",
    "        loss = loss_fn(x_pre, x_tar)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizn_vis = 8er.step()\n",
    "        epoch_loss += loss.item()\n",
    "        steps += 1\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "                {'loss': loss.item(),\n",
    "                 'epoch': epoch_loss / steps\n",
    "                }\n",
    "        )\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e60906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,_ = next(iter(dl_valid))\n",
    "#\n",
    "x = x.to(device)\n",
    "        \n",
    "# INPUT AND TARGET\n",
    "mask = gen_mask()\n",
    "mask = mask.to(device)\n",
    "x_emb = torch.cat([x, pe], dim=1)\n",
    "\n",
    "x_tar = masked_select(x.permute(0,2,3,1).reshape(b,h*w,c), ~mask).cpu()\n",
    "x_que = masked_select(pe.permute(0,2,3,1).reshape(b,h*w,-1), ~mask).cpu()\n",
    "x_inp = masked_select(x_emb.permute(0,2,3,1).reshape(b,h*w,-1), mask).cpu()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_pre = model.forward(x_inp.to(device), x_que.to(device)).cpu()\n",
    "\n",
    "x = x.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49a2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_tar.min(), x_tar.max())\n",
    "print(x_que.min(), x_que.max())\n",
    "print(x_inp.min(), x_inp.max())\n",
    "print(x_pre.min(), x_pre.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83d6b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vis = 8\n",
    "for idx in range(n_vis):\n",
    "    x_ori = x[idx].permute(1,2,0)\n",
    "    m = mask[idx]\n",
    "    x_rec = torch.clone(x_ori).reshape(h*w, -1)\n",
    "    x_rec[~m] = x_pre[idx]\n",
    "    x_rec = x_rec.reshape(h,w,c)\n",
    "    x_mask = torch.zeros(h*w)\n",
    "    x_mask[~m] = 1\n",
    "    x_mask = x_mask.reshape(h, w)\n",
    "    \n",
    "    # DIFF\n",
    "    diff = torch.abs(x_ori.reshape(h*w,-1)[~m] - x_pre[idx])\n",
    "    x_diff = torch.zeros(h * w, c)\n",
    "    x_diff[~m] = diff\n",
    "    x_diff = x_diff.reshape(h, w, c)\n",
    "    #\n",
    "    fig, axes = plt.subplots(1,4,figsize=(20, 5))\n",
    "    axes[0].imshow(x_ori)\n",
    "    axes[1].imshow(x_mask)\n",
    "    axes[2].imshow(x_rec)\n",
    "    axes[3].imshow(x_diff)\n",
    "    #axes[1].imshow()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84e0a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.abs(x_ori.reshape(h*w,-1)[~m] - x_pre[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2550ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_diff[~m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8725504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rec[~m] = x_pre[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b46dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rec = torch.clone(x_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73769644",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce824820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
