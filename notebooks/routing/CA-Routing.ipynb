{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95896ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_l = 5\n",
    "n_h = 3\n",
    "C = torch.rand(n_l, n_h)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb40e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.Tensor([\n",
    "    [1, 0.1, 0.1],\n",
    "    [0.3, 0.3, 0.3],\n",
    "    [0.1, 0.5, 0.5],\n",
    "    [0.1, 0.1, 0.9],\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b746b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_c = torch.softmax(C / (1/n_l), dim=0)\n",
    "C_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32159005",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_r = torch.softmax(C / (1/n_h), dim=1)\n",
    "C_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d90e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = C_c * C_r\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c6fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "C.min(), C.max(), C.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bceb144",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, mult=4, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(dim, dim * mult * 2),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(dropout),\n",
    "                                 nn.Linear(dim * mult, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class CrossAttention1(nn.Module):\n",
    "    def __init__(self, n_z, d_z, d_x, d_i):\n",
    "        super().__init__()\n",
    "        self.z = torch.nn.Parameter(torch.rand(n_z, d_z), requires_grad=True)\n",
    "        self.to_q = nn.Linear(d_z, d_i, bias=False)\n",
    "        self.to_kv = nn.Linear(d_x, d_i * 2, bias=False)\n",
    "        self.scale = d_i**-0.5\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.to_q(self.z)\n",
    "        k, v = self.to_kv(x).chunk(2, dim=-1)\n",
    "        #\n",
    "        S = torch.einsum(\"id, ...jd -> ...ij\", q, k) * self.scale\n",
    "        \n",
    "        A_l = S.softmax(dim=1)\n",
    "        A_h = S.softmax(dim=2)\n",
    "        #\n",
    "        C = A_l * A_h\n",
    "        \n",
    "        # (b,n_h,n_l) (b,n_l,d_i)\n",
    "        out = torch.einsum(\"...ij, ...jk -> ...ik\", C, v)\n",
    "        #\n",
    "        return out\n",
    "\n",
    "def cosine_distance_torch(x1, x2=None, eps=1e-8):\n",
    "    x2 = x1 if x2 is None else x2\n",
    "    \n",
    "    w1 = x1.norm(p=2, dim=1, keepdim=True)\n",
    "    w2 = x2.norm(p=2, dim=1, keepdim=True)\n",
    "    return torch.mm(x1, x2.t()) / (w1 * w2.t()).clamp(min=eps)\n",
    "\n",
    "def pw_cosine_distance(input_a, input_b):\n",
    "    normalized_input_a = torch.nn.functional.normalize(input_a)  \n",
    "    normalized_input_b = torch.nn.functional.normalize(input_b)\n",
    "    res = torch.mm(normalized_input_a, normalized_input_b.T)\n",
    "    #res *= -1 # 1-res without copy\n",
    "    #res += 1\n",
    "    return res\n",
    "    \n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, n_z, d_z, d_x, d_i):\n",
    "        super().__init__()\n",
    "        self.z = torch.nn.Parameter(torch.rand(n_z, d_z), requires_grad=True)\n",
    "        self.to_q = nn.Linear(d_z, d_i, bias=False)\n",
    "        self.to_kv = nn.Linear(d_x, d_i * 2, bias=False)\n",
    "        self.scale = d_i**-0.5\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.to_q(self.z)\n",
    "        k, v = self.to_kv(x).chunk(2, dim=-1)\n",
    "        #\n",
    "        S = torch.einsum(\"id, ...jd -> ...ij\", q, k)\n",
    "        \n",
    "        C = torch.softmax(S / self.scale, dim=1)\n",
    "        \n",
    "        # (b,n_h,n_l) (b,n_l,d_i)\n",
    "        out = torch.einsum(\"...ij, ...jk -> ...ik\", C, v)\n",
    "        #\n",
    "        return q, k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05312e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 1\n",
    "n_z = 2\n",
    "n_x = 3\n",
    "d_z = 4\n",
    "d_x = 5\n",
    "#\n",
    "d_i = 6\n",
    "#\n",
    "x = torch.rand(b, n_x, d_x)\n",
    "model = CrossAttention(n_z, d_z, d_x, d_i)\n",
    "q,k,v = model(x)\n",
    "#\n",
    "S = torch.einsum(\"id, ...jd -> ...ij\", q, k)\n",
    "C = torch.softmax(S, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f8474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = pw_cosine_distance(q, k[0])\n",
    "cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa61f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = nn.CosineSimilarity(dim=2, eps=1e-6)\n",
    "cos(q.unsqueeze(0), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4530a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84577eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1606f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h,l\n",
    "S = torch.Tensor([\n",
    "    [1, 0.1, 0.1],\n",
    "    [0.3, 0.3, 0.5],\n",
    "]) * 4\n",
    "S = S.reshape(1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39161629",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_l = S.softmax(dim=1)\n",
    "C_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ada2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_h = S.softmax(dim=2)\n",
    "#\n",
    "C_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad17cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "        I_l = C_h.sum(axis=1) / C_h.sum(axis=1).sum()\n",
    "        I_h = C_l.sum(axis=2) / C_l.sum(axis=2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e03f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = torch.einsum(\"...i,...j->ij\", I_h, I_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48dad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d12ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "I.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e0276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149ae228",
   "metadata": {},
   "outputs": [],
   "source": [
    "(C_h * C_l).sum(axis=0).softmax(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f820d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "(C_h * C_l).sum(axis=1).softmax(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a52fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 1\n",
    "n_z = 3\n",
    "d_z = 4\n",
    "n_x = 5\n",
    "d_x = 2\n",
    "#\n",
    "d_i = 6\n",
    "#\n",
    "x = torch.rand(b, n_x, d_x)\n",
    "\n",
    "model = CrossAttention(n_z, d_z, d_x, d_i)\n",
    "out, C = model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3779244",
   "metadata": {},
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = torch.einsum(\"id, ...jd -> ...ij\", q, k)\n",
    "A_l = S.softmax(dim=1)\n",
    "A_h = S.softmax(dim=2)\n",
    "#\n",
    "A = A_l * A_h\n",
    "out = torch.einsum(\"...ij, ...jk -> ...ik\", A, v)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 query_dim,\n",
    "                 context_dim=None,\n",
    "                 heads=8,\n",
    "                 dim_head=64,\n",
    "                 dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = helpers.default(context_dim, query_dim)\n",
    "\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Linear(inner_dim, query_dim),\n",
    "                                    nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, x, context=None, mask=None):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        context = helpers.default(context, x)\n",
    "        k, v = self.to_kv(context).chunk(2, dim=-1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> (b h) n d\", h=h),\n",
    "                      (q, k, v))\n",
    "\n",
    "        sim = einsum(\"b i d, b j d -> b i j\", q, k) * self.scale\n",
    "\n",
    "        if helpers.exists(mask):\n",
    "            mask = rearrange(mask, \"b ... -> b (...)\")\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, \"b j -> (b h) () j\", h=h)\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        attn = sim.softmax(dim=1)\n",
    "\n",
    "        out = einsum(\"b i j, b j d -> b i d\", attn, v)\n",
    "        out = rearrange(out, \"(b h) n d -> b n (h d)\", h=h)\n",
    "\n",
    "        return self.to_out(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
