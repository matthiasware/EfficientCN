{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "from torch.nn.modules.loss import _Loss\n",
    "import numpy as np\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spread Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### my interprtation off spread loss\n",
    "\n",
    "\n",
    "def spread_loss(y_pred, y_true, m):\n",
    "\n",
    "    at = torch.zeros(y_true.shape).to(device)\n",
    "    zr = torch.zeros((y_pred.shape[0],y_pred.shape[1]-1)).to(device)\n",
    "\n",
    "    #create at\n",
    "    for i, cl in enumerate(y_true):\n",
    "        at[i] = y_pred[i][cl]\n",
    "    \n",
    "    at = at.unsqueeze(1).repeat(1,y_pred.shape[1])\n",
    "    ai = y_pred[y_pred!=at].view(y_pred.shape[0],-1)\n",
    "\n",
    "    loss = ((torch.max( m-(at[:,:-1] - ai), zr))**2).sum(dim=1)\n",
    "\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpreadLoss(_Loss):\n",
    "\n",
    "    def __init__(self, device, m_min=0.2, m_max=0.9):\n",
    "        super(SpreadLoss, self).__init__()\n",
    "        self.m_min = m_min\n",
    "        self.m_max = m_max\n",
    "        self.device = device\n",
    "\n",
    "    def margin(self, reps):\n",
    "        return self.m_min + (self.m_max - self.m_min)*reps\n",
    "\n",
    "    def forward(self, y_pred, y_true, reps):\n",
    "        at = torch.zeros(y_true.shape).to(self.device)\n",
    "        zr = torch.zeros((y_pred.shape[0],y_pred.shape[1]-1)).to(self.device)\n",
    "        ma = self.margin(reps)\n",
    "\n",
    "        #create at\n",
    "        for i, cl in enumerate(y_true):\n",
    "            at[i] = y_pred[i][cl]\n",
    "        \n",
    "        at = at.unsqueeze(1).repeat(1,y_pred.shape[1])\n",
    "        ai = y_pred[y_pred!=at].view(y_pred.shape[0],-1)\n",
    "\n",
    "        loss = ((torch.max( ma - (at[:,:-1] - ai), zr))**2).sum(dim=1)\n",
    "\n",
    "        # mean over batch\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0286, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\")\n",
    "torch.manual_seed(0)\n",
    "bs = 8\n",
    "y_true = torch.randint(0, 9, (bs,), requires_grad=False).to(device)\n",
    "y_pred = torch.rand(bs,10, requires_grad=True).to(device)\n",
    "spread_loss(y_pred, y_true, 0.2)\n",
    "\n",
    "#y_true.unsqueeze(1).repeat(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0286, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = SpreadLoss(device)\n",
    "A.margin(0)\n",
    "loss = A.forward(y_pred, y_true, 0)\n",
    "\n",
    "#loss.backward()\n",
    "loss\n",
    "\n",
    "#same result as in gitstuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CapsNetEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = datasets.MNIST(root='../../data', train=True, download=True, transform=T.ToTensor())\n",
    "\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, \n",
    "                                        batch_size=1, \n",
    "                                        shuffle=False,\n",
    "                                        num_workers=2)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 28, 28]), tensor([5]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(dl_train))\n",
    "\n",
    "x.shape,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNetEM(nn.Module):\n",
    "    \"\"\"\n",
    "    Genrate CapsNet with EM routing\n",
    "    Args:\n",
    "        A: output channels of normal conv\n",
    "        B: output channels of primary caps\n",
    "        C: output channels of 1st conv caps\n",
    "        D: output channels of 2nd conv caps\n",
    "        E: output channels of class caps (i.e. number of classes)\n",
    "        K: kernel of conv caps\n",
    "        P: size of square pose matrix\n",
    "        iters: number of EM iterations\n",
    "        ...\n",
    "\n",
    "        input: (bs, 1, 28, 28)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, A=32, B=32, C=32, D=32, K=3, P=4, iter=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=A, kernel_size=(5, 5), stride=2, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.BatchNorm2d(num_features=A),\n",
    "        )\n",
    "        self.prim_caps0 = PrimaryCaps(ch_in=A, ch_out=B, K=1, P=P, stride=1, padding=\"valid\")\n",
    "        self.conv_caps1 = ConvCaps(ch_in=B, ch_out=C, K=K, P=P, stride=2, iter=iter, class_caps=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.prim_caps0(x)\n",
    "        x = self.conv_caps1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        A: output of the normal conv layer\n",
    "        B: number of types of capsules\n",
    "        K: kernel size of convolution\n",
    "        P: size of pose matrix is P*P\n",
    "        stride: stride of convolution\n",
    "    Shape:\n",
    "        input:  (*, A, h, w)                (bs, 32, 14, 14)\n",
    "        output: p -> (*,B, h', w', P, P)    (bs, 32, 14, 14, 4, 4)\n",
    "                a -> (*,B, h', w')          (bs, 32, 14, 14)\n",
    "        h', w' is computed the same way as convolution layer\n",
    "        parameter size is: K*K*A*B*P*P + B*P*P\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ch_in=32, ch_out=32, K=1, P=4, stride=1, padding=\"valid\"):\n",
    "        super().__init__()\n",
    "        self.pose = nn.Conv2d(in_channels=ch_in, out_channels=ch_out*P*P, kernel_size=K, stride=stride, bias=True)\n",
    "        self.acti = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=ch_in, out_channels=ch_out, kernel_size=K, stride=stride, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.pose(x)\n",
    "        a = self.acti(x)\n",
    "        p = p.view(p.shape[0],-1,p.shape[2],p.shape[3],4,4)\n",
    "\n",
    "        return p, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvCaps(nn.Module):\n",
    "    \"\"\"Create a convolutional capsule layer\n",
    "    that transfer capsule layer L to capsule layer L+1\n",
    "    by EM routing.\n",
    "    Args:\n",
    "        B: input number of types of capsules\n",
    "        C: output number on types of capsules\n",
    "        K: kernel size of convolution\n",
    "        P: size of pose matrix is P*P\n",
    "        stride: stride of convolution\n",
    "        iters: number of EM iterations\n",
    "        coor_add: use scaled coordinate addition or not\n",
    "        w_shared: share transformation matrix across w*h.\n",
    "    Shape:\n",
    "        input:  (*,B, h,  w, P, P)      (bs, 32, 14, 14, 4, 4)\n",
    "                (*,B, h,  w, 1)         (bs, 32, 14, 14)\n",
    "        output: (*,C, h,  w, P, P)      (bs, 32, 6, 6, 4, 4)\n",
    "                (*,C, h,  w, 1)         (bs, 32, 6, 6)\n",
    "        h', w' is computed the same way as convolution layer\n",
    "        parameter size is: K*K*B*C*P*P + B*P*P\n",
    "    \"\"\"   \n",
    "\n",
    "    def __init__(self, ch_in=32, ch_out=32, K=3, P=4, stride=2, iter=3, class_caps=False):\n",
    "        super().__init__()\n",
    "        # init vars\n",
    "        self.ch_in  = ch_in\n",
    "        self.ch_out = ch_out\n",
    "        self.K = K\n",
    "        self.P = P\n",
    "        self.stride = stride\n",
    "        self.iter = iter\n",
    "        self.class_caps = class_caps\n",
    "\n",
    "        # constants\n",
    "            #actualy none\n",
    "        \n",
    "        # params\n",
    "        self.b_u = nn.Parameter(torch.zeros(ch_out))\n",
    "        self.b_a = nn.Parameter(torch.zeros(ch_out))\n",
    "        self.M   = nn.Parameter(torch.randn(1, K*K*ch_in, ch_out, P, P))\n",
    "\n",
    "        # activations\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #split pose and activation\n",
    "        p, a = x\n",
    "\n",
    "        print(self.M.shape)\n",
    "\n",
    "\n",
    "        return p, a\n",
    "\n",
    "    def em_routing(self):\n",
    "        pass\n",
    "\n",
    "    def e_step(self):\n",
    "        pass\n",
    "\n",
    "    def m_step(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 288, 32, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 32, 14, 14, 4, 4]), torch.Size([1, 32, 14, 14]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = CapsNetEM()\n",
    "\n",
    "z, b = B(x)\n",
    "\n",
    "z.shape, b.shape\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "188faa17072d374bec02d17fca5e544867bade69f71230dfd1a560a6ca303930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('EffCN': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
