{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "from torch.nn.modules.loss import _Loss\n",
    "import numpy as np\n",
    "#local\n",
    "from misc.utils import count_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spread Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "### my interprtation off spread loss\n",
    "\n",
    "\n",
    "def spread_loss(y_pred, y_true, m):\n",
    "\n",
    "    at = torch.zeros(y_true.shape).to(device)\n",
    "    zr = torch.zeros((y_pred.shape[0],y_pred.shape[1]-1)).to(device)\n",
    "\n",
    "    #create at\n",
    "    for i, cl in enumerate(y_true):\n",
    "        at[i] = y_pred[i][cl]\n",
    "    \n",
    "    at = at.unsqueeze(1).repeat(1,y_pred.shape[1])\n",
    "    ai = y_pred[y_pred!=at].view(y_pred.shape[0],-1)\n",
    "\n",
    "    loss = ((torch.max( m-(at[:,:-1] - ai), zr))**2).sum(dim=1)\n",
    "\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpreadLoss(_Loss):\n",
    "\n",
    "    def __init__(self, device, m_min=0.2, m_max=0.9):\n",
    "        super(SpreadLoss, self).__init__()\n",
    "        self.m_min = m_min\n",
    "        self.m_max = m_max\n",
    "        self.device = device\n",
    "\n",
    "    def margin(self, reps):\n",
    "        return self.m_min + (self.m_max - self.m_min)*reps\n",
    "\n",
    "    def forward(self, y_pred, y_true, reps):\n",
    "        at = torch.zeros(y_true.shape).to(self.device)\n",
    "        zr = torch.zeros((y_pred.shape[0],y_pred.shape[1]-1)).to(self.device)\n",
    "        ma = self.margin(reps)\n",
    "\n",
    "        #create at\n",
    "        for i, cl in enumerate(y_true):\n",
    "            at[i] = y_pred[i][cl]\n",
    "        \n",
    "        at = at.unsqueeze(1).repeat(1,y_pred.shape[1])\n",
    "        ai = y_pred[y_pred!=at].view(y_pred.shape[0],-1)\n",
    "\n",
    "        loss = ((torch.max( ma - (at[:,:-1] - ai), zr))**2).sum(dim=1)\n",
    "\n",
    "        # mean over batch\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0286, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\")\n",
    "torch.manual_seed(0)\n",
    "bs = 8\n",
    "y_true = torch.randint(0, 9, (bs,), requires_grad=False).to(device)\n",
    "y_pred = torch.rand(bs,10, requires_grad=True).to(device)\n",
    "spread_loss(y_pred, y_true, 0.2)\n",
    "\n",
    "#y_true.unsqueeze(1).repeat(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0286, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = SpreadLoss(device)\n",
    "A.margin(0)\n",
    "loss = A.forward(y_pred, y_true, 0)\n",
    "\n",
    "#loss.backward()\n",
    "loss\n",
    "\n",
    "#same result as in gitstuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CapsNetEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = datasets.MNIST(root='../../data', train=True, download=True, transform=T.ToTensor())\n",
    "\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, \n",
    "                                        batch_size=1, \n",
    "                                        shuffle=False,\n",
    "                                        num_workers=2)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 28, 28]), tensor([5]))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(dl_train))\n",
    "\n",
    "x.shape,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNetEM(nn.Module):\n",
    "    \"\"\"\n",
    "    Genrate CapsNet with EM routing\n",
    "    Args:\n",
    "        A: output channels of normal conv\n",
    "        B: output channels of primary caps\n",
    "        C: output channels of 1st conv caps\n",
    "        D: output channels of 2nd conv caps\n",
    "        E: output channels of class caps (i.e. number of classes)\n",
    "        K: kernel of conv caps\n",
    "        P: size of square pose matrix\n",
    "        iters: number of EM iterations\n",
    "        ...\n",
    "\n",
    "        input: (bs, 1, 28, 28)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, A=32, B=32, C=32, D=32,E=10, K=3, P=4, iter=3, hw_out=(28,28)):\n",
    "        super().__init__()\n",
    "        hw_out = self.hw_cal(hw_out, kernel=5, padding=2, dilatation=1, stride=2)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=A, kernel_size=(5, 5), stride=2, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.BatchNorm2d(num_features=A),\n",
    "        )\n",
    "        hw_out = self.hw_cal(hw_out, kernel=1, padding=0, dilatation=1, stride=1)\n",
    "        self.prime_caps = PrimaryCaps(ch_in=A, ch_out=B, K=1, P=P, stride=1, padding=\"valid\")\n",
    "        #\n",
    "        hw_out = self.hw_cal(hw_out, kernel=K, padding=0, dilatation=1, stride=2)\n",
    "        self.conv_caps1 = ConvCaps(ch_in=B, ch_out=C, K=K, P=P, stride=2, iter=iter, hw_out=hw_out, class_caps=False)\n",
    "        #\n",
    "        hw_out = self.hw_cal(hw_out, kernel=K, padding=0, dilatation=1, stride=1)\n",
    "        self.conv_caps2 = ConvCaps(ch_in=C, ch_out=B, K=K, P=P, stride=1, iter=iter, hw_out=hw_out, class_caps=False)\n",
    "        #\n",
    "        self.class_caps = ConvCaps(ch_in=D, ch_out=E, K=1, P=P, stride=1, iter=iter, hw_out=hw_out, class_caps=True)\n",
    "\n",
    "    def hw_cal(self, hw_in, kernel, padding=0, dilatation=1, stride=1):\n",
    "        if type(hw_in) == type(int()):\n",
    "            hw_out = math.floor((hw_in + 2*padding - dilatation * (kernel - 1) - 1) / stride + 1)\n",
    "        elif type(hw_in) == type(tuple()):\n",
    "            h_out = math.floor((hw_in[0] + 2*padding - dilatation * (kernel - 1) - 1) / stride + 1)\n",
    "            w_out = math.floor((hw_in[1] + 2*padding - dilatation * (kernel - 1) - 1) / stride + 1)\n",
    "            hw_out = (h_out, w_out)\n",
    "        return hw_out\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.prime_caps(x)\n",
    "        x = self.conv_caps1(x)\n",
    "        x = self.conv_caps2(x)\n",
    "        #x = self.class_caps(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        A: output of the normal conv layer\n",
    "        B: number of types of capsules\n",
    "        K: kernel size of convolution\n",
    "        P: size of pose matrix is P*P\n",
    "        stride: stride of convolution\n",
    "    Shape:\n",
    "        input:  (*, A, h, w)                (bs, 32, 14, 14)\n",
    "        output: p -> (*,B, h', w', P, P)    (bs, 32, 14, 14, 4, 4)\n",
    "                a -> (*,B, h', w')          (bs, 32, 14, 14)\n",
    "        h', w' is computed the same way as convolution layer\n",
    "        parameter size is: K*K*A*B*P*P + B*P*P\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ch_in=32, ch_out=32, K=1, P=4, stride=1, padding=\"valid\"):\n",
    "        super().__init__()\n",
    "        self.pose = nn.Conv2d(in_channels=ch_in, out_channels=ch_out*P*P, kernel_size=K, stride=stride, bias=True)\n",
    "        self.acti = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=ch_in, out_channels=ch_out, kernel_size=K, stride=stride, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.P = P\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.pose(x)\n",
    "        a = self.acti(x)\n",
    "        p = p.view(p.shape[0],-1,p.shape[2],p.shape[3],self.P,self.P)\n",
    "\n",
    "        return p, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvCaps(nn.Module):\n",
    "    \"\"\"Create a convolutional capsule layer\n",
    "    that transfer capsule layer L to capsule layer L+1\n",
    "    by EM routing.\n",
    "    Args:\n",
    "        B: input number of types of capsules\n",
    "        C: output number on types of capsules\n",
    "        K: kernel size of convolution\n",
    "        P: size of pose matrix is P*P\n",
    "        stride: stride of convolution\n",
    "        iters: number of EM iterations\n",
    "        coor_add: use scaled coordinate addition or not\n",
    "        w_shared: share transformation matrix across w*h.\n",
    "    Shape:\n",
    "        input:  (*,B, h,  w, P, P)      (bs, 32, 14, 14, 4, 4)\n",
    "                (*,B, h,  w, 1)         (bs, 32, 14, 14)\n",
    "        output: (*,C, h,  w, P, P)      (bs, 32, 6, 6, 4, 4)\n",
    "                (*,C, h,  w, 1)         (bs, 32, 6, 6)\n",
    "        h', w' is computed the same way as convolution layer\n",
    "        parameter size is: K*K*B*C*P*P + B*P*P\n",
    "    \"\"\"   \n",
    "\n",
    "    def __init__(self, ch_in=32, ch_out=32, K=3, P=4, stride=2, iter=3, hw_out=(1,1), lambd=1e-03, class_caps=False):\n",
    "        super().__init__()\n",
    "        # init vars\n",
    "        self.ch_in  = ch_in\n",
    "        self.ch_out = ch_out\n",
    "        self.K = K\n",
    "        self.P = P\n",
    "        self.psize = P*P\n",
    "        self.stride = stride\n",
    "        self.iter = iter\n",
    "        self.hw_out = hw_out\n",
    "        self.class_caps = class_caps\n",
    "        self.lambd = lambd\n",
    "\n",
    "        # constants\n",
    "            #actualy none\n",
    "        \n",
    "        # params\n",
    "        self.b_u = nn.Parameter(torch.zeros(ch_out), requires_grad=True)\n",
    "        self.b_a = nn.Parameter(torch.zeros(ch_out), requires_grad=True)\n",
    "        self.w = nn.Parameter(torch.rand([1, ch_in, hw_out[0], hw_out[0], P, P, ch_out]), requires_grad=True)\n",
    "\n",
    "        #conv with static kernel\n",
    "        self.conv_stat = nn.Conv2d(in_channels=ch_in, out_channels=ch_in, kernel_size=K, stride=stride, bias=False, padding=0)\n",
    "        self.conv_stat.weight = torch.nn.Parameter((torch.ones_like(self.conv_stat.weight)/K**2),requires_grad=False)\n",
    "\n",
    "        # activations\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def voting(self, x):\n",
    "        \"\"\"\n",
    "            For conv_caps:\n",
    "                Input:     (bs, ch_in, h, w, p, p)\n",
    "                Output:    (bs, ch_in, h, w, p, p, ch_out)\n",
    "            For class_caps:\n",
    "                Input:     (b, H*W*B, P*P)\n",
    "                Output:    (b, H*W*B, C, P*P)\n",
    "        \"\"\"        \n",
    "        sh_in = x.shape\n",
    "        #conv & shaping\n",
    "        x = x.view(sh_in[0]*self.P*self.P, self.ch_in,sh_in[2], sh_in[3])\n",
    "        x = self.conv_stat(x)\n",
    "        x = x.view(-1, x.shape[1], x.shape[2], x.shape[3], self.P,self.P)\n",
    "        \n",
    "        #expand x and w to number of out channels\n",
    "        x = x.unsqueeze(-1).repeat([1, 1, 1, 1, 1, 1, self.ch_out])\n",
    "        w = self.w.repeat([sh_in[0], 1, 1, 1, 1, 1, 1])\n",
    "        \n",
    "        assert x.shape == w.shape\n",
    "\n",
    "        # compute v\n",
    "        v = torch.mul(x, w)\n",
    "        return v\n",
    "\n",
    "    def em_routing(self, v, a):\n",
    "        \"\"\"\n",
    "            Input:\n",
    "                v:         (bs, ch_in, h, w, p, p, ch_out)\n",
    "                a_in:      (bs, ch_in, h, w)\n",
    "            Output:\n",
    "                mu:        (bs, ch_in, h, w, p, p)\n",
    "                a_out:     (bs, ch_in, h, w)\n",
    "            Note that some dimensions are merged\n",
    "            for computation convenient, that is\n",
    "                v:         (bs*h*w, ch_in, p*p, ch_out)\n",
    "                a_in:      (bs*h*w, ch_in, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        #split shapes and reshape\n",
    "        s_bs, s_ch_in, s_h, s_w, s_p1, s_p2, s_ch_out = v.shape \n",
    "\n",
    "        v = v.view(s_bs*s_h*s_w, s_ch_in, s_ch_out, s_p1*s_p2)\n",
    "        a = a.view(s_bs*s_h*s_w, s_ch_in).unsqueeze(-1)\n",
    "\n",
    "        #declare r\n",
    "        r = torch.FloatTensor(s_bs*s_h*s_w, s_ch_in, s_ch_out).fill_(1./s_ch_out)\n",
    "\n",
    "        #iteration\n",
    "        for it in range(self.iter):\n",
    "            # M-Step\n",
    "            a_out, mu, sig_sq = self.m_step(a, r, v)\n",
    "            \n",
    "            # E-Step\n",
    "            if it < self.iter - 1:\n",
    "                r = self.e_step(mu, sig_sq, a_out, v)\n",
    "\n",
    "        #reshape from M and a as output\n",
    "        mu = mu.view(s_bs, s_ch_out, s_h, s_w, s_p1, s_p2)\n",
    "        a_out = a_out.view(s_bs, s_ch_out, s_h, s_w)\n",
    "\n",
    "        return mu, a_out\n",
    "\n",
    "    def e_step(self, mu, sig_sq, a_out, v):\n",
    "        \"\"\"\n",
    "            ln_p_j = sum_h \\dfrac{(\\V^h_{ij} - \\mu^h_j)^2}{2 \\sigma^h_j}\n",
    "                    - sum_h ln(\\sigma^h_j) - 0.5*\\sum_h ln(2*\\pi)\n",
    "            r = softmax(ln(a_j*p_j))\n",
    "              = softmax(ln(a_j) + ln(p_j))\n",
    "            Input:\n",
    "                mu:        (bs*h*w, 1, ch_out, P*P)\n",
    "                sig_sq:    (bs*h*w, 1, ch_out, P*P)\n",
    "                a_out:     (bs*h*w, 1, ch_out, 1)\n",
    "                v:         (bs*h*w, ch_in, ch_out, p*p)\n",
    "            Local:\n",
    "                p_ln:  (bs*h*w, ch_in, ch_out, p*p)\n",
    "                ap_ln:     (bs*h*w, ch_in, ch_out, 1)\n",
    "            Output:\n",
    "                r:         (bs*h*w, ch_in, ch_out)\n",
    "        \"\"\"\n",
    "        p_ln = -1. * (v - mu)**2 / (2 * sig_sq) - torch.log(sig_sq.sqrt()) - 0.5*torch.log(torch.tensor(2*math.pi))\n",
    "        ap_ln = (p_ln.sum(dim=3, keepdim=True) + torch.log(a_out)).squeeze(-1)\n",
    "        r = self.softmax(ap_ln)\n",
    "\n",
    "        return r\n",
    "\n",
    "    def m_step(self, a, r, v):\n",
    "        \"\"\"\n",
    "            \\mu^h_j = \\dfrac{\\sum_i r_{ij} V^h_{ij}}{\\sum_i r_{ij}}\n",
    "            (\\sigma^h_j)^2 = \\dfrac{\\sum_i r_{ij} (V^h_{ij} - mu^h_j)^2}{\\sum_i r_{ij}}\n",
    "            cost_h = (\\beta_u + log \\sigma^h_j) * \\sum_i r_{ij}\n",
    "            a_j = logistic(\\lambda * (\\beta_a - \\sum_h cost_h))\n",
    "            Input:\n",
    "                a_in:      (bs*h*w, ch_in, 1)\n",
    "                r:         (bs*h*w, ch_in, ch_out)\n",
    "                v:         (bs*h*w, ch_in, ch_out, p*p)\n",
    "            Local:\n",
    "                cost_h:    (bs*h*w, 1, ch_out, P*P)\n",
    "                r_sum:     (bs*h*w, 1, ch_out, 1)\n",
    "            Output:\n",
    "                a_out:     (bs*h*w, 1, ch_out, 1)\n",
    "                mu:        (bs*h*w, 1, ch_out, P*P)\n",
    "                sig_sq:    (bs*h*w, 1, ch_out, P*P)\n",
    "        \"\"\"\n",
    "        s_st, s_ch_in, s_ch_out, p = v.shape \n",
    "        r = (r * a).unsqueeze(-1)\n",
    "        r_sum = r.sum(dim=1, keepdim=True)\n",
    "        mu = torch.sum(r * v, dim=1, keepdim=True) / r_sum\n",
    "        sig_sq = torch.sum(r * (v - mu)**2, dim=1, keepdim=True) / r_sum\n",
    "        cost = (self.b_u.view(1,1,s_ch_out,1) + torch.log(sig_sq.sqrt())) * r_sum\n",
    "        a_out = self.lambd*(self.b_a.view(1,1,s_ch_out,1) - cost.sum(dim=3, keepdim=True))\n",
    "        \n",
    "        return a_out, mu, sig_sq\n",
    "\n",
    "    def forward(self, x):\n",
    "        #split pose and activation\n",
    "        x, a = x\n",
    "\n",
    "        # conv of poses to get votes v\n",
    "        x = self.voting(x) \n",
    "        \n",
    "        # conv activations\n",
    "        a = self.conv_stat(a)\n",
    "\n",
    "        #routing\n",
    "        x, a = self.em_routing(x, a)\n",
    "\n",
    "\n",
    "        return x, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "952820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 32, 4, 4, 4, 4]), torch.Size([1, 32, 4, 4]))"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = CapsNetEM()\n",
    "print(count_parameters(B))\n",
    "z, a = B(x)\n",
    "\n",
    "z.shape, a.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pathes(x, B, K, psize, stride):\n",
    "    \"\"\"\n",
    "        Shape:\n",
    "            Input:     (b, H, W, B*(P*P+1))\n",
    "            Output:    (b, H', W', K, K, B*(P*P+1))\n",
    "    \"\"\"\n",
    "    b, h, w, c = x.shape\n",
    "    assert h == w\n",
    "    assert c == B*(psize+1)\n",
    "    oh = ow = int(((h - K )/stride)+ 1) # moein - changed from: oh = ow = int((h - K + 1) / stride)\n",
    "    idxs = [[(h_idx + k_idx) \\\n",
    "            for k_idx in range(0, K)] \\\n",
    "            for h_idx in range(0, h - K + 1, stride)]\n",
    "    print(idxs)\n",
    "    print(x.shape)\n",
    "    x = x[:, idxs, :, :]\n",
    "    print(x.shape)\n",
    "    x = x[:, :, :, idxs, :]\n",
    "    print(x.shape)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "    return x, oh, ow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2], [2, 3, 4], [4, 5, 6], [6, 7, 8], [8, 9, 10], [10, 11, 12]]\n",
      "torch.Size([1, 14, 14, 544])\n",
      "torch.Size([1, 6, 3, 14, 544])\n",
      "torch.Size([1, 6, 3, 6, 3, 544])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 6, 3, 3, 544]), 6, 6)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.randn(1, 14, 14,32*17)\n",
    "B = 32\n",
    "K = 3\n",
    "psize = 16\n",
    "stride = 2\n",
    "\n",
    "q, oh, ow = add_pathes(x=q, B=B, K=K, psize=psize, stride=stride)\n",
    "\n",
    "q.shape, oh, ow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16384, 6, 6])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = z.view(z.shape[0],-1,z.shape[2],z.shape[3])\n",
    "r.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1179904\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, -1, 6, 6, 4, 4]' is invalid for input of size 1024",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_47652/1411800657.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, -1, 6, 6, 4, 4]' is invalid for input of size 1024"
     ]
    }
   ],
   "source": [
    "c_out = 16 * 4 * 4\n",
    "CV =nn.Conv2d(in_channels= r.shape[1], out_channels=c_out, kernel_size=(3,3), stride=2, padding=0,\n",
    " dilation=1, groups=32, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "\n",
    "\n",
    "print(count_parameters(CV))\n",
    "cv = CV(r).view(1,-1,6,6,4,4).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,  16,  32,  48,  64,  80,  96, 112, 128, 144, 160, 176, 192, 208,\n",
       "        224, 240, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432,\n",
       "        448, 464, 480, 496,   1,  17,  33,  49,  65,  81,  97, 113, 129, 145,\n",
       "        161, 177, 193, 209, 225, 241, 257, 273, 289, 305, 321, 337, 353, 369,\n",
       "        385, 401, 417, 433, 449, 465, 481, 497,   2,  18,  34,  50,  66,  82,\n",
       "         98, 114, 130, 146, 162, 178, 194, 210, 226, 242, 258, 274, 290, 306,\n",
       "        322, 338, 354, 370, 386, 402, 418, 434, 450, 466, 482, 498,   3,  19,\n",
       "         35,  51,  67,  83,  99, 115, 131, 147, 163, 179, 195, 211, 227, 243,\n",
       "        259, 275, 291, 307, 323, 339, 355, 371, 387, 403, 419, 435, 451, 467,\n",
       "        483, 499,   4,  20,  36,  52,  68,  84, 100, 116, 132, 148, 164, 180,\n",
       "        196, 212, 228, 244, 260, 276, 292, 308, 324, 340, 356, 372, 388, 404,\n",
       "        420, 436, 452, 468, 484, 500,   5,  21,  37,  53,  69,  85, 101, 117,\n",
       "        133, 149, 165, 181, 197, 213, 229, 245, 261, 277, 293, 309, 325, 341,\n",
       "        357, 373, 389, 405, 421, 437, 453, 469, 485, 501,   6,  22,  38,  54,\n",
       "         70,  86, 102, 118, 134, 150, 166, 182, 198, 214, 230, 246, 262, 278,\n",
       "        294, 310, 326, 342, 358, 374, 390, 406, 422, 438, 454, 470, 486, 502,\n",
       "          7,  23,  39,  55,  71,  87, 103, 119, 135, 151, 167, 183, 199, 215,\n",
       "        231, 247, 263, 279, 295, 311, 327, 343, 359, 375, 391, 407, 423, 439,\n",
       "        455, 471, 487, 503,   8,  24,  40,  56,  72,  88, 104, 120, 136, 152,\n",
       "        168, 184, 200, 216, 232, 248, 264, 280, 296, 312, 328, 344, 360, 376,\n",
       "        392, 408, 424, 440, 456, 472, 488, 504,   9,  25,  41,  57,  73,  89,\n",
       "        105, 121, 137, 153, 169, 185, 201, 217, 233, 249, 265, 281, 297, 313,\n",
       "        329, 345, 361, 377, 393, 409, 425, 441, 457, 473, 489, 505,  10,  26,\n",
       "         42,  58,  74,  90, 106, 122, 138, 154, 170, 186, 202, 218, 234, 250,\n",
       "        266, 282, 298, 314, 330, 346, 362, 378, 394, 410, 426, 442, 458, 474,\n",
       "        490, 506,  11,  27,  43,  59,  75,  91, 107, 123, 139, 155, 171, 187,\n",
       "        203, 219, 235, 251, 267, 283, 299, 315, 331, 347, 363, 379, 395, 411,\n",
       "        427, 443, 459, 475, 491, 507,  12,  28,  44,  60,  76,  92, 108, 124,\n",
       "        140, 156, 172, 188, 204, 220, 236, 252, 268, 284, 300, 316, 332, 348,\n",
       "        364, 380, 396, 412, 428, 444, 460, 476, 492, 508,  13,  29,  45,  61,\n",
       "         77,  93, 109, 125, 141, 157, 173, 189, 205, 221, 237, 253, 269, 285,\n",
       "        301, 317, 333, 349, 365, 381, 397, 413, 429, 445, 461, 477, 493, 509,\n",
       "         14,  30,  46,  62,  78,  94, 110, 126, 142, 158, 174, 190, 206, 222,\n",
       "        238, 254, 270, 286, 302, 318, 334, 350, 366, 382, 398, 414, 430, 446,\n",
       "        462, 478, 494, 510,  15,  31,  47,  63,  79,  95, 111, 127, 143, 159,\n",
       "        175, 191, 207, 223, 239, 255, 271, 287, 303, 319, 335, 351, 367, 383,\n",
       "        399, 415, 431, 447, 463, 479, 495, 511])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test view\n",
    "dd = torch.arange(0,512,1)\n",
    "dd = dd.reshape(-1,4,4)\n",
    "dd.view(-1)\n",
    "\n",
    "cc = dd.clone()\n",
    "#cc =cc.reshape(4,4,-1)\n",
    "cc = cc.permute(1,2,0)\n",
    "cc.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 32, 4]' is invalid for input of size 18432",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_47364/3911184894.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#test view\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#dd.view(-1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, 32, 4]' is invalid for input of size 18432"
     ]
    }
   ],
   "source": [
    "#test view\n",
    "dd = torch.rand(1,32,4,4,6,6)\n",
    "dd = dd.reshape(1,32,4)\n",
    "#dd.view(-1)\n",
    "dd.shape\n",
    "#cc = dd.clone()\n",
    "#cc =cc.reshape(4,4,-1)\n",
    "#cc = cc.permute(1,2,0)\n",
    "#cc.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17408\n",
      "147456\n",
      "147456\n",
      "5120\n",
      "317440\n"
     ]
    }
   ],
   "source": [
    "2359808\n",
    "147968\n",
    "74240\n",
    "\n",
    "4609\n",
    "\n",
    "print(32*32*17)\n",
    "print(9*32*32*16)\n",
    "print(9*32*32*16)\n",
    "print(32*10*16)\n",
    "print(32*32*17 + 9*32*32*16 + 9*32*32*16+ 32*10*16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 288, 32, 4, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.randn(24, 288, 32, 4, 4)\n",
    "j = torch.randn(24, 288, 32, 4, 4)\n",
    "torch.matmul(q,j).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 12, 12])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "nb_channels = 10\n",
    "h, w = 14,14\n",
    "k = 3\n",
    "x = torch.randn(5, nb_channels, h, w)\n",
    "weights = torch.tensor([[0., 0., 0.],\n",
    "                        [0., 1., 0.],\n",
    "                        [0., 0., 0.]])\n",
    "weights = (torch.ones(1,1,k,k)/k**2).repeat(2, 5, 1, 1)\n",
    "#weights = weights.view(1, 1, 3, 3).repeat(1, nb_channels, 1, 1)\n",
    "\n",
    "output = F.conv2d(x, weights,groups=2)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_conv(x):\n",
    "    D = nn.Conv2d(in_channels=nb_channels, out_channels=out_channels, kernel_size=k, stride=1, bias=False)\n",
    "    D.weight = torch.nn.Parameter((torch.ones_like(D.weight)/k**2),requires_grad=False)\n",
    "\n",
    "    \n",
    "    x = x.view(bs*p*p, nb_channels,h, w)\n",
    "    x = D(x)\n",
    "    x = x.view(-1, x.shape[1], x.shape[2], x.shape[3], p,p)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 32, 10, 10, 4, 4, 32]), 3276800, 3.2768)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "nb_channels = 32\n",
    "out_channels = 32\n",
    "h, w = 14,14\n",
    "p = 4\n",
    "k = 5\n",
    "bs = 2\n",
    "x = torch.randn(bs, nb_channels, p, p , h, w)\n",
    "C = 32\n",
    "\n",
    "#D = nn.Conv2d(in_channels=nb_channels, out_channels=out_channels, kernel_size=k, stride=1, bias=False)\n",
    "#D.weight = torch.nn.Parameter((torch.ones_like(D.weight)/k**2),requires_grad=False)\n",
    "\n",
    "\n",
    "#x = x.view(bs*p*p, nb_channels,h, w)\n",
    "#x = x.view(bs, nb_channels, p, p , h, w)\n",
    "#x.shape\n",
    "#D.weight.shape , D.weight.requires_grad, D.weight.mean(), torch.numel(D.weight)\n",
    "\n",
    "#x = D(x)\n",
    "#x = x.view(-1, x.shape[1], x.shape[2], x.shape[3], p,p)\n",
    "\n",
    "x = fix_conv(x)\n",
    "\n",
    "x = x.unsqueeze(-1).repeat([1, 1, 1, 1, 1, 1, C])\n",
    "w = torch.rand([1, 32, x.shape[2], x.shape[3], 4, 4, C])\n",
    "w = w.repeat([bs, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "assert x.shape == w.shape\n",
    "\n",
    "v = torch.mul(x, w)\n",
    "\n",
    "\n",
    "\n",
    "v.shape, v.numel(), w.numel()/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(10,1).repeat(1,4)\n",
    "b = torch.rand(10,4)\n",
    "\n",
    "c = torch.mul(a,b)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def transform_view(x, w, C, P, w_shared=False):\n",
    "        \"\"\"\n",
    "            For conv_caps:\n",
    "                Input:     (b*H*W, K*K*B, P*P)\n",
    "                Output:    (b*H*W, K*K*B, C, P*P)\n",
    "            For class_caps:\n",
    "                Input:     (b, H*W*B, P*P)\n",
    "                Output:    (b, H*W*B, C, P*P)\n",
    "        \"\"\"\n",
    "        b, B, psize = x.shape\n",
    "        assert psize == P*P\n",
    "\n",
    "        x = x.view(b, B, 1, P, P)\n",
    "        if w_shared:\n",
    "            hw = int(B / w.size(1))\n",
    "            w = w.repeat(1, hw, 1, 1, 1)\n",
    "\n",
    "        w = w.repeat(b, 1, 1, 1, 1)\n",
    "        x = x.repeat(1, 1, C, 1, 1)\n",
    "        v = torch.matmul(x, w)\n",
    "        v = v.view(b, B, C, P*P)\n",
    "        print(w.shape, w.numel(), x.shape, x.numel())\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "21233664//2359296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([144, 288, 32, 4, 4]) 21233664 torch.Size([144, 288, 32, 4, 4]) 21233664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([144, 288, 32, 16]),\n",
       " 21233664,\n",
       " 147456,\n",
       " torch.Size([1, 12, 12, 32, 9, 32, 16]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1*12*12,3*3*32,4*4)\n",
    "w = torch.rand(1,3*3*32,32,4,4)\n",
    "t = transform_view(x=x, w=w, C=32, P=4)\n",
    "\n",
    "t.shape, t.numel(), w.numel(), t.view(-1,12,12,32,9,32,16).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10]) tensor([[   0,    1,    4,    9,   16,   25,   36,   49,   64,   81],\n",
      "        [ 100,  121,  144,  169,  196,  225,  256,  289,  324,  361],\n",
      "        [ 400,  441,  484,  529,  576,  625,  676,  729,  784,  841],\n",
      "        [ 900,  961, 1024, 1089, 1156, 1225, 1296, 1369, 1444, 1521],\n",
      "        [1600, 1681, 1764, 1849, 1936, 2025, 2116, 2209, 2304, 2401],\n",
      "        [2500, 2601, 2704, 2809, 2916, 3025, 3136, 3249, 3364, 3481],\n",
      "        [3600, 3721, 3844, 3969, 4096, 4225, 4356, 4489, 4624, 4761],\n",
      "        [4900, 5041, 5184, 5329, 5476, 5625, 5776, 5929, 6084, 6241],\n",
      "        [6400, 6561, 6724, 6889, 7056, 7225, 7396, 7569, 7744, 7921],\n",
      "        [8100, 8281, 8464, 8649, 8836, 9025, 9216, 9409, 9604, 9801]])\n",
      "torch.Size([10, 10]) tensor([[ 2850,  2895,  2940,  2985,  3030,  3075,  3120,  3165,  3210,  3255],\n",
      "        [ 7350,  7495,  7640,  7785,  7930,  8075,  8220,  8365,  8510,  8655],\n",
      "        [11850, 12095, 12340, 12585, 12830, 13075, 13320, 13565, 13810, 14055],\n",
      "        [16350, 16695, 17040, 17385, 17730, 18075, 18420, 18765, 19110, 19455],\n",
      "        [20850, 21295, 21740, 22185, 22630, 23075, 23520, 23965, 24410, 24855],\n",
      "        [25350, 25895, 26440, 26985, 27530, 28075, 28620, 29165, 29710, 30255],\n",
      "        [29850, 30495, 31140, 31785, 32430, 33075, 33720, 34365, 35010, 35655],\n",
      "        [34350, 35095, 35840, 36585, 37330, 38075, 38820, 39565, 40310, 41055],\n",
      "        [38850, 39695, 40540, 41385, 42230, 43075, 43920, 44765, 45610, 46455],\n",
      "        [43350, 44295, 45240, 46185, 47130, 48075, 49020, 49965, 50910, 51855]])\n",
      "torch.Size([10, 10]) tensor([[ 2850,  2895,  2940,  2985,  3030,  3075,  3120,  3165,  3210,  3255],\n",
      "        [ 7350,  7495,  7640,  7785,  7930,  8075,  8220,  8365,  8510,  8655],\n",
      "        [11850, 12095, 12340, 12585, 12830, 13075, 13320, 13565, 13810, 14055],\n",
      "        [16350, 16695, 17040, 17385, 17730, 18075, 18420, 18765, 19110, 19455],\n",
      "        [20850, 21295, 21740, 22185, 22630, 23075, 23520, 23965, 24410, 24855],\n",
      "        [25350, 25895, 26440, 26985, 27530, 28075, 28620, 29165, 29710, 30255],\n",
      "        [29850, 30495, 31140, 31785, 32430, 33075, 33720, 34365, 35010, 35655],\n",
      "        [34350, 35095, 35840, 36585, 37330, 38075, 38820, 39565, 40310, 41055],\n",
      "        [38850, 39695, 40540, 41385, 42230, 43075, 43920, 44765, 45610, 46455],\n",
      "        [43350, 44295, 45240, 46185, 47130, 48075, 49020, 49965, 50910, 51855]])\n"
     ]
    }
   ],
   "source": [
    "#h = torch.arange(0,10,1)\n",
    "p = torch.arange(0,100,1)\n",
    "p = p.reshape(-1,10)\n",
    "h = p.clone()\n",
    "\n",
    "mu = torch.mul(h,p)\n",
    "ma = torch.matmul(h,p)\n",
    "ei = torch.einsum('ij,jk->ik', h, p)\n",
    "\n",
    "#print(h.shape, h)\n",
    "#print(p.shape, p)\n",
    "print(mu.shape, mu)\n",
    "print(ma.shape, ma)\n",
    "print(ei.shape, ei)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "188faa17072d374bec02d17fca5e544867bade69f71230dfd1a560a6ca303930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('EffCN': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
