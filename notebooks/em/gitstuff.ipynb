{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./../..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch import optim\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "from misc.utils import count_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R = np.array([[1,2,3],[1,2,3],[1,2,3]])\n",
    "#V = np.array([[1,2,3],[1,2,3],[1,2,3]])+1\n",
    "np.random.seed(0)\n",
    "\n",
    "d1 = 10\n",
    "d2 = 2\n",
    "R  = np.random.rand(d1,d2)\n",
    "V  = np.random.rand(d1,d2)\n",
    "bv = np.random.rand(d2)\n",
    "ba = np.random.rand(d2)\n",
    "la = 0.5\n",
    "\n",
    "ax = 0\n",
    "mu = np.expand_dims((np.sum(R * V, axis=ax)/ np.sum(R, axis=ax)), axis=0)\n",
    "s2 = (np.sum(R * (V-mu)**2, axis=ax)/ np.sum(R, axis=ax))\n",
    "si = np.sqrt(s2)\n",
    "sl = np.log(si)\n",
    "co = (bv + sl) * np.sum(R, axis=ax)\n",
    "ac = (1 / (1 + np.exp(-(la*(ba - np.sum(co))))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pn = 1/np.sqrt(2*math.pi*s2) * np.exp(-((V-mu)**2 / 2*s2))\n",
    "pl = -((V-mu)**2 / 2*s2) - sl - np.log(2*math.pi)/2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "R, V, mu, s2, si, sl, pn, pl, (np.round(pl - np.log(pn), 10))\n",
    "\n",
    "xa = np.arange(1,len(pn[:,0])+1,1)\n",
    "\n",
    "plt.plot(xa, pn[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1 / (1+np.exp(-R)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CN EM from Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    r\"\"\"Creates a primary convolutional capsule layer\n",
    "    that outputs a pose matrix and an activation.\n",
    "    Note that for computation convenience, pose matrix\n",
    "    are stored in first part while the activations are\n",
    "    stored in the second part.\n",
    "    Args:\n",
    "        A: output of the normal conv layer\n",
    "        B: number of types of capsules\n",
    "        K: kernel size of convolution\n",
    "        P: size of pose matrix is P*P\n",
    "        stride: stride of convolution\n",
    "    Shape:\n",
    "        input:  (*, A, h, w)\n",
    "        output: (*, h', w', B*(P*P+1))\n",
    "        h', w' is computed the same way as convolution layer\n",
    "        parameter size is: K*K*A*B*P*P + B*P*P\n",
    "    \"\"\"\n",
    "    def __init__(self, A=32, B=32, K=1, P=4, stride=1):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "        self.pose = nn.Conv2d(in_channels=A, out_channels=B*P*P,\n",
    "                            kernel_size=K, stride=stride, bias=True)\n",
    "        self.a = nn.Conv2d(in_channels=A, out_channels=B,\n",
    "                            kernel_size=K, stride=stride, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.pose(x)\n",
    "        a = self.a(x)\n",
    "        a = self.sigmoid(a)\n",
    "        out = torch.cat([p, a], dim=1)\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvCaps(nn.Module):\n",
    "    r\"\"\"Create a convolutional capsule layer\n",
    "    that transfer capsule layer L to capsule layer L+1\n",
    "    by EM routing.\n",
    "    Args:\n",
    "        B: input number of types of capsules\n",
    "        C: output number on types of capsules\n",
    "        K: kernel size of convolution\n",
    "        P: size of pose matrix is P*P\n",
    "        stride: stride of convolution\n",
    "        iters: number of EM iterations\n",
    "        coor_add: use scaled coordinate addition or not\n",
    "        w_shared: share transformation matrix across w*h.\n",
    "    Shape:\n",
    "        input:  (*, h,  w, B*(P*P+1))\n",
    "        output: (*, h', w', C*(P*P+1))\n",
    "        h', w' is computed the same way as convolution layer\n",
    "        parameter size is: K*K*B*C*P*P + B*P*P\n",
    "    \"\"\"\n",
    "    def __init__(self, B=32, C=32, K=3, P=4, stride=2, iters=3,\n",
    "                 coor_add=False, w_shared=False):\n",
    "        super(ConvCaps, self).__init__()\n",
    "        # TODO: lambda scheduler\n",
    "        # Note that .contiguous() for 3+ dimensional tensors is very slow\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.K = K\n",
    "        self.P = P\n",
    "        self.psize = P*P\n",
    "        self.stride = stride\n",
    "        self.iters = iters\n",
    "        self.coor_add = coor_add\n",
    "        self.w_shared = w_shared\n",
    "        # constant\n",
    "        self.eps = 1e-8\n",
    "        self._lambda = 1e-03\n",
    "        self.ln_2pi = torch.cuda.FloatTensor(1).fill_(math.log(2*math.pi))\n",
    "        # params\n",
    "        # Note that \\beta_u and \\beta_a are per capsule type,\n",
    "        # which are stated at https://openreview.net/forum?id=HJWLfGWRb&noteId=rJUY2VdbM\n",
    "        self.beta_u = nn.Parameter(torch.zeros(C))\n",
    "        self.beta_a = nn.Parameter(torch.zeros(C))\n",
    "        # Note that the total number of trainable parameters between\n",
    "        # two convolutional capsule layer types is 4*4*k*k\n",
    "        # and for the whole layer is 4*4*k*k*B*C,\n",
    "        # which are stated at https://openreview.net/forum?id=HJWLfGWRb&noteId=r17t2UIgf\n",
    "        self.weights = nn.Parameter(torch.randn(1, K*K*B, C, P, P))\n",
    "        # op\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def m_step(self, a_in, r, v, eps, b, B, C, psize):\n",
    "        \"\"\"\n",
    "            \\mu^h_j = \\dfrac{\\sum_i r_{ij} V^h_{ij}}{\\sum_i r_{ij}}\n",
    "            (\\sigma^h_j)^2 = \\dfrac{\\sum_i r_{ij} (V^h_{ij} - mu^h_j)^2}{\\sum_i r_{ij}}\n",
    "            cost_h = (\\beta_u + log \\sigma^h_j) * \\sum_i r_{ij}\n",
    "            a_j = logistic(\\lambda * (\\beta_a - \\sum_h cost_h))\n",
    "            Input:\n",
    "                a_in:      (b, C, 1)\n",
    "                r:         (b, B, C, 1)\n",
    "                v:         (b, B, C, P*P)\n",
    "            Local:\n",
    "                cost_h:    (b, C, P*P)\n",
    "                r_sum:     (b, C, 1)\n",
    "            Output:\n",
    "                a_out:     (b, C, 1)\n",
    "                mu:        (b, 1, C, P*P)\n",
    "                sigma_sq:  (b, 1, C, P*P)\n",
    "        \"\"\"\n",
    "        r = r * a_in\n",
    "        r = r / (r.sum(dim=2, keepdim=True) + eps)\n",
    "        r_sum = r.sum(dim=1, keepdim=True)\n",
    "        coeff = r / (r_sum + eps)\n",
    "        coeff = coeff.view(b, B, C, 1)\n",
    "\n",
    "        mu = torch.sum(coeff * v, dim=1, keepdim=True)\n",
    "        sigma_sq = torch.sum(coeff * (v - mu)**2, dim=1, keepdim=True) + eps\n",
    "\n",
    "        r_sum = r_sum.view(b, C, 1)\n",
    "        sigma_sq = sigma_sq.view(b, C, psize)\n",
    "        cost_h = (self.beta_u.view(C, 1) + torch.log(sigma_sq.sqrt())) * r_sum\n",
    "\n",
    "        a_out = self.sigmoid(self._lambda*(self.beta_a - cost_h.sum(dim=2)))\n",
    "        sigma_sq = sigma_sq.view(b, 1, C, psize)\n",
    "\n",
    "        return a_out, mu, sigma_sq\n",
    "\n",
    "    def e_step(self, mu, sigma_sq, a_out, v, eps, b, C):\n",
    "        \"\"\"\n",
    "            ln_p_j = sum_h \\dfrac{(\\V^h_{ij} - \\mu^h_j)^2}{2 \\sigma^h_j}\n",
    "                    - sum_h ln(\\sigma^h_j) - 0.5*\\sum_h ln(2*\\pi)\n",
    "            r = softmax(ln(a_j*p_j))\n",
    "              = softmax(ln(a_j) + ln(p_j))\n",
    "            Input:\n",
    "                mu:        (b, 1, C, P*P)\n",
    "                sigma:     (b, 1, C, P*P)\n",
    "                a_out:     (b, C, 1)\n",
    "                v:         (b, B, C, P*P)\n",
    "            Local:\n",
    "                ln_p_j_h:  (b, B, C, P*P)\n",
    "                ln_ap:     (b, B, C, 1)\n",
    "            Output:\n",
    "                r:         (b, B, C, 1)\n",
    "        \"\"\"\n",
    "        ln_p_j_h = -1. * (v - mu)**2 / (2 * sigma_sq) \\\n",
    "                    - torch.log(sigma_sq.sqrt()) \\\n",
    "                    - 0.5*self.ln_2pi\n",
    "\n",
    "        ln_ap = ln_p_j_h.sum(dim=3) + torch.log(a_out.view(b, 1, C))\n",
    "        r = self.softmax(ln_ap)\n",
    "        return r\n",
    "\n",
    "    def caps_em_routing(self, v, a_in, C, eps):\n",
    "        \"\"\"\n",
    "            Input:\n",
    "                v:         (b, B, C, P*P)\n",
    "                a_in:      (b, C, 1)\n",
    "            Output:\n",
    "                mu:        (b, 1, C, P*P)\n",
    "                a_out:     (b, C, 1)\n",
    "            Note that some dimensions are merged\n",
    "            for computation convenient, that is\n",
    "            `b == batch_size*oh*ow`,\n",
    "            `B == self.K*self.K*self.B`,\n",
    "            `psize == self.P*self.P`\n",
    "        \"\"\"\n",
    "        b, B, c, psize = v.shape\n",
    "        assert c == C\n",
    "        assert (b, B, 1) == a_in.shape\n",
    "\n",
    "        r = torch.cuda.FloatTensor(b, B, C).fill_(1./C)\n",
    "        for iter_ in range(self.iters):\n",
    "            a_out, mu, sigma_sq = self.m_step(a_in, r, v, eps, b, B, C, psize)\n",
    "            if iter_ < self.iters - 1:\n",
    "                r = self.e_step(mu, sigma_sq, a_out, v, eps, b, C)\n",
    "\n",
    "        return mu, a_out\n",
    "\n",
    "    def add_pathes(self, x, B, K, psize, stride):\n",
    "        \"\"\"\n",
    "            Shape:\n",
    "                Input:     (b, H, W, B*(P*P+1))\n",
    "                Output:    (b, H', W', K, K, B*(P*P+1))\n",
    "        \"\"\"\n",
    "        b, h, w, c = x.shape\n",
    "        assert h == w\n",
    "        assert c == B*(psize+1)\n",
    "        oh = ow = int(((h - K )/stride)+ 1) # moein - changed from: oh = ow = int((h - K + 1) / stride)\n",
    "        idxs = [[(h_idx + k_idx) \\\n",
    "                for k_idx in range(0, K)] \\\n",
    "                for h_idx in range(0, h - K + 1, stride)]\n",
    "        x = x[:, idxs, :, :]\n",
    "        x = x[:, :, :, idxs, :]\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "        return x, oh, ow\n",
    "\n",
    "    def transform_view(self, x, w, C, P, w_shared=False):\n",
    "        \"\"\"\n",
    "            For conv_caps:\n",
    "                Input:     (b*H*W, K*K*B, P*P)\n",
    "                Output:    (b*H*W, K*K*B, C, P*P)\n",
    "            For class_caps:\n",
    "                Input:     (b, H*W*B, P*P)\n",
    "                Output:    (b, H*W*B, C, P*P)\n",
    "        \"\"\"\n",
    "        b, B, psize = x.shape\n",
    "        assert psize == P*P\n",
    "\n",
    "        x = x.view(b, B, 1, P, P)\n",
    "        if w_shared:\n",
    "            hw = int(B / w.size(1))\n",
    "            w = w.repeat(1, hw, 1, 1, 1)\n",
    "\n",
    "        w = w.repeat(b, 1, 1, 1, 1)\n",
    "        x = x.repeat(1, 1, C, 1, 1)\n",
    "        v = torch.matmul(x, w)\n",
    "        v = v.view(b, B, C, P*P)\n",
    "        return v\n",
    "\n",
    "    def add_coord(self, v, b, h, w, B, C, psize):\n",
    "        \"\"\"\n",
    "            Shape:\n",
    "                Input:     (b, H*W*B, C, P*P)\n",
    "                Output:    (b, H*W*B, C, P*P)\n",
    "        \"\"\"\n",
    "        assert h == w\n",
    "        v = v.view(b, h, w, B, C, psize)\n",
    "        coor = torch.arange(h, dtype=torch.float32) / h\n",
    "        coor_h = torch.cuda.FloatTensor(1, h, 1, 1, 1, self.psize).fill_(0.)\n",
    "        coor_w = torch.cuda.FloatTensor(1, 1, w, 1, 1, self.psize).fill_(0.)\n",
    "        coor_h[0, :, 0, 0, 0, 0] = coor\n",
    "        coor_w[0, 0, :, 0, 0, 1] = coor\n",
    "        v = v + coor_h + coor_w\n",
    "        v = v.view(b, h*w*B, C, psize)\n",
    "        return v\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, h, w, c = x.shape\n",
    "        if not self.w_shared:\n",
    "            # add patches\n",
    "            x, oh, ow = self.add_pathes(x, self.B, self.K, self.psize, self.stride)\n",
    "\n",
    "            # transform view\n",
    "            p_in = x[:, :, :, :, :, :self.B*self.psize].contiguous()\n",
    "            a_in = x[:, :, :, :, :, self.B*self.psize:].contiguous()\n",
    "            p_in = p_in.view(b*oh*ow, self.K*self.K*self.B, self.psize)\n",
    "            a_in = a_in.view(b*oh*ow, self.K*self.K*self.B, 1)\n",
    "            v = self.transform_view(p_in, self.weights, self.C, self.P)\n",
    "\n",
    "            # em_routing\n",
    "            p_out, a_out = self.caps_em_routing(v, a_in, self.C, self.eps)\n",
    "            p_out = p_out.view(b, oh, ow, self.C*self.psize)\n",
    "            a_out = a_out.view(b, oh, ow, self.C)\n",
    "            out = torch.cat([p_out, a_out], dim=3)\n",
    "        else:\n",
    "            assert c == self.B*(self.psize+1)\n",
    "            assert 1 == self.K\n",
    "            assert 1 == self.stride\n",
    "            p_in = x[:, :, :, :self.B*self.psize].contiguous()\n",
    "            p_in = p_in.view(b, h*w*self.B, self.psize)\n",
    "            a_in = x[:, :, :, self.B*self.psize:].contiguous()\n",
    "            a_in = a_in.view(b, h*w*self.B, 1)\n",
    "\n",
    "            # transform view\n",
    "            v = self.transform_view(p_in, self.weights, self.C, self.P, self.w_shared)\n",
    "\n",
    "            # coor_add\n",
    "            if self.coor_add:\n",
    "                v = self.add_coord(v, b, h, w, self.B, self.C, self.psize)\n",
    "\n",
    "            # em_routing\n",
    "            _, out = self.caps_em_routing(v, a_in, self.C, self.eps)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    \"\"\"A network with one ReLU convolutional layer followed by\n",
    "    a primary convolutional capsule layer and two more convolutional capsule layers.\n",
    "    Suppose image shape is 28x28x1, the feature maps change as follows:\n",
    "    1. ReLU Conv1\n",
    "        (_, 1, 28, 28) -> 5x5 filters, 32 out channels, stride 2 with padding\n",
    "        x -> (_, 32, 14, 14)\n",
    "    2. PrimaryCaps\n",
    "        (_, 32, 14, 14) -> 1x1 filter, 32 out capsules, stride 1, no padding\n",
    "        x -> pose: (_, 14, 14, 32x4x4), activation: (_, 14, 14, 32)\n",
    "    3. ConvCaps1\n",
    "        (_, 14, 14, 32x(4x4+1)) -> 3x3 filters, 32 out capsules, stride 2, no padding\n",
    "        x -> pose: (_, 6, 6, 32x4x4), activation: (_, 6, 6, 32)\n",
    "    4. ConvCaps2\n",
    "        (_, 6, 6, 32x(4x4+1)) -> 3x3 filters, 32 out capsules, stride 1, no padding\n",
    "        x -> pose: (_, 4, 4, 32x4x4), activation: (_, 4, 4, 32)\n",
    "    5. ClassCaps\n",
    "        (_, 4, 4, 32x(4x4+1)) -> 1x1 conv, 10 out capsules\n",
    "        x -> pose: (_, 10x4x4), activation: (_, 10)\n",
    "        Note that ClassCaps only outputs activation for each class\n",
    "    Args:\n",
    "        A: output channels of normal conv\n",
    "        B: output channels of primary caps\n",
    "        C: output channels of 1st conv caps\n",
    "        D: output channels of 2nd conv caps\n",
    "        E: output channels of class caps (i.e. number of classes)\n",
    "        K: kernel of conv caps\n",
    "        P: size of square pose matrix\n",
    "        iters: number of EM iterations\n",
    "        ...\n",
    "    \"\"\"\n",
    "    def __init__(self, A=32, B=32, C=32, D=32, E=10, K=3, P=4, iters=3):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=A,\n",
    "                               kernel_size=5, stride=2, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=A, eps=0.001,\n",
    "                                 momentum=0.1, affine=True)\n",
    "        self.relu1 = nn.ReLU(inplace=False)\n",
    "        self.primary_caps = PrimaryCaps(A, B, 1, P, stride=1)\n",
    "        self.conv_caps1 = ConvCaps(B, C, K, P, stride=2, iters=iters)\n",
    "        self.conv_caps2 = ConvCaps(C, D, K, P, stride=1, iters=iters)\n",
    "        self.class_caps = ConvCaps(D, E, 1, P, stride=1, iters=iters,\n",
    "                                        coor_add=True, w_shared=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.primary_caps(x)\n",
    "        x = self.conv_caps1(x)\n",
    "        x = self.conv_caps2(x)\n",
    "        x = self.class_caps(x)\n",
    "        return x\n",
    "\n",
    "    def forward_debug(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.primary_caps(x)\n",
    "        x = self.conv_caps1(x)\n",
    "        x = self.conv_caps2(x)\n",
    "        x = self.class_caps(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capsules(**kwargs):\n",
    "    \"\"\"Constructs a CapsNet model.\n",
    "    \"\"\"\n",
    "    model = CapsNet(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CapsNet(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (bn1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU()\n",
      "  (primary_caps): PrimaryCaps(\n",
      "    (pose): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (a): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (conv_caps1): ConvCaps(\n",
      "    (sigmoid): Sigmoid()\n",
      "    (softmax): Softmax(dim=2)\n",
      "  )\n",
      "  (conv_caps2): ConvCaps(\n",
      "    (sigmoid): Sigmoid()\n",
      "    (softmax): Softmax(dim=2)\n",
      "  )\n",
      "  (class_caps): ConvCaps(\n",
      "    (sigmoid): Sigmoid()\n",
      "    (softmax): Softmax(dim=2)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TEST\n",
    "Run this code with:\n",
    "```\n",
    "python -m capsules.py\n",
    "```\n",
    "'''\n",
    "#if __name__ == '__main__':\n",
    "model = capsules(C=64, E=10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpreadLoss(_Loss):\n",
    "\n",
    "    def __init__(self, m_min=0.2, m_max=0.9, num_class=10):\n",
    "        super(SpreadLoss, self).__init__()\n",
    "        self.m_min = m_min\n",
    "        self.m_max = m_max\n",
    "        self.num_class = num_class\n",
    "\n",
    "    def forward(self, x, target, r):\n",
    "        b, E = x.shape\n",
    "        assert E == self.num_class\n",
    "        margin = self.m_min + (self.m_max - self.m_min)*r\n",
    "\n",
    "        at = torch.cuda.FloatTensor(b).fill_(0)\n",
    "        for i, lb in enumerate(target):\n",
    "            at[i] = x[i][lb]\n",
    "        at = at.view(b, 1).repeat(1, E)\n",
    "\n",
    "        zeros = x.new_zeros(x.shape)\n",
    "        loss = torch.max(margin - (at - x), zeros)\n",
    "        loss = loss**2\n",
    "        loss = loss.sum() / b - margin**2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = datasets.MNIST(root='../../data', train=True, download=True, transform=T.ToTensor())\n",
    "ds_valid = datasets.MNIST(root=\"../../data\", train=False, download=True, transform=T.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = torch.utils.data.DataLoader(ds_train, \n",
    "                                        batch_size=8, \n",
    "                                        shuffle=True,\n",
    "                                        num_workers=4)\n",
    "\n",
    "dl_valid = torch.utils.data.DataLoader(ds_valid, \n",
    "                                        batch_size=8, \n",
    "                                        shuffle=True,\n",
    "                                        num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1, 28, 28]), tensor([3, 7, 0, 4, 8, 6, 1, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(dl_train))\n",
    "\n",
    "x.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19416/581911108.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#CN.to(device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_debug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CN' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "CN = CapsNet(C=64, E=10)\n",
    "CN.to(device)\n",
    "\n",
    "o = CN.forward_debug(x.to(device))\n",
    "\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319028"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CapsNet()\n",
    "model = model.to(device)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = SpreadLoss(num_class=10, m_min=0.2, m_max=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def exp_lr_decay(optimizer, global_step, init_lr = 3e-3, decay_steps = 20000,\n",
    "                                        decay_rate = 0.96, lr_clip = 3e-3 ,staircase=False):\n",
    "    \n",
    "    ''' decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)  '''\n",
    "    \n",
    "    if staircase:\n",
    "        lr = (init_lr * decay_rate**(global_step // decay_steps)) \n",
    "    else:\n",
    "        lr = (init_lr * decay_rate**(global_step / decay_steps)) \n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "train_len = len(dl_train)\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for idx, (x, y_true) in enumerate(dl_train):\n",
    "        x = x.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        r = (1.*idx + (epoch-1)*train_len) / (num_epochs*train_len)\n",
    "        loss = loss_func(y_pred, y_true,r)         \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if idx % 1000 == 0:\n",
    "            print(\"Epoch[{}/{}] - step {} loss: {:.4f}\".format(epoch, num_epochs, idx, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train [  0/  2]:â–ˆ         | 781/7500 [03:25<29:26,  3.80it/s, loss=-.0129, acc=[tensor([25.], device='cuda:0')]]    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb Cell 22'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000025vscode-remote?line=13'>14</a>\u001b[0m y_true \u001b[39m=\u001b[39m y_true\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000025vscode-remote?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000025vscode-remote?line=17'>18</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000025vscode-remote?line=18'>19</a>\u001b[0m r \u001b[39m=\u001b[39m (\u001b[39m1.\u001b[39m\u001b[39m*\u001b[39midx \u001b[39m+\u001b[39m (epoch_idx\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mtrain_len) \u001b[39m/\u001b[39m (num_epochs\u001b[39m*\u001b[39mtrain_len)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000025vscode-remote?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_func(y_pred, y_true,r)         \n",
      "File \u001b[0;32m~/anaconda3/envs/EffCN/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/mkoch/anaconda3/envs/EffCN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/mkoch/anaconda3/envs/EffCN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/mkoch/anaconda3/envs/EffCN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/mkoch/anaconda3/envs/EffCN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/mkoch/anaconda3/envs/EffCN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/mkoch/anaconda3/envs/EffCN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/mkoch/anaconda3/envs/EffCN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb Cell 9'\u001b[0m in \u001b[0;36mCapsNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000008vscode-remote?line=48'>49</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprimary_caps(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000008vscode-remote?line=49'>50</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_caps1(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000008vscode-remote?line=50'>51</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_caps2(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000008vscode-remote?line=51'>52</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_caps(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000008vscode-remote?line=52'>53</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/EffCN/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/mkoch/anaconda3/envs/EffCN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/mkoch/anaconda3/envs/EffCN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/mkoch/anaconda3/envs/EffCN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/mkoch/anaconda3/envs/EffCN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/mkoch/anaconda3/envs/EffCN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/mkoch/anaconda3/envs/EffCN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/mkoch/anaconda3/envs/EffCN/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb Cell 8'\u001b[0m in \u001b[0;36mConvCaps.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000007vscode-remote?line=197'>198</a>\u001b[0m b, h, w, c \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000007vscode-remote?line=198'>199</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_shared:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000007vscode-remote?line=199'>200</a>\u001b[0m     \u001b[39m# add patches\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000007vscode-remote?line=200'>201</a>\u001b[0m     x, oh, ow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_pathes(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mB, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mK, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpsize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000007vscode-remote?line=202'>203</a>\u001b[0m     \u001b[39m# transform view\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000007vscode-remote?line=203'>204</a>\u001b[0m     p_in \u001b[39m=\u001b[39m x[:, :, :, :, :, :\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mB\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpsize]\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[1;32m/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb Cell 8'\u001b[0m in \u001b[0;36mConvCaps.add_pathes\u001b[0;34m(self, x, B, K, psize, stride)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000007vscode-remote?line=147'>148</a>\u001b[0m oh \u001b[39m=\u001b[39m ow \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(((h \u001b[39m-\u001b[39m K )\u001b[39m/\u001b[39mstride)\u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m# moein - changed from: oh = ow = int((h - K + 1) / stride)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000007vscode-remote?line=148'>149</a>\u001b[0m idxs \u001b[39m=\u001b[39m [[(h_idx \u001b[39m+\u001b[39m k_idx) \\\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000007vscode-remote?line=149'>150</a>\u001b[0m         \u001b[39mfor\u001b[39;00m k_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, K)] \\\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000007vscode-remote?line=150'>151</a>\u001b[0m         \u001b[39mfor\u001b[39;00m h_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, h \u001b[39m-\u001b[39m K \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, stride)]\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000007vscode-remote?line=151'>152</a>\u001b[0m x \u001b[39m=\u001b[39m x[:, idxs, :, :]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000007vscode-remote?line=152'>153</a>\u001b[0m x \u001b[39m=\u001b[39m x[:, :, :, idxs, :]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bamy.inf-i2.uni-jena.de/home/mkoch/projects/EfficientCN/notebooks/em/gitstuff.ipynb#ch0000007vscode-remote?line=153'>154</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "train_len = len(dl_train)\n",
    "#\n",
    "for epoch_idx in range(num_epochs):\n",
    "    # ####################\n",
    "    # TRAIN\n",
    "    # ####################\n",
    "    model.train()\n",
    "    desc = \"Train [{:3}/{:3}]:\".format(epoch_idx, num_epochs)\n",
    "    pbar = tqdm(dl_train, bar_format=desc + '{bar:10}{r_bar}{bar:-10b}')\n",
    "    \n",
    "    for idx, (x,y_true) in enumerate(pbar):\n",
    "        x = x.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        y_pred = model(x)\n",
    "        r = (1.*idx + (epoch_idx-1)*train_len) / (num_epochs*train_len)\n",
    "        loss = loss_func(y_pred, y_true,r)         \n",
    "        acc = accuracy(y_pred, y_true)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "                {'loss': loss.item(),\n",
    "                 'acc': acc\n",
    "                 }\n",
    "        )\n",
    "    \n",
    "\n",
    "        \n",
    "    # I guess this is done once per epoch\n",
    "    #lr_scheduler.step()\n",
    "    #\n",
    "    # ####################\n",
    "    # VALID\n",
    "    # ####################\n",
    "    model.eval()\n",
    "    \n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x,y_true in dl_valid:\n",
    "        x = x.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            acc = accuracy(y_pred, y_true)\n",
    "    print(\"   acc_valid: {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf positional addition encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial output of ConvCaps2 is 4x4\n",
    "v1 = [[[8.],  [12.], [16.], [20.]],\n",
    "      [[8.],  [12.], [16.], [20.]],\n",
    "      [[8.],  [12.], [16.], [20.]],\n",
    "      [[8.],  [12.], [16.], [20.]],\n",
    "     ]\n",
    "\n",
    "v2 = [[[8.],  [8.],  [8.],  [8]],\n",
    "      [[12.], [12.], [12.], [12.]],\n",
    "      [[16.], [16.], [16.], [16.]],\n",
    "      [[20.], [20.], [20.], [20.]]\n",
    "         ]\n",
    "\n",
    "c1 = np.array(v1, dtype=np.float32) / 28.0\n",
    "c2 = np.array(v2, dtype=np.float32) / 28.0\n",
    "\n",
    "\n",
    "c1.shape,c2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord_addition(votes, H, W):\n",
    "    \"\"\"Coordinate addition.\n",
    "\n",
    "    :param votes: (24, 4, 4, 32, 10, 16)\n",
    "    :param H, W: spaital height and width 4\n",
    "\n",
    "    :return votes: (24, 4, 4, 32, 10, 16)\n",
    "    \"\"\"\n",
    "    coordinate_offset_hh = tf.reshape(\n",
    "      (tf.range(H, dtype=tf.float32) + 0.50) / H, [1, H, 1, 1, 1]\n",
    "    )\n",
    "    coordinate_offset_h0 = tf.constant(\n",
    "      0.0, shape=[1, H, 1, 1, 1], dtype=tf.float32\n",
    "    )\n",
    "    coordinate_offset_h = tf.stack(\n",
    "      [coordinate_offset_hh, coordinate_offset_h0] + [coordinate_offset_h0 for _ in range(14)], axis=-1\n",
    "    )  # (1, 4, 1, 1, 1, 16)\n",
    "\n",
    "    coordinate_offset_ww = tf.reshape(\n",
    "      (tf.range(W, dtype=tf.float32) + 0.50) / W, [1, 1, W, 1, 1]\n",
    "    )\n",
    "    coordinate_offset_w0 = tf.constant(\n",
    "      0.0, shape=[1, 1, W, 1, 1], dtype=tf.float32\n",
    "    )\n",
    "    coordinate_offset_w = tf.stack(\n",
    "      [coordinate_offset_w0, coordinate_offset_ww] + [coordinate_offset_w0 for _ in range(14)], axis=-1\n",
    "    ) # (1, 1, 4, 1, 1, 16)\n",
    "\n",
    "    # (24, 4, 4, 32, 10, 16)\n",
    "    votes = votes + coordinate_offset_h + coordinate_offset_w\n",
    "\n",
    "    return votes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "a = np.random.rand(24, 4, 4, 32, 10, 16)\n",
    "print(a.shape)\n",
    "\n",
    "\n",
    "b = coord_addition(votes=a, H=4, W=4)\n",
    "\n",
    "c = b-a\n",
    "p = 3\n",
    "c[0,:,:,31,9,0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "188faa17072d374bec02d17fca5e544867bade69f71230dfd1a560a6ca303930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('EffCN': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
