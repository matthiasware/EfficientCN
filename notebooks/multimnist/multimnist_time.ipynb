{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "import hashlib\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dotted_dict import DottedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_root='/mnt/data/datasets/multimnist_test'\n",
    "p_root=Path(p_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mkdir_directories(dirs, parents, exist_ok):\n",
    "    for director in dirs:\n",
    "        Path(director).mkdir(parents=parents, exist_ok=exist_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __pad_rand__():\n",
    "        ref = 8\n",
    "        left = np.random.randint(1, high=9, size=None, dtype=int)\n",
    "        rigth = ref - left\n",
    "        up = np.random.randint(1, high=9, size=None, dtype=int)\n",
    "        down = ref - up\n",
    "        \n",
    "        return [left, up, rigth, down]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist\n",
    "ds_train = datasets.MNIST(root=p_root, train=True, download=True, transform=T.ToTensor())\n",
    "ds_valid = datasets.MNIST(root=p_root, train=False, download=True, transform=T.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "p_data = p_root / \"Train\"\n",
    "p_imgs = p_data / 'Img'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=ds_train.data.unsqueeze(1)\n",
    "labels=ds_train.targets\n",
    "n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".unsqueeze(1) -> before loop\n",
    "label1 = labels[j].item() -> give label item insted of tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mkdir_directories([p_data, p_imgs], parents=True, exist_ok=True)\n",
    "\n",
    "Start = time.time()\n",
    "\n",
    "\n",
    "#lists\n",
    "all_targets1 = []\n",
    "all_targets2 = []\n",
    "\n",
    "#generator index\n",
    "index = 1\n",
    "\n",
    "\n",
    "StartLoop = time.time()\n",
    "#test dataset\n",
    "test = images[0:6]\n",
    "print(test.size())\n",
    "for j, image in enumerate(test):\n",
    "\n",
    "#generate for whole dataset\n",
    "#for j, image in enumerate(images):\n",
    "\n",
    "    #reference img\n",
    "    img_ref = images[j]\n",
    "    lab_ref = labels[j]\n",
    "\n",
    "    #choose random top images from different classes\n",
    "    top_idx = np.where(labels != lab_ref)[0]\n",
    "    top_idx = np.random.choice(top_idx,n,replace=False)\n",
    "\n",
    "    #generate images\n",
    "    for i, idx in enumerate(top_idx):\n",
    "        \n",
    "        #randomize position\n",
    "        base  = T.Pad(padding=__pad_rand__())(images[j])\n",
    "        top   = T.Pad(padding=__pad_rand__())(images[top_idx[i]])\n",
    "        \n",
    "        #merge images\n",
    "        merge = torch.clamp(base + top,min=0, max=1)\n",
    "        #merge_s = merge.unsqueeze(0)\n",
    "\n",
    "        \n",
    "        #add labels to list\n",
    "        label1 = labels[j].item()\n",
    "        label2 = labels[top_idx[i]].item()\n",
    "        all_targets1.append(label1)\n",
    "        all_targets2.append(label2)\n",
    "\n",
    "        #Save Img as png\n",
    "        #a = plt.imshow(merge.float().permute(1,2,0))\n",
    "        #a.savefig(p_imgs / \"{:08d}.png\".format(index))\n",
    "        #a.close()\n",
    "\n",
    "        #img = Image.fromarray(merge.float().numpy(), mode='L')\n",
    "        #img.save( p_imgs / \"{:08d}.png\".format(index))\n",
    "        torchvision.utils.save_image(merge.float(), p_imgs / \"{:08d}.png\".format(index))\n",
    "        index += 1 \n",
    "\n",
    "EndLoop = time.time()\n",
    "print(sys.getsizeof(all_targets1))\n",
    "print(len(all_targets1))\n",
    "print(sys.getsizeof(all_targets2))\n",
    "print(len(all_targets2))\n",
    "\n",
    "\n",
    "#create target 1\n",
    "file_targets1 = open(p_data /'targets_1.plk', 'wb')\n",
    "pickle.dump(all_targets1, file_targets1)\n",
    "file_targets1.close()\n",
    "\n",
    "#create target 2\n",
    "file_targets2 = open(p_data /'targets_2.plk', 'wb')\n",
    "pickle.dump(all_targets2, file_targets2)\n",
    "file_targets2.close()\n",
    "\n",
    "End = time.time()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 113 / 60000\n",
    "b = a * 60000000\n",
    "b / (60 * 60 * 24)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets1[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_all = End - Start\n",
    "t_loop = EndLoop - StartLoop\n",
    "print(t_all)\n",
    "print(t_loop)\n",
    "print(t_all-t_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5270397663116455\n",
    "1.5180883407592773\n",
    "1.39308500289917\n",
    "12.76402497291565"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img =ds_train.data[0]\n",
    "\n",
    "img.shape\n",
    "\n",
    "img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train.data.shape\n",
    "\n",
    "ds = ds_train.data.unsqueeze(1).float()\n",
    "ds.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadtrageds():\n",
    "    #load target files\n",
    "    file = open(p_root / 'Train/targets_1.plk', 'rb')\n",
    "    targets_1 = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "    file = open(p_root / 'Train/targets_2.plk', 'rb')\n",
    "    targets_2 = pickle.load(file)\n",
    "    file.close()  \n",
    "\n",
    "    #for i, target in enumerate(targets_1):\n",
    "    # \tprint(i+1, targets_1[i], targets_2[i])\n",
    "    return targets_1, targets_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1, t2 = loadtrageds()\n",
    "len(t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split in preparing and gerneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __pad_rand_l__(m=1,n=1):\n",
    "        ref = 8\n",
    "        left = np.random.randint(1, high=9, size=(m*n), dtype=int)\n",
    "        rigth = ref - left\n",
    "        up = np.random.randint(1, high=9, size=(m*n), dtype=int)\n",
    "        down = ref - up\n",
    "\n",
    "        l_stack = np.stack((left,up, rigth, down), axis=1).tolist()\n",
    "        \n",
    "        return l_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = __pad_rand_l__(60,10)\n",
    "\n",
    "\n",
    "print(len(a))\n",
    "\n",
    "\n",
    "base  = T.Pad(padding=a[0])(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=ds_train.data\n",
    "labels=ds_train.targets\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=ds_train.data\n",
    "labels=ds_train.targets\n",
    "n = 20\n",
    "\n",
    "Start = time.time()\n",
    "# add 1 dim to mnist\n",
    "images = images.unsqueeze(1)\n",
    "\n",
    "# proof path\n",
    "mkdir_directories([p_data, p_imgs], parents=True, exist_ok=True)\n",
    "\n",
    "#generate index list for mmnist\n",
    "all_ref = []\n",
    "all_top = []\n",
    "\n",
    "test = labels[0:1000]\n",
    "print(test.size())\n",
    "\n",
    "StartLoop = time.time()\n",
    "for j, label in enumerate(test):\n",
    "\n",
    "    top_idx = np.where(labels != label)[0]\n",
    "    list_top = np.random.choice(top_idx,n,replace=False).tolist()\n",
    "    list_ref = np.full(n,j).tolist()\n",
    "\n",
    "    all_ref.extend(list_ref)\n",
    "    all_top.extend(list_top)\n",
    "\n",
    "EndLoop = time.time()\n",
    "\n",
    "StartPickle = time.time()\n",
    "#save labels\n",
    "#create target 1\n",
    "targets1 = labels[all_ref].tolist()\n",
    "file_targets1 = open(p_data /'targets_1.plk', 'wb')\n",
    "pickle.dump(targets1, file_targets1)\n",
    "file_targets1.close()\n",
    "\n",
    "#create target 2\n",
    "targets2 = labels[all_top].tolist()\n",
    "file_targets2 = open(p_data /'targets_2.plk', 'wb')\n",
    "pickle.dump(targets2, file_targets2)\n",
    "file_targets2.close()\n",
    "\n",
    "EndPickle = time.time()\n",
    "\n",
    "StartPadding = time.time()\n",
    "#create rand padding pos\n",
    "padding1 = __pad_rand_l__(len(test),n)\n",
    "padding2 = __pad_rand_l__(len(test),n)\n",
    "\n",
    "EndPadding = time.time()\n",
    "\n",
    "StartImgGen = time.time()\n",
    "all_merge = []\n",
    "all_file = []\n",
    "#generate images\n",
    "for i, ref in enumerate(all_ref):\n",
    "    base  = T.Pad(padding=padding1[i])(ds_train[all_ref[i]][0])\n",
    "    top   = T.Pad(padding=padding2[i])(ds_train[all_top[i]][0])\n",
    "\n",
    "    merge = torch.clamp(base + top,min=0, max=1)\n",
    "    all_merge.append(merge.float())\n",
    "    all_file.append(p_imgs / \"{:08d}.png\".format((i+1)))\n",
    "    #torchvision.utils.save_image(merge, p_imgs / \"{:08d}.png\".format((i+1)))\n",
    "    \n",
    "\n",
    "\n",
    "EndImgGen = time.time()\n",
    "End = time.time()\n",
    "print(EndLoop - StartLoop)\n",
    "print(EndPickle - StartPickle)\n",
    "print(EndPadding - StartPadding)\n",
    "print(EndImgGen - StartImgGen)\n",
    "print(End-Start)  \n",
    "\n",
    "#gen idx lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1, t2 = loadtrageds()\n",
    "len(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Image.fromarray(images[all_top[3]].squeeze(0).numpy(), mode='L')\n",
    "\n",
    "print(len(all_merge))\n",
    "print(all_merge[0].type())\n",
    "print(all_file)\n",
    "\n",
    "#torchvision.utils.save_image(all_merge, all_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(all_ref))\n",
    "print(len(all_ref))\n",
    "print(all_top)\n",
    "\n",
    "\n",
    "print(labels[all_ref].tolist())\n",
    "print(targets1)\n",
    "print(labels[all_top].tolist())\n",
    "print(targets2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ref.extend(list_ref)\n",
    "\n",
    "len(all_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "60000 / 111.27461862564087\n",
    "\n",
    "#126 * 1000 / (3600)\n",
    "\n",
    "\n",
    "60000000 / 539 /(60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=ds_train.data\n",
    "labels=ds_train.targets\n",
    "n = 1000\n",
    "\n",
    "Start = time.time()\n",
    "# add 1 dim to mnist\n",
    "images = images.unsqueeze(1)\n",
    "\n",
    "# proof path\n",
    "mkdir_directories([p_data, p_imgs], parents=True, exist_ok=True)\n",
    "\n",
    "#generate index list for mmnist\n",
    "all_ref = []\n",
    "all_top = []\n",
    "\n",
    "#generate top label idx for each class\n",
    "all_label = np.unique(labels)\n",
    "top_idx = []\n",
    "\n",
    "for k in all_label:\n",
    "    top_idx.append(np.where(labels != k)[0])\n",
    "\n",
    "test = labels[0:6000]\n",
    "print(test.size())\n",
    "\n",
    "StartLoop = time.time()\n",
    "for j, label in enumerate(labels):\n",
    "    #print(label)\n",
    "    #top_idx = np.where(labels != label)[0]\n",
    "    list_top = np.random.choice(top_idx[label.item()],n,replace=False).tolist()\n",
    "    list_ref = np.full(n,j).tolist()\n",
    "\n",
    "    all_ref.extend(list_ref)\n",
    "    all_top.extend(list_top)\n",
    "\n",
    "EndLoop = time.time()\n",
    "\n",
    "StartPickle = time.time()\n",
    "\n",
    "#create target 1 id's\n",
    "file_targets1 = open(p_data /'idx_1.plk', 'wb')\n",
    "pickle.dump(all_ref, file_targets1)\n",
    "file_targets1.close()\n",
    "\n",
    "#create target 2\n",
    "file_targets2 = open(p_data /'idx_2.plk', 'wb')\n",
    "pickle.dump(all_top, file_targets2)\n",
    "file_targets2.close()\n",
    "\n",
    "EndPickle = time.time()\n",
    "\n",
    "StartPadding = time.time()\n",
    "#create rand padding pos\n",
    "padding1 = __pad_rand_l__(len(labels),n)\n",
    "padding2 = __pad_rand_l__(len(labels),n)\n",
    "\n",
    "#create padding 1 \n",
    "file_targets1 = open(p_data /'padding_1.plk', 'wb')\n",
    "pickle.dump(padding1, file_targets1)\n",
    "file_targets1.close()\n",
    "\n",
    "#create padding 2 \n",
    "file_targets1 = open(p_data /'padding_2.plk', 'wb')\n",
    "pickle.dump(padding2, file_targets1)\n",
    "file_targets1.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EndPadding = time.time()\n",
    "\n",
    "\n",
    "End = time.time()\n",
    "print(EndLoop - StartLoop)\n",
    "print(EndPickle - StartPickle)\n",
    "print(EndPadding - StartPadding)\n",
    "print(End-Start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meta():\n",
    "    #load meta profil for mmnist\n",
    "    file1 = open(p_root / 'Train/idx_1.plk', 'rb')\n",
    "    id_1 = pickle.load(file1)\n",
    "    file1.close()\n",
    "\n",
    "    file = open(p_root / 'Train/idx_2.plk', 'rb')\n",
    "    id_2 = pickle.load(file)\n",
    "    file.close()  \n",
    "\n",
    "    file = open(p_root / 'Train/padding_1.plk', 'rb')\n",
    "    padding_1 = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "    file = open(p_root / 'Train/padding_2.plk', 'rb')\n",
    "    padding_2 = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "    return id_1, id_2, padding_1, padding_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id1,id2,p1,p2 = load_meta()\n",
    "\n",
    "len(id1)\n",
    "\n",
    "#id1[0]\n",
    "\n",
    "#print(id1)\n",
    "#print(id2)\n",
    "#print(p1)\n",
    "#print(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(id1))\n",
    "print(len(id2))\n",
    "print(len(p1))\n",
    "print(len(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_label = np.unique(labels)\n",
    "top_idx = []\n",
    "for k in all_label:\n",
    "    top_idx.append(np.where(labels != k)[0])\n",
    "\n",
    "for h, hh in enumerate(top_idx):\n",
    "    print(np.unique(labels[top_idx[h]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gen data from list idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = datasets.MNIST(root=p_root, train=True, download=True, transform=T.ToTensor())\n",
    "ds_valid = datasets.MNIST(root=p_root, train=False, download=True, transform=T.ToTensor())\n",
    "\n",
    "images=ds_train.data\n",
    "labels=ds_train.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= ds_train[0][0]\n",
    "\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def __len__():\n",
    "    return len(id1)\n",
    "\n",
    "__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id1,id2,p1,p2\n",
    "def mm_clamp(mmn_idx):\n",
    "    base  = T.Pad(padding=p1[mmn_idx])(ds_train[id1[mmn_idx]][0])\n",
    "    top   = T.Pad(padding=p2[mmn_idx])(ds_train[id2[mmn_idx]][0])\n",
    "    merge = torch.clamp(base + top, min=0, max=1)\n",
    "    return merge\n",
    "\n",
    "mmn_idx =0\n",
    "a = mm_clamp(mmn_idx)\n",
    "plt.imshow(a.permute(1,2,0).numpy())\n",
    "plt.show()\n",
    "\n",
    "print(labels[id1[mmn_idx]])\n",
    "print(labels[id2[mmn_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!! nicht so, sonst error!!!\n",
    "\n",
    "\n",
    "#id1,id2,p1,p2\n",
    "def mm_clamp2(mmn_idx):\n",
    "    base  = T.Pad(padding=p1[mmn_idx])(images[id1[mmn_idx]])\n",
    "    top   = T.Pad(padding=p2[mmn_idx])(images[id2[mmn_idx]])\n",
    "    merge = torch.clamp(base + top, min=0, max=1)\n",
    "    return merge\n",
    "\n",
    "\n",
    "a = mm_clamp2(2)\n",
    "a.shape\n",
    "plt.imshow(a.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images=ds_train.data\n",
    "#labels=ds_train.targets\n",
    "\n",
    "def __getitem__(mmn_idx):\n",
    "    x = mm_clamp(mmn_idx)\n",
    "    y = labels[id1[mmn_idx]]\n",
    "    z = labels[id2[mmn_idx]]\n",
    "    \"\"\"\n",
    "    if self.transform is not None:\n",
    "        x = self.transform(x)\n",
    "        \n",
    "    if self.target_transform is not None:\n",
    "        y = self.target_transform(y)\n",
    "        z = self.target_transform(z)\n",
    "    \"\"\"\n",
    "    return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmn_idx = 44157\n",
    "x,y,z = __getitem__(mmn_idx)\n",
    "\n",
    "print(y)\n",
    "print(z)\n",
    "\n",
    "plt.imshow(x.permute(1,2,0).numpy())\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmn_idx = np.arange(0,10000,1)\n",
    "Start = time.time()\n",
    "\n",
    "for i in mmn_idx:\n",
    "    x,y,z = __getitem__(i)\n",
    "\n",
    "\n",
    "End = time.time()\n",
    "\n",
    "\n",
    "print(End-Start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "import hashlib\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dotted_dict import DottedDict\n",
    "\n",
    "from datasets.multimnist import MultiMNist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = MultiMNist(root='/mnt/data/datasets/multimnist',train=True, generate=True, g_samples=[1000,1000])\n",
    "ds_valid = MultiMNist(root='/mnt/data/datasets/multimnist',train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ds_train))\n",
    "\n",
    "print(len(ds_train[0]))\n",
    "print(ds_train[0][0].shape)\n",
    "print(ds_train[0][0].type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = torch.utils.data.DataLoader(ds_train, \n",
    "                                    batch_size=16, \n",
    "                                    shuffle=False)\n",
    "\n",
    "dl_valid = torch.utils.data.DataLoader(ds_train, \n",
    "                                    batch_size=4000, \n",
    "                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,z = next(iter(dl_train))\n",
    "\n",
    "a,b,c = next(iter(dl_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "\n",
    "print(a.shape)\n",
    "\n",
    "i = 1\n",
    "\n",
    "print(y[i])\n",
    "print(z[i])\n",
    "\n",
    "plt.imshow(x[i].permute(1,2,0).numpy())\n",
    "plt.show()\n",
    "\n",
    "img = torchvision.utils.make_grid(x, nrow=x.shape[0])\n",
    "plt.imshow(img.permute(1,2,0))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "188faa17072d374bec02d17fca5e544867bade69f71230dfd1a560a6ca303930"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
