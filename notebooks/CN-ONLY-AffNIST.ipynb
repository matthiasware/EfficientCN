{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c42d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727bd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beeb573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "#\n",
    "\n",
    "from misc.plot_utils import plot_mat, imshow\n",
    "from effcn.layers import FCCaps, FCCapsWOBias, Squash\n",
    "from effcn.utils import count_parameters\n",
    "from effcn.functions import margin_loss\n",
    "from datasets import AffNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7fac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be8450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54f86e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = None\n",
    "transform_valid = None # converts [0,255] to [0,1] by dividing through 255\n",
    "\n",
    "p_data = '/home/matthias/projects/EfficientCN/data'\n",
    "\n",
    "ds_mnist_train = AffNIST(p_root=p_data, split=\"mnist_train\", download=True, transform=transform_train, target_transform=None)\n",
    "ds_mnist_valid = AffNIST(p_root=p_data, split=\"mnist_valid\", download=True, transform=transform_valid, target_transform=None)\n",
    "ds_affnist_valid = AffNIST(p_root=p_data, split=\"affnist_valid\", download=True, transform=transform_valid, target_transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae53463",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_mnist_train = torch.utils.data.DataLoader(\n",
    "    ds_mnist_train, \n",
    "    batch_size=256, \n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4)\n",
    "dl_mnist_valid= torch.utils.data.DataLoader(\n",
    "    ds_mnist_valid, \n",
    "    batch_size=256, \n",
    "    shuffle=True, \n",
    "    pin_memory=True,\n",
    "    num_workers=4)\n",
    "dl_affnist_valid= torch.utils.data.DataLoader(\n",
    "    ds_affnist_valid, \n",
    "    batch_size=256, \n",
    "    shuffle=True, \n",
    "    pin_memory=True,\n",
    "    num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1129f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = next(iter(dl_mnist_train))\n",
    "x_vis_train = x[:32]\n",
    "\n",
    "x, _ = next(iter(dl_mnist_valid))\n",
    "x_vis_mnist_valid = x[:32]\n",
    "\n",
    "x, _ = next(iter(dl_affnist_valid))\n",
    "x_vis_affnist_valid = x[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c9f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torchvision.utils.make_grid(x_vis_train).permute(1,2,0))\n",
    "plt.show()\n",
    "#\n",
    "plt.imshow(torchvision.utils.make_grid(x_vis_mnist_valid).permute(1,2,0))\n",
    "plt.show()\n",
    "#\n",
    "plt.imshow(torchvision.utils.make_grid(x_vis_affnist_valid).permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5124e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalPrimeCaps(nn.Module):\n",
    "    def __init__(self, n_l, n_h, d_l, d_h):\n",
    "        super().__init__()\n",
    "        self.n_l = n_l\n",
    "        self.n_h = n_h\n",
    "        self.d_l = d_l\n",
    "        self.d_h = d_h\n",
    "        \n",
    "        self.W = torch.nn.Parameter(torch.rand(n_h, d_l, d_h), requires_grad=True)\n",
    "        self.squash = Squash(eps=1e-20)\n",
    "        \n",
    "        torch.nn.init.kaiming_normal_(\n",
    "            self.W, a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "        self.attention_scaling = np.sqrt(d_l)\n",
    "    \n",
    "    def forward(self, U_l):\n",
    "        \"\"\"\n",
    "            In:  U_l\n",
    "            Out: U_h\n",
    "            DIMS:\n",
    "                U_l  (n_l, d_l)\n",
    "                U_h  (n_h, d_h)\n",
    "                W    (n_h, d_l, d_h)\n",
    "        \"\"\"\n",
    "        U_hat = torch.einsum(\"...ij, ...kjl -> ...ikl\", U_l, self.W)\n",
    "        A = torch.einsum(\"...ikl, ...hkl -> ...hik\", U_hat, U_hat)\n",
    "        A_scaled = A / self.attention_scaling\n",
    "        A_sum = torch.einsum(\"...hij->...hj\", A_scaled)\n",
    "        C = torch.softmax(A_sum, dim=-1)\n",
    "        U_h = torch.einsum('...ikl,...ik->...kl', U_hat, C)\n",
    "        return self.squash(U_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2582b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualCapsules(nn.Module):\n",
    "    \"\"\"\n",
    "        Add Dropout ?\n",
    "    \"\"\"\n",
    "    def __init__(self, h, w, c, patch_hw, d_out):\n",
    "        super().__init__()\n",
    "        self.h = h                    # img height\n",
    "        self.w = w                    # img width\n",
    "        self.c = c                    # img channels\n",
    "        self.d_out = d_out            # embedding dim\n",
    "        self.patch_hw = patch_hw      # patch size = patch_h, patch_w\n",
    "        \n",
    "        self.n_patches = h//patch_hw * w//patch_hw  # = #capsules\n",
    "        self.n_caps = self.n_patches\n",
    "        \n",
    "        # make sure it adds up\n",
    "        self.d_patch = patch_hw**2 * c \n",
    "        assert self.n_patches * self.d_patch == h * w * c\n",
    "\n",
    "        self.pos_emb = nn.Parameter(\n",
    "            torch.rand(1, self.n_patches, d_out),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        self.patch_emb = nn.Sequential(\n",
    "            nn.Linear(self.d_patch, self.d_out),\n",
    "        )\n",
    "        dd = 256\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.d_out, dd),\n",
    "            #nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dd, dd),\n",
    "            #nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dd, dd),\n",
    "            #nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dd, self.d_out),\n",
    "            Squash(eps=1e-20)\n",
    "        )\n",
    "        self.emb_dropout = nn.Dropout(0.1)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            in:  x (b, c, h, w)\n",
    "            out: (b, patch_hw, d)\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        x = rearrange(\n",
    "            x, 'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',\n",
    "                                patch_x=self.patch_hw, patch_y=self.patch_hw)\n",
    "        x = self.patch_emb(x)\n",
    "        x = x + self.pos_emb\n",
    "        x = self.emb_dropout(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "    \n",
    "class CapsOnly(nn.Module):\n",
    "    def __init__(self, h, w, c, patch_hw, dim, n_pc, n_dc):\n",
    "        super().__init__()\n",
    "        self.caps_visual = VisualCapsules(h,w,c,patch_hw,dim)\n",
    "        self.caps_parts = UniversalPrimeCaps(n_l=self.caps_visual.n_caps, n_h=n_pc, d_l=dim, d_h=dim)\n",
    "        self.caps_digits = FCCaps(n_l=n_pc, n_h=n_dc, d_l=dim, d_h=dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.caps_visual(x)\n",
    "        x = self.caps_parts(x)\n",
    "        x = self.caps_digits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b790b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 32\n",
    "patch_hw = 8\n",
    "n_pc = 16\n",
    "n_dc = 10\n",
    "\n",
    "model = CapsOnly(40, 40, 1, patch_hw=patch_hw, dim=d, n_pc=n_pc, n_dc=n_dc)  # 93, 52\n",
    "model = model.to(device)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b5c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27eb805",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 51\n",
    "#\n",
    "for epoch_idx in range(num_epochs):\n",
    "    # ####################\n",
    "    # TRAIN\n",
    "    # ####################\n",
    "    model.train()\n",
    "    desc = \"Train [{:3}/{:3}]:\".format(epoch_idx, num_epochs)\n",
    "    pbar = tqdm(dl_mnist_train, bar_format=desc + '{bar:10}{r_bar}{bar:-10b}')\n",
    "    \n",
    "    for x,y_true in pbar:\n",
    "        x = x.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        u_h = model.forward(x)\n",
    "        \n",
    "        # LOSS\n",
    "        y_one_hot = F.one_hot(y_true, num_classes=10)\n",
    "        loss = margin_loss(u_h, y_one_hot)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        y_pred = torch.argmax(torch.norm(u_h, dim=2), dim=1)\n",
    "        acc = (y_true == y_pred).sum() / y_true.shape[0]\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "                {'loss': loss.item(),\n",
    "                 'acc': acc.item()\n",
    "                 }\n",
    "        )\n",
    "    lr_scheduler.step()\n",
    "    #\n",
    "    # ####################\n",
    "    # VALID\n",
    "    # ####################\n",
    "    if epoch_idx % 5 != 0:\n",
    "        continue\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x,y_true in dl_mnist_valid:\n",
    "        x = x.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            u_h = model.forward(x)\n",
    "            \n",
    "            y_pred = torch.argmax(torch.norm(u_h, dim=2), dim=1)\n",
    "            total_correct += (y_true == y_pred).sum()\n",
    "            total += y_true.shape[0]\n",
    "    print(\"   mnist acc_valid: {:.3f}\".format(total_correct / total))\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x,y_true in dl_affnist_valid:\n",
    "        x = x.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            u_h = model.forward(x)\n",
    "            \n",
    "            y_pred = torch.argmax(torch.norm(u_h, dim=2), dim=1)\n",
    "            total_correct += (y_true == y_pred).sum()\n",
    "            total += y_true.shape[0]\n",
    "    print(\"   affnist acc_valid: {:.3f}\".format(total_correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43793a9",
   "metadata": {},
   "source": [
    "93, 57\n",
    "94, 54\n",
    "### Results\n",
    "- pos emb\n",
    "- dropout\n",
    "d = 32\n",
    "patch_hw = 8\n",
    "n_pc = 16\n",
    "n_dc = 10\n",
    "nl = 256\n",
    "\n",
    "-> 91., 51\n",
    "\n",
    "### NEW\n",
    "d = 32\n",
    "patch_hw = 8\n",
    "n_pc = 16\n",
    "n_dc = 10\n",
    "\n",
    "nl = 0 84, 39\n",
    "nl = 1 82, 39\n",
    "\n",
    "dh = 64\n",
    "nl = 2 86, 42\n",
    "nl = 3 85, 45\n",
    "nl = 4 86, 45\n",
    "\n",
    "dh = 128\n",
    "nl = 2 87, 44\n",
    "nl = 3 88, 46\n",
    "nl = 4 89, 46\n",
    "\n",
    "dh = 256\n",
    "nl = 2 88, 43\n",
    "nl = 3 91, 47\n",
    "nl = 4 91, 50\n",
    "\n",
    "dh = 512\n",
    "nl = 2 90, 47\n",
    "nl = 3 92, 49\n",
    "nl = 4 92, 50\n",
    "\n",
    "dh = 1024\n",
    "nl = 2 90, 48\n",
    "nl = 3 92, 51\n",
    "nl = 4 -\n",
    "\n",
    "\n",
    "### OLD\n",
    "##### dh\n",
    "d = 32\n",
    "patch_hw = 8\n",
    "n_pc = 16\n",
    "n_dc = 10\n",
    "\n",
    "dh = 512 91, 50\n",
    "dh = 256 91, 50\n",
    "dh = 128 89, 46\n",
    "\n",
    "-> 256\n",
    "\n",
    "##### patch_hw\n",
    "dh = 256\n",
    "d = 32\n",
    "n_pc = 16\n",
    "n_dc = 10\n",
    "\n",
    "patch_hw = 4   -> 82, 41\n",
    "patch_hw = 5   -> 83, 40\n",
    "patch_hw = 8   -> 90, 47\n",
    "patch_hw = 10   -> 93, 53\n",
    "patch_hw = 20   -> 94, 49\n",
    "\n",
    "---> 10\n",
    "\n",
    "##### n_pc\n",
    "dh = 256\n",
    "d = 32\n",
    "n_pc = 16\n",
    "n_dc = 10\n",
    "patch_hw = 10\n",
    "\n",
    "n_pc = 4  94, 51\n",
    "n_pc = 8  94, 54\n",
    "n_pc = 10 94, 54\n",
    "n_pc = 16 93, 54\n",
    "n_pc = 32 94, 53\n",
    "n_pc = 64 94, 52\n",
    "\n",
    "##### d\n",
    "dh = 256\n",
    "n_dc = 10\n",
    "patch_hw = 10\n",
    "n_pc = 16\n",
    "\n",
    "d = 8  83,43\n",
    "d = 16 92,49\n",
    "d = 32 94,52\n",
    "d = 64 94,54"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa86704",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c9846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, cmap=\"gray\", vmin=None, vmax=None):\n",
    "    npimg = img.detach().cpu().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap=cmap, vmin=None, vmax=None)\n",
    "    plt.show()\n",
    "\n",
    "def visTensor(tensor, ch=0, allkernels=False, nrow=8, padding=1):\n",
    "    tensor = tensor.cpu()\n",
    "    n,c,w,h = tensor.shape\n",
    "\n",
    "    if allkernels: tensor = tensor.view(n*c, -1, w, h)\n",
    "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
    "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
    "    plt.figure( figsize=(nrow,rows) )\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "    #return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f453a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ec4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "YY = []\n",
    "CC = []\n",
    "UUH = []\n",
    "UUHSQ = []\n",
    "UUHFIN = []\n",
    "UUL = []\n",
    "\n",
    "for X, Y in dl_valid:\n",
    "    X = X.to(device)\n",
    "    X = model.caps_visual(X)\n",
    "    U_l = model.caps_parts(X)\n",
    "    U_hat, A, A_scaled, A_sum, C, U_h_fin, U_h_sq = model.caps_digits.forward_debug(U_l)\n",
    "    \n",
    "    UUL.append(U_l.detach().cpu().numpy())\n",
    "    UUH.append(U_hat.detach().cpu().numpy())\n",
    "    YY.append(Y.numpy())\n",
    "    CC.append(C.detach().cpu().numpy())\n",
    "    UUHSQ.append(U_h_sq.detach().cpu())\n",
    "    UUHFIN.append(U_h_fin.detach().cpu())\n",
    "YY = np.concatenate(YY)\n",
    "CC = np.concatenate(CC)\n",
    "UUHSQ = np.concatenate(UUHSQ)\n",
    "UUHFIN = np.concatenate(UUHFIN)\n",
    "UUH = np.concatenate(UUH)\n",
    "UUL = np.concatenate(UUL)\n",
    "\n",
    "print(YY.shape)\n",
    "print(CC.shape)\n",
    "\n",
    "print(\"U_l      \", U_l.shape[1:])\n",
    "print(\"U_hat    \", U_hat.shape[1:])\n",
    "print(\"A        \", A.shape[1:])\n",
    "print(\"A_scaled \", A_scaled.shape[1:])\n",
    "print(\"A_sum    \", A_sum.shape[1:])\n",
    "print(\"C        \", C.shape[1:])\n",
    "print(\"U_h_fin  \", U_h_fin.shape[1:])\n",
    "print(\"U_h_sq   \", U_h_sq.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7abf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "idcs = np.where(YY == 0)\n",
    "Y = YY[idcs]\n",
    "C = CC[idcs]\n",
    "UH = UUH[idcs]\n",
    "UHS = UUHSQ[idcs]\n",
    "UHF = UUHFIN[idcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27f9ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx in range(5):\n",
    "    print(\"#\"*100)\n",
    "    y = Y[idx]\n",
    "    c = C[idx]\n",
    "    uhs = UHS[idx]\n",
    "    uhf = UHF[idx]\n",
    "    ul = UUL[idx]\n",
    "    #plot_mat(ul, scale_factor=0.4, title=\"U_l = lower level capsules\")\n",
    "    plot_mat(c, scale_factor=0.4, title=\"C\")\n",
    "    #plot_mat(uhf, scale_factor=0.4, title=\"U_h, upper layer capsules w/o squash\")\n",
    "    #plot_mat(uhs, scale_factor=0.4, title=\"squash(U_h)\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 2))\n",
    "    axes[0].bar(range(10), c.mean(axis=0))\n",
    "    axes[0].set_title(\"C\")\n",
    "    axes[3].bar(range(10), np.linalg.norm(uhs, axis=1))\n",
    "    axes[3].set_title(\"sqash(U_h)\")\n",
    "    axes[2].bar(range(10), np.linalg.norm(uhf, axis=1))\n",
    "    axes[2].set_title(\"U_h without Squash\")\n",
    "    axes[1].bar(range(ul.shape[0]), np.linalg.norm(ul, axis=1))\n",
    "    axes[1].set_title(\"U_l\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4b0d3d",
   "metadata": {},
   "source": [
    "# Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7be6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_p = int(np.sqrt(model.caps_visual.n_patches))\n",
    "all_pe = model.caps_visual.pos_emb.data.detach().cpu()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.repeat_interleave(49).reshape((49, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ffdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab819c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(n_p, n_p, figsize=(10, 10))\n",
    "for ix in range(n_p):\n",
    "    for iy in range(n_p):\n",
    "        ax = axes[ix][iy]\n",
    "        pe = all_pe[ix * n_p + iy]\n",
    "        pe = pe.repeat_interleave(49).reshape((49, -1))\n",
    "        cossim = F.cosine_similarity(pe, all_pe)\n",
    "        ax.imshow(cossim.reshape((7, 7)), vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f338a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cossim.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b17342",
   "metadata": {},
   "outputs": [],
   "source": [
    "cossim.reshape((7, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deef2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e0d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cosine_similarity(pe, pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc35be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.view((7, 7, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29e477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a684f26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
