{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "import hashlib\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dotted_dict import DottedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_root='/mnt/data/datasets/multimnist_test'\n",
    "p_root=Path(p_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mkdir_directories(dirs, parents, exist_ok):\n",
    "    for director in dirs:\n",
    "        Path(director).mkdir(parents=parents, exist_ok=exist_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __pad_rand__():\n",
    "        ref = 8\n",
    "        left = np.random.randint(1, high=9, size=None, dtype=int)\n",
    "        rigth = ref - left\n",
    "        up = np.random.randint(1, high=9, size=None, dtype=int)\n",
    "        down = ref - up\n",
    "        \n",
    "        return [left, up, rigth, down]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist\n",
    "ds_train = datasets.MNIST(root=p_root, train=True, download=True, transform=T.ToTensor())\n",
    "ds_valid = datasets.MNIST(root=p_root, train=False, download=True, transform=T.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "p_data = p_root / \"Train\"\n",
    "p_imgs = p_data / 'Img'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=ds_train.data.unsqueeze(1)\n",
    "labels=ds_train.targets\n",
    "n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".unsqueeze(1) -> before loop\n",
    "label1 = labels[j].item() -> give label item insted of tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6000, 1, 28, 28])\n",
      "499960\n",
      "60000\n",
      "499960\n",
      "60000\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "mkdir_directories([p_data, p_imgs], parents=True, exist_ok=True)\n",
    "\n",
    "Start = time.time()\n",
    "\n",
    "\n",
    "#lists\n",
    "all_targets1 = []\n",
    "all_targets2 = []\n",
    "\n",
    "#generator index\n",
    "index = 1\n",
    "\n",
    "\n",
    "StartLoop = time.time()\n",
    "#test dataset\n",
    "test = images[0:6000]\n",
    "print(test.size())\n",
    "for j, image in enumerate(test):\n",
    "\n",
    "#generate for whole dataset\n",
    "#for j, image in enumerate(images):\n",
    "\n",
    "    #reference img\n",
    "    img_ref = images[j]\n",
    "    lab_ref = labels[j]\n",
    "\n",
    "    #choose random top images from different classes\n",
    "    top_idx = np.where(labels != lab_ref)[0]\n",
    "    top_idx = np.random.choice(top_idx,n,replace=False)\n",
    "\n",
    "    #generate images\n",
    "    for i, idx in enumerate(top_idx):\n",
    "        \n",
    "        #randomize position\n",
    "        base  = T.Pad(padding=__pad_rand__())(images[j])\n",
    "        top   = T.Pad(padding=__pad_rand__())(images[top_idx[i]])\n",
    "        \n",
    "        #merge images\n",
    "        merge = torch.clamp(base + top,min=0, max=1)\n",
    "        #merge_s = merge.unsqueeze(0)\n",
    "\n",
    "        \n",
    "        #add labels to list\n",
    "        label1 = labels[j].item()\n",
    "        label2 = labels[top_idx[i]].item()\n",
    "        all_targets1.append(label1)\n",
    "        all_targets2.append(label2)\n",
    "\n",
    "        #Save Img as png\n",
    "        #a = plt.imshow(merge.float().permute(1,2,0))\n",
    "        #a.savefig(p_imgs / \"{:08d}.png\".format(index))\n",
    "        #a.close()\n",
    "\n",
    "        #img = Image.fromarray(merge.float().numpy(), mode='L')\n",
    "        #img.save( p_imgs / \"{:08d}.png\".format(index))\n",
    "        torchvision.utils.save_image(merge.float(), p_imgs / \"{:08d}.png\".format(index))\n",
    "        index += 1 \n",
    "\n",
    "EndLoop = time.time()\n",
    "print(sys.getsizeof(all_targets1))\n",
    "print(len(all_targets1))\n",
    "print(sys.getsizeof(all_targets2))\n",
    "print(len(all_targets2))\n",
    "\n",
    "\n",
    "#create target 1\n",
    "file_targets1 = open(p_data /'targets_1.plk', 'wb')\n",
    "pickle.dump(all_targets1, file_targets1)\n",
    "file_targets1.close()\n",
    "\n",
    "#create target 2\n",
    "file_targets2 = open(p_data /'targets_2.plk', 'wb')\n",
    "pickle.dump(all_targets2, file_targets2)\n",
    "file_targets2.close()\n",
    "\n",
    "End = time.time()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3078703703703705"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 113 / 60000\n",
    "b = a * 60000000\n",
    "b / (60 * 60 * 24)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_targets1[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.98723864555359\n",
      "112.98349809646606\n",
      "0.003740549087524414\n"
     ]
    }
   ],
   "source": [
    "t_all = End - Start\n",
    "t_loop = EndLoop - StartLoop\n",
    "print(t_all)\n",
    "print(t_loop)\n",
    "print(t_all-t_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5270397663116455\n",
    "1.5180883407592773\n",
    "1.39308500289917\n",
    "12.76402497291565"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_train.permute(1,2,0))\n",
    "plt.tight_layout()\n",
    "plt.savefig(p_imgs / \"img_train_{:03d}.png\".format(epoch_idx))\n",
    "plt.tight_layout()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.fromarray(img.numpy(), mode='L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F51C02605B0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img =ds_train.data[0]\n",
    "\n",
    "img.shape\n",
    "\n",
    "img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 1, 28, 28])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.data.shape\n",
    "\n",
    "ds = ds_train.data.unsqueeze(1).float()\n",
    "ds.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadtrageds():\n",
    "    #load target files\n",
    "    file = open(p_root / 'Train/targets_1.plk', 'rb')\n",
    "    targets_1 = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "    file = open(p_root / 'Train/targets_2.plk', 'rb')\n",
    "    targets_2 = pickle.load(file)\n",
    "    file.close()  \n",
    "\n",
    "    #for i, target in enumerate(targets_1):\n",
    "    # \tprint(i+1, targets_1[i], targets_2[i])\n",
    "    return targets_1, targets_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1, t2 = loadtrageds()\n",
    "len(t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split in preparing and gerneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __pad_rand_l__(m=1,n=1):\n",
    "        ref = 8\n",
    "        left = np.random.randint(1, high=9, size=(m*n), dtype=int)\n",
    "        rigth = ref - left\n",
    "        up = np.random.randint(1, high=9, size=(m*n), dtype=int)\n",
    "        down = ref - up\n",
    "\n",
    "        l_stack = np.stack((left,up, rigth, down), axis=1).tolist()\n",
    "        \n",
    "        return l_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000000\n"
     ]
    }
   ],
   "source": [
    "a = __pad_rand_l__(60000,1000)\n",
    "\n",
    "\n",
    "print(len(a))\n",
    "\n",
    "\n",
    "base  = T.Pad(padding=a[0])(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=ds_train.data\n",
    "labels=ds_train.targets\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "0.0029058456420898438\n",
      "0.001027822494506836\n",
      "0.0002522468566894531\n",
      "0.0008211135864257812\n",
      "0.01578664779663086\n"
     ]
    }
   ],
   "source": [
    "images=ds_train.data\n",
    "labels=ds_train.targets\n",
    "n = 2\n",
    "\n",
    "Start = time.time()\n",
    "# add 1 dim to mnist\n",
    "images = images.unsqueeze(1)\n",
    "\n",
    "# proof path\n",
    "mkdir_directories([p_data, p_imgs], parents=True, exist_ok=True)\n",
    "\n",
    "#generate index list for mmnist\n",
    "all_ref = []\n",
    "all_top = []\n",
    "\n",
    "test = labels[0:2]\n",
    "print(test.size())\n",
    "\n",
    "StartLoop = time.time()\n",
    "for j, label in enumerate(test):\n",
    "\n",
    "    top_idx = np.where(labels != label)[0]\n",
    "    list_top = np.random.choice(top_idx,n,replace=False).tolist()\n",
    "    list_ref = np.full(n,j).tolist()\n",
    "\n",
    "    all_ref.extend(list_ref)\n",
    "    all_top.extend(list_top)\n",
    "\n",
    "EndLoop = time.time()\n",
    "\n",
    "StartPickle = time.time()\n",
    "#save labels\n",
    "#create target 1\n",
    "targets1 = labels[all_ref].tolist()\n",
    "file_targets1 = open(p_data /'targets_1.plk', 'wb')\n",
    "pickle.dump(targets1, file_targets1)\n",
    "file_targets1.close()\n",
    "\n",
    "#create target 2\n",
    "targets2 = labels[all_top].tolist()\n",
    "file_targets2 = open(p_data /'targets_2.plk', 'wb')\n",
    "pickle.dump(targets2, file_targets2)\n",
    "file_targets2.close()\n",
    "\n",
    "EndPickle = time.time()\n",
    "\n",
    "StartPattern = time.time()\n",
    "#create rand pattern pos\n",
    "pattern1 = __pad_rand_l__(len(test),n)\n",
    "pattern2 = __pad_rand_l__(len(test),n)\n",
    "\n",
    "EndPattern = time.time()\n",
    "\n",
    "StartImgGen = time.time()\n",
    "all_merge = []\n",
    "all_file = []\n",
    "#generate images\n",
    "for i, ref in enumerate(all_ref):\n",
    "    base  = T.Pad(padding=pattern1[i])(images[all_ref[i]])\n",
    "    top   = T.Pad(padding=pattern2[i])(images[all_top[i]])\n",
    "\n",
    "    merge = torch.clamp(base + top,min=0, max=1)\n",
    "    all_merge.append(merge.float())\n",
    "    all_file.append(p_imgs / \"{:08d}.png\".format((i+1)))\n",
    "    #torchvision.utils.save_image(merge.float(), p_imgs / \"{:08d}.png\".format((i+1)))\n",
    "\n",
    "\n",
    "EndImgGen = time.time()\n",
    "End = time.time()\n",
    "print(EndLoop - StartLoop)\n",
    "print(EndPickle - StartPickle)\n",
    "print(EndPattern - StartPattern)\n",
    "print(EndImgGen - StartImgGen)\n",
    "print(End-Start)  \n",
    "\n",
    "#gen idx lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1, t2 = loadtrageds()\n",
    "len(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.FloatTensor\n",
      "[PosixPath('/mnt/data/datasets/multimnist_test/Train/Img/00000001.png'), PosixPath('/mnt/data/datasets/multimnist_test/Train/Img/00000002.png'), PosixPath('/mnt/data/datasets/multimnist_test/Train/Img/00000003.png'), PosixPath('/mnt/data/datasets/multimnist_test/Train/Img/00000004.png')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Image.fromarray(images[all_top[3]].squeeze(0).numpy(), mode='L')\n",
    "\n",
    "print(len(all_merge))\n",
    "print(all_merge[0].type())\n",
    "print(all_file)\n",
    "\n",
    "#torchvision.utils.save_image(all_merge, all_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "9\n",
      "[13579, 51211, 35493, 35787, 47094, 8505, 39726, 47292, 34141]\n",
      "[5, 5, 5, 0, 0, 0, 4, 4, 4]\n",
      "[5, 5, 5, 0, 0, 0, 4, 4, 4]\n",
      "[3, 3, 4, 5, 4, 3, 2, 9, 1]\n",
      "[3, 3, 4, 5, 4, 3, 2, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "print(type(all_ref))\n",
    "print(len(all_ref))\n",
    "print(all_top)\n",
    "\n",
    "\n",
    "print(labels[all_ref].tolist())\n",
    "print(targets1)\n",
    "print(labels[all_top].tolist())\n",
    "print(targets2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ref.extend(list_ref)\n",
    "\n",
    "len(all_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.921459492888065"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "60000 / 111.27461862564087\n",
    "\n",
    "#126 * 1000 / (3600)\n",
    "\n",
    "\n",
    "60000000 / 539 /(60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6000])\n",
      "61.99512219429016\n",
      "0.05639910697937012\n",
      "0.07731962203979492\n",
      "62.14114308357239\n"
     ]
    }
   ],
   "source": [
    "images=ds_train.data\n",
    "labels=ds_train.targets\n",
    "n = 10\n",
    "\n",
    "Start = time.time()\n",
    "# add 1 dim to mnist\n",
    "images = images.unsqueeze(1)\n",
    "\n",
    "# proof path\n",
    "mkdir_directories([p_data, p_imgs], parents=True, exist_ok=True)\n",
    "\n",
    "#generate index list for mmnist\n",
    "all_ref = []\n",
    "all_top = []\n",
    "\n",
    "test = labels[0:6000]\n",
    "print(test.size())\n",
    "\n",
    "StartLoop = time.time()\n",
    "for j, label in enumerate(labels):\n",
    "\n",
    "    top_idx = np.where(labels != label)[0]\n",
    "    list_top = np.random.choice(top_idx,n,replace=False).tolist()\n",
    "    list_ref = np.full(n,j).tolist()\n",
    "\n",
    "    all_ref.extend(list_ref)\n",
    "    all_top.extend(list_top)\n",
    "\n",
    "EndLoop = time.time()\n",
    "\n",
    "StartPickle = time.time()\n",
    "\n",
    "#create target 1 id's\n",
    "file_targets1 = open(p_data /'targets_1_id.plk', 'wb')\n",
    "pickle.dump(all_ref, file_targets1)\n",
    "file_targets1.close()\n",
    "\n",
    "#create target 2\n",
    "file_targets2 = open(p_data /'targets_2_id.plk', 'wb')\n",
    "pickle.dump(all_top, file_targets2)\n",
    "file_targets2.close()\n",
    "\n",
    "EndPickle = time.time()\n",
    "\n",
    "StartPattern = time.time()\n",
    "#create rand pattern pos\n",
    "pattern1 = __pad_rand_l__(len(test),n)\n",
    "pattern2 = __pad_rand_l__(len(test),n)\n",
    "\n",
    "#create pattern 1 \n",
    "file_targets1 = open(p_data /'pattern_1.plk', 'wb')\n",
    "pickle.dump(pattern1, file_targets1)\n",
    "file_targets1.close()\n",
    "\n",
    "#create pattern 2 \n",
    "file_targets1 = open(p_data /'pattern_2.plk', 'wb')\n",
    "pickle.dump(pattern2, file_targets1)\n",
    "file_targets1.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EndPattern = time.time()\n",
    "\n",
    "\n",
    "End = time.time()\n",
    "print(EndLoop - StartLoop)\n",
    "print(EndPickle - StartPickle)\n",
    "print(EndPattern - StartPattern)\n",
    "print(End-Start)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meta():\n",
    "    #load meta profil for mmnist\n",
    "    file = open(p_root / 'Train/targets_1_id.plk', 'rb')\n",
    "    id_1 = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "    file = open(p_root / 'Train/targets_2_id.plk', 'rb')\n",
    "    id_2 = pickle.load(file)\n",
    "    file.close()  \n",
    "\n",
    "    file = open(p_root / 'Train/pattern_1.plk', 'rb')\n",
    "    pattern_1 = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "    file = open(p_root / 'Train/pattern_2.plk', 'rb')\n",
    "    pattern_2 = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "    return id_1, id_2, pattern_1, pattern_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600000"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id1,id2,p1,p2 = load_meta()\n",
    "\n",
    "len(id1)\n",
    "\n",
    "#print(id1)\n",
    "#print(id2)\n",
    "#print(p1)\n",
    "#print(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100/60"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "188faa17072d374bec02d17fca5e544867bade69f71230dfd1a560a6ca303930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('EffCN': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
