{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b9baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dotted-dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2381122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pprint\n",
    "import hashlib\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dotted_dict import DottedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b49a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DottedDict({\n",
    "    'mnist_train': {},\n",
    "    'mnist_valid': {},\n",
    "    'affnist_train': {},\n",
    "    'affnist_valid': {}\n",
    "})\n",
    "config.p_data_root_mnist = \"./../data\"\n",
    "config.p_data_affnist = \"./../data/AFFNIST\"\n",
    "\n",
    "config.img_height = 40\n",
    "config.img_width = 40\n",
    "\n",
    "# MNIST TRAIN CONFIG\n",
    "config.mnist_train.samples_per_sample = 1\n",
    "config.mnist_train.strip_zeros = False\n",
    "config.mnist_train.file_name = \"mnist_train.pt\"\n",
    "\n",
    "# MNIST VALID CONFIG\n",
    "config.mnist_valid.samples_per_sample = 1\n",
    "config.mnist_valid.strip_zeros = False\n",
    "config.mnist_valid.file_name = \"mnist_valid.pt\"\n",
    "\n",
    "# AFFNIST TRAIN CONFIG\n",
    "config.rotate_degrees = (-20, 20)\n",
    "config.sheer_degrees = (-40, 40)\n",
    "config.scale = (0.8, 1.2)\n",
    "\n",
    "config.affnist_train.samples_per_sample = 1\n",
    "config.affnist_train.file_name = \"affnist_train.pt\"\n",
    "\n",
    "# AFFNIST VALID CONFIG\n",
    "config.affnist_valid.samples_per_sample = 1\n",
    "config.affnist_valid.file_name = \"affnist_valid.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7645c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pprint = pp.pprint \n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd7ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset directory\n",
    "Path(config.p_data_affnist).mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e661d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_idcs_1d(T):\n",
    "    assert len(T.shape) == 1\n",
    "    for idx_from in range(len(T)):\n",
    "        if T[idx_from] > 0:\n",
    "            break\n",
    "    for idx_to in range(len(T) -1, 0, -1):\n",
    "        if T[idx_to] > 0:\n",
    "            idx_to += 1\n",
    "            break\n",
    "    return idx_from, idx_to\n",
    "\n",
    "def strip_tensor_2d(T):\n",
    "    assert len(T.shape) == 2\n",
    "    col_from, col_to = get_content_idcs_1d(T.sum(axis=0))\n",
    "    row_from, row_to = get_content_idcs_1d(T.sum(axis=1))\n",
    "    return T[row_from:row_to, col_from:col_to]\n",
    "\n",
    "def randomly_place(A, B=None, h_B=None, w_B=None):\n",
    "    \"\"\"\n",
    "        Randly place A into B\n",
    "        with a margin of 1\n",
    "    \"\"\"\n",
    "    assert len(A.shape) == 2\n",
    "    if B is not None:\n",
    "        assert len(B.shape) == 2\n",
    "        h_b, w_B = B.shape\n",
    "    else:\n",
    "        assert h_B is not None\n",
    "        assert w_B is not None\n",
    "        B = torch.zeros((h_B, w_B), dtype=A.dtype)\n",
    "        \n",
    "    h_A, w_A = A.shape\n",
    "    #\n",
    "    row_from = np.random.randint(low=0, high=h_B - h_A + 1)\n",
    "    row_to = row_from + h_A\n",
    "\n",
    "    col_from = np.random.randint(low=0, high=w_B - w_A + 1)\n",
    "    col_to = col_from + w_A\n",
    "    B[row_from:row_to, col_from:col_to] = A\n",
    "    return B\n",
    "\n",
    "def create_padded_mnist_tensors(train, strip, height, width, n_samples, p_root):\n",
    "    ds = datasets.MNIST(root=p_root, train=train, download=True, transform=None)\n",
    "    data, targets = ds.data, ds.targets\n",
    "\n",
    "    all_imgs = []\n",
    "    all_targets = []\n",
    "    for idx, img in enumerate(data):\n",
    "        if strip:\n",
    "            img = strip_tensor_2d(img)\n",
    "        all_imgs += torch.stack([randomly_place(img, h_B=height, w_B=width) for _ in range(n_samples)])\n",
    "    all_imgs = torch.stack(all_imgs)\n",
    "    all_targets = targets.repeat_interleave(n_samples)\n",
    "    return all_imgs, all_targets\n",
    "\n",
    "def create_affnist_tensors(train, height, width, n_samples, p_root, transforms):\n",
    "    ds = datasets.MNIST(root=p_root, train=train, download=True, transform=None)\n",
    "    data, targets = ds.data, ds.targets\n",
    "    \n",
    "    # padding\n",
    "    pad = (height - 28) // 2\n",
    "    assert 2*pad + 28 == height\n",
    "    assert height == width\n",
    "    \n",
    "    all_imgs = []\n",
    "    all_targets = []\n",
    "    for idx, img in enumerate(data):\n",
    "        for _ in range(n_samples):\n",
    "            # pad img to 40x40\n",
    "            img_aff = F.pad(input=img, pad=(pad, pad, pad, pad), mode='constant', value=0)\n",
    "\n",
    "            # rotation, scaling, sheering\n",
    "            img_aff = transforms(img_aff.unsqueeze(0)).squeeze()\n",
    "            r=n\n",
    "            # strip img\n",
    "            img_aff = strip_tensor_2d(img_aff)\n",
    "    \n",
    "            # translate\n",
    "            img_aff = randomly_place(img_aff, h_B=height, w_B=width)\n",
    "            \n",
    "            all_imgs.append(img_aff)\n",
    "    all_imgs = torch.stack(all_imgs)\n",
    "    all_targets = targets.repeat_interleave(n_samples)\n",
    "    return all_imgs, all_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b669632e",
   "metadata": {},
   "source": [
    "# MNIST Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dc93a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, targets = create_padded_mnist_tensors(\n",
    "    train=True,\n",
    "    strip=config.mnist_train.strip_zeros,\n",
    "    height=config.img_height,\n",
    "    width=config.img_width,\n",
    "    n_samples=config.mnist_train.samples_per_sample,\n",
    "    p_root=config.p_data_root_mnist\n",
    ")\n",
    "print(imgs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ea224",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vis = 8\n",
    "grid = torchvision.utils.make_grid(imgs[:n_vis].unsqueeze(1), nrow=config.mnist_train.samples_per_sample)\n",
    "plt.figure(figsize=(2 * 2, n_vis * 2))\n",
    "plt.imshow(grid.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[:n_vis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d0145",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_mnist_train = Path(config.p_data_affnist) / config.mnist_train.file_name\n",
    "\n",
    "# save stuff\n",
    "torch.save([imgs, targets], p_mnist_train)\n",
    "\n",
    "# try load\n",
    "imgs, targets = torch.load(p_mnist_train)\n",
    "print(imgs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ceedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashlib.md5(open(p_mnist_train,'rb').read()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7a815d",
   "metadata": {},
   "source": [
    "# MNIST Valid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc3ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, targets = create_padded_mnist_tensors(\n",
    "    train=False,\n",
    "    strip=config.mnist_valid.strip_zeros,\n",
    "    height=config.img_height,\n",
    "    width=config.img_width,\n",
    "    n_samples=config.mnist_valid.samples_per_sample,\n",
    "    p_root=config.p_data_root_mnist\n",
    ")\n",
    "print(imgs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe563a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vis = 8\n",
    "grid = torchvision.utils.make_grid(imgs[:n_vis].unsqueeze(1), nrow=config.mnist_train.samples_per_sample)\n",
    "plt.figure(figsize=(2 * 2, n_vis * 2))\n",
    "plt.imshow(grid.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ffa431",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[:n_vis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3c2e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_mnist_valid = Path(config.p_data_affnist) / config.mnist_valid.file_name\n",
    "\n",
    "# save stuff\n",
    "torch.save([imgs, targets], p_mnist_valid)\n",
    "\n",
    "# try load\n",
    "imgs, targets = torch.load(p_mnist_valid)\n",
    "print(imgs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashlib.md5(open(p_mnist_valid,'rb').read()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c7155",
   "metadata": {},
   "source": [
    "# AffNIST Train Data \n",
    "\n",
    "The AffNIST dataset is created by padding the orignal 28x28 MNIST images with 6 px to 40x40 and then randomly affine transform them via:\n",
    "- rotation within 20 degrees\n",
    "- sheering withing 45 degrees\n",
    "- scaling from 0.8 to 1.2 in both vertical and horizonal directions\n",
    "- translations within 8 pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f1fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36782862",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.RandomAffine(\n",
    "    degrees=config.rotate_degrees,\n",
    "    translate=None,  # we do that later manually\n",
    "    shear=config.sheer_degrees,\n",
    "    scale=config.scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc5ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, targets = create_affnist_tensors(\n",
    "    train=True,\n",
    "    height=config.img_height,\n",
    "    width=config.img_width,\n",
    "    n_samples=config.affnist_train.samples_per_sample,\n",
    "    p_root=config.p_data_root_mnist,\n",
    "    transforms=transforms\n",
    ")\n",
    "print(imgs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vis = 16\n",
    "grid = torchvision.utils.make_grid(imgs[:n_vis].unsqueeze(1), nrow=config.mnist_train.samples_per_sample)\n",
    "plt.figure(figsize=(2 * 2, n_vis * 2))\n",
    "plt.imshow(grid.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9902ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[:n_vis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741eabae",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_affnist_train = Path(config.p_data_affnist) / config.affnist_train.file_name\n",
    "\n",
    "# save stuff\n",
    "torch.save([imgs, targets], p_affnist_train)\n",
    "\n",
    "# try load\n",
    "imgs, targets = torch.load(p_affnist_train)\n",
    "print(imgs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2801013",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashlib.md5(open(p_affnist_train,'rb').read()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b1ec3",
   "metadata": {},
   "source": [
    "# AffNIST Valid Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b3464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.RandomAffine(\n",
    "    degrees=config.rotate_degrees,\n",
    "    translate=None,  # we do that later manually\n",
    "    shear=config.sheer_degrees,\n",
    "    scale=config.scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd098584",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, targets = create_affnist_tensors(\n",
    "    train=False,\n",
    "    height=config.img_height,\n",
    "    width=config.img_width,\n",
    "    n_samples=config.affnist_valid.samples_per_sample,\n",
    "    p_root=config.p_data_root_mnist,\n",
    "    transforms=transforms\n",
    ")\n",
    "print(imgs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8b595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vis = 16\n",
    "grid = torchvision.utils.make_grid(imgs[:n_vis].unsqueeze(1), nrow=config.mnist_train.samples_per_sample)\n",
    "plt.figure(figsize=(2 * 2, n_vis * 2))\n",
    "plt.imshow(grid.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(targets[:n_vis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ef989",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_affnist_valid = Path(config.p_data_affnist) / config.affnist_valid.file_name\n",
    "\n",
    "# save stuff\n",
    "torch.save([imgs, targets], p_affnist_valid)\n",
    "\n",
    "# try load\n",
    "imgs, targets = torch.load(p_affnist_valid)\n",
    "print(imgs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32d726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashlib.md5(open(p_affnist_valid,'rb').read()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bb8894",
   "metadata": {},
   "source": [
    "## AffNIST Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ee4d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./..\")\n",
    "from datasets import AffNIST\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbd5c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"affnist_valid\"\n",
    "transform = None\n",
    "target_transform = None\n",
    "p_root = './../data'\n",
    "\n",
    "ds = AffNIST(p_root, split, transform, target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d68551",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ds[0][0].squeeze())\n",
    "plt.title(ds[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42db7fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = None\n",
    "target_transform = None\n",
    "p_root = './../data'\n",
    "\n",
    "for split in AffNIST.SPLIT_FILES.values():\n",
    "    ds = AffNIST(p_root, split[:-3], transform, target_transform)\n",
    "    print(\"Split: {} length: {}\".format(split, len(ds)))\n",
    "    plt.imshow(ds[989][0].squeeze())\n",
    "    plt.title(ds[989][1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b5fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = T.Compose([T.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a409dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"affnist_valid\"\n",
    "transform = None\n",
    "target_transform = None\n",
    "p_root = './../data'\n",
    "\n",
    "ds = AffNIST(p_root, split, None, target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed2f25bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0][0].shape\n",
    "ds[0][0].min()\n",
    "ds[0][0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "043c0941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.uint8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.data.shape\n",
    "ds.data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "525e44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = ds.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "324d01c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([1, 40, 40])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f06c08526d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD6CAYAAABnLjEDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOHElEQVR4nO3df6xU5Z3H8c9nb3FLFhLBKrkRVmmrW03TUoPERjAo64bFJrSRmGq60sQEm6ymJqtbbOIiJCTWtbiJ2TShEWUTtorSVbNpXQlLwjYY8GLBolAF1gYIP+wCAdwIQb77xxy6U8653GHmnPn1vF/JzZ353nNmnhPuhzPz3DPP1xEhAP3vTzo9AADtQdiBRBB2IBGEHUgEYQcSQdiBRLQUdtuzbf/W9i7bC8saFIDyudm/s9sekPS+pNsl7ZP0lqS7I+K9C+zDH/WBikWEi+qtnNmnSdoVEXsi4rSkFyTNbeHxAFSolbBfKWlv3f19WQ1AF/pM1U9ge4GkBVU/D4ALayXs+yVNqrs/Mav9kYhYLmm5xHt2oJNaeRn/lqRrbE+2fYmkb0t6rZxhAShb02f2iDhj+wFJ/yFpQNKKiHi3tJEBKFXTf3pr6sl4GQ9Uroo/vQHoIYQdSARhBxJB2IFEEHYgEYQdSARhBxJB2IFEEHYgEYQdSARhBxJB2IFEEHYgEYQdSARhBxJB2IFEEHYgEYQdSERLS0nb/lDSCUmfSjoTEVPLGBSA8pWxbvytEfH7Eh4HQIV4GQ8kotWwh6Q3bG/JOr8A6FKtvoyfHhH7bV8haa3tnRGxoX4D2j8B3aG0deNtPy7pZEQ8dYFtWDceqFjp68bb/jPbY8/dlvRXkrY3+3gAqtXKy/gJkv7N9rnH+deIeL2UUQEoHe2fgD5D+ycgcYQdSARhBxJB2IFEEHYgEYQdSARhBxJB2IFEEHYgEYQdSARhBxJB2IFEEHYgEYQdSEQZq8uii23cuLHhbfft25erPfnkk7na0NBQS2NCZ3BmBxJB2IFEEHYgEYQdSMSIa9DZXiHpG5IOR8SXs9p4SS9KulrSh5LuioijIz5Zl65B99xzzxXWP/nkk1xt1apVudrBgwcL99+1a1eudjETZmWYOjXffu+jjz4q3HZwcLChx1y2bFlh/eGHH258YKhMK2vQPS9p9nm1hZLWRcQ1ktZl9wF0sRHDnnV4OXJeea6kldntlZK+We6wAJSt2b+zT4iIA9ntg6qtIV+I9k9Ad2j5opqIiAu9F4+I5ZKWS937nh1IQbOz8YdsD0pS9v1weUMCUIVmz+yvSZov6Yns+6uljahi8+bNy9Xmz5/f8P73339/w9ueOHEiVxs7dmzhtnv37s3VJk2a1PBzDWfGjBm52vvvv1+47Y4dO3K18ePH52q7d+9ueVxovxHP7LZ/JulNSX9he5/t+1QL+e22P5D0l9l9AF1sxDN7RNw9zI9mlTwWABXiCjogEYQdSERyLZv37NmTqz3zzDOF2xZNWF133XW52g033FC4/8yZM3O1e++9t3DbLVu25Go33nhj4bYXY926dbnanXfeWbjt6tWrc7Xt27fnarfeemvh/keOnH/tFTqBls1A4gg7kAjCDiSCsAOJIOxAIpKbjZ81K38tUNGMdRnGjRuXqx09OuIaH5U7dOhQYf2KK67I1YouL16zZk3pY0J5mI0HEkfYgUQQdiARhB1IRHITdJAWL15cWH/sscdytWPHjuVqt9xyS+H+RZfWov2YoAMSR9iBRBB2IBGEHUhEI2vQrbB92Pb2utrjtvfb3pp9zal2mABa1Uivt1sknZT0L3W93h6XdDIinrqoJ2M2viucOnWqsD5q1KhcrWgBjg0bNpQ9JJSo6dn4Ydo/Aegxrbxnf8D2O9nL/PwnPgB0lWbD/hNJX5A0RdIBST8ebkPbC2wP2R5q8rkAlKCpsEfEoYj4NCLOSvqppGkX2HZ5REyNiHyjcABt01T7J9uDdV1cvyWJ6yS71M0335yrFU3EScWf63/zzTdLHxM6Y8SwZ+2fZkr6nO19khZJmml7iqSQ9KGkxhugAeiIZts/PVvBWABUiCvogEQQdiARhB1IRFOz8egdc+bkP7Zw+vTpwm0XLVqUqxX1m9u4cWPrA0PbcWYHEkHYgUQQdiARhB1IBKvL9rnRo0fnah9//HHhtq+//nquVjTBh+7G6rJA4gg7kAjCDiSCsAOJIOxAIrhcts898sgjudrx48cLt12yZEnVw0EHcWYHEkHYgUQQdiARjbR/mmR7ve33bL9r+/tZfbzttbY/yL6zdjzQxRpp/zQoaTAi3rY9VtIWSd+U9F1JRyLiCdsLJY2LiB+M8FhcLluRO+64o7D+yiuv5GovvfRS4bb33HNPmUNCh7TS/ulARLyd3T4haYekKyXNlbQy22ylav8BAOhSF/We3fbVkr4maZOkCXVrxx+UNKHcoQEoU8N/Z7c9RtIaSQ9FxHH7/18pREQM9xLd9gJJC1odKIDWNHRmtz1KtaCvioifZ+VD2fv5c+/rDxftS/snoDs0MkFn1d6TH4mIh+rq/yjpf+om6MZHxN+P8FhM0FXksssuK6xv3rw5Vzt79mzhtrNnz87Vdu/e3drA0HbDTdA18jL+Zkl/I+k3trdmtR9KekLSatv3SfqdpLtKGCeAijTS/ulXkgr/p5A0q9zhAKgKV9ABiSDsQCIIO5AIPs/egwYGBnK1opVhJWny5Mm52ty5cwu3Zea9v3FmBxJB2IFEEHYgEYQdSATtn3rQtddem6vt3Lmz4f2LJvgkqZ2/C6gO7Z+AxBF2IBGEHUgEYQcSQdiBRHC5bBe76qqrCutvvPFGw49R1P6JWfc0cWYHEkHYgUQQdiARrbR/etz2fttbs6851Q8XQLNaaf90l6STEfFUw0/G5bKlWLp0aa726KOPNrz/tGnTCutDQ0NNjwndo+nVZbOuLwey2ydsn2v/BKCHtNL+SZIesP2O7RV0cQW6W8NhP7/9k6SfSPqCpCmqnfl/PMx+C2wP2eY1ItBBTbd/iohDEfFpRJyV9FNJhW8Eaf8EdIdGZuMt6VlJOyJiWV19sG6zb0naXv7wAJSllfZPd9ueIikkfSjp/grGl7Tp06cX1h988MGWHvfkyZMt7Y/e1Er7p1+UPxwAVeEKOiARhB1IBGEHEsHn2bvYjBkzCutjxoxp+DGKWjpdzEq06B+c2YFEEHYgEYQdSARhBxJB2IFEMBvfJ7Zt21ZYnzVrVptHgm7FmR1IBGEHEkHYgUQQdiARI64uW+qTsbosULnhVpflzA4kgrADiSDsQCIaWXDys7Y3296WtX9anNUn295ke5ftF21fUv1wATSrkTP7KUm3RcRXVVsjfrbtmyT9SNLTEfFFSUcl3VfZKAG0bMSwR8255UhHZV8h6TZJL2f1lar1fwPQpRptEjGQLSN9WNJaSbslHYuIM9km+0T/N6CrNRT2rPPLFEkTVev88qVGn4D2T0B3uKjZ+Ig4Jmm9pK9LutT2uU/NTZS0f5h9aP8EdIFGZuMvt31pdnu0pNsl7VAt9POyzeZLerWiMQIowYiXy9r+imoTcAOq/eewOiKW2P68pBckjZf0a0nfiYhTIzwWl8sCFRvuclmujQf6DNfGA4kj7EAiCDuQCMIOJIKwA4kg7EAiCDuQCMIOJIKwA4kg7EAiCDuQCMIOJIKwA4kg7EAiCDuQCMIOJIKwA4kg7EAiWmn/9Lzt/7a9NfuaUvloATTtMyNv8of2Tydtj5L0K9u/zH72SES8fIF9AXSJEcMetRUpi9o/AeghTbV/iohN2Y+W2n7H9tO2/7SqQQJoXVPtn2x/WdKjqrWBulG1teN/ULQv7Z+A7nDR68bb/gdJ/xsRT9XVZkp6OCK+McK+vPwHKtb0uvHDtH/aaXswq1m1ds3byxosgPI1Mhs/KGml7fr2T/9u+z9tXy7JkrZK+l51wwTQKto/AX2G9k9A4gg7kAjCDiSCsAOJIOxAIgg7kAjCDiSCsAOJIOxAIgg7kAjCDiSCsAOJIOxAIgg7kAjCDiSCsAOJIOxAIgg7kAjCDiSCsAOJaGR12TL9XtLvstufy+73G46r9/TTsV013A/aurrsHz2xPRQRUzvy5BXiuHpPPx9bPV7GA4kg7EAiOhn25R187ipxXL2nn4/tDzr2nh1Ae/EyHkhE28Nue7bt39reZXthu5+/TLZX2D5se3tdbbzttbY/yL6P6+QYm2F7ku31tt+z/a7t72f1nj4225+1vdn2tuy4Fmf1ybY3Zb+TL9q+pNNjrUJbw551gv1nSX8t6XpJd9u+vp1jKNnzkmafV1soaV1EXCNpXXa/15yR9HcRcb2kmyT9bfbv1OvHdkrSbRHxVUlTJM22fZOkH0l6OiK+KOmopPs6N8TqtPvMPk3SrojYExGnJb0gaW6bx1CaiNgg6ch55bmSVma3V6rWu76nRMSBiHg7u31C0g5JV6rHjy1qTmZ3R2VfIek2SS9n9Z47rka1O+xXStpbd39fVusnEyLiQHb7oKQJnRxMq2xfLelrkjapD47N9oDtrZIOS1orabekYxFxJtukH38nJTFBV6mo/amjZ//cYXuMpDWSHoqI4/U/69Vji4hPI2KKpImqvdL8UmdH1D7tDvt+SZPq7k/Mav3kkO1BScq+H+7weJpie5RqQV8VET/Pyn1xbJIUEcckrZf0dUmX2j73OZF+/J2U1P6wvyXpmmz28xJJ35b0WpvHULXXJM3Pbs+X9GoHx9IU25b0rKQdEbGs7kc9fWy2L7d9aXZ7tKTbVZuPWC9pXrZZzx1Xo9p+UY3tOZL+SdKApBURsbStAyiR7Z9Jmqnap6YOSVok6RVJqyX9uWqf8LsrIs6fxOtqtqdL+i9Jv5F0Niv/ULX37T17bLa/otoE3IBqJ7rVEbHE9udVmyweL+nXkr4TEac6N9JqcAUdkAgm6IBEEHYgEYQdSARhBxJB2IFEEHYgEYQdSARhBxLxf60CJGtWLXTgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = ds[0][0]\n",
    "print(img.dtype)\n",
    "print(img.shape)\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
