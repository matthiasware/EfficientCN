{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a71aef-4b9a-4da7-b97b-7a9a4660382a",
   "metadata": {},
   "source": [
    "### forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25baa0a-e3db-42d1-b5a7-6c3e01e5f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf724af-6082-4332-b50a-9be1d9663d16",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac82884-5db6-442f-abe8-095bee8b6073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash_func(x,eps=10e-21):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        x(b,n,d)\n",
    "    Output:\n",
    "        y = squash(x(b,n,d))\n",
    "    \"\"\"\n",
    "    \n",
    "    x_norm = torch.norm(x, dim=-1, keepdim=True)\n",
    "    y = (1 - 1/torch.exp(x_norm) + eps) * (x/x_norm + eps)\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "def margin_loss(u, y_true, lbd=0.5, m_plus=0.9, m_minus=0.1):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "        u      (b,n,d)  ... capsules with n equals the numbe of classes\n",
    "        y_true (b,n)    .... labels vector, categorical representation\n",
    "    OUT:\n",
    "        loss, scalar  \n",
    "    \"\"\"\n",
    "    u_norm = torch.norm(u, dim=2)\n",
    "    term_left  = F.relu(m_plus - u_norm)\n",
    "    term_right = F.relu(u_norm - m_minus)\n",
    "    #\n",
    "    loss = y_true * term_left + lbd * (1.0 - y_true) * term_right\n",
    "    loss = loss.sum(dim=1).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def margin_loss2(u, y_true, lbd=0.5, m_plus=0.9, m_minus=0.1):\n",
    "    \"\"\"\n",
    "    Input:  u      (b,n,d)  ... capsules with n equals the numbe of classes\n",
    "            y_true (b,n)    ... labels vector, categorical representation\n",
    "    Output:\n",
    "        loss, scalar  \n",
    "    \"\"\"\n",
    "    \n",
    "    u_norm = torch.norm(u, dim=-1)\n",
    "    p_true = torch.square(F.relu(m_plus - u_norm))     #square is the difference to margin_loss!\n",
    "    p_false = torch.square(F.relu(u_norm - m_minus))\n",
    "\n",
    "    loss = y_true * p_true + lbd * (1-y_true) * p_false\n",
    "    loss = loss.sum(dim=1).mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def max_norm_masking(u):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "        u (b, n d) ... capsules\n",
    "    OUT:\n",
    "        masked(u)  (b, n, d) where:\n",
    "        - normalise over dimension d of u\n",
    "        - keep largest vector in dimension n\n",
    "        - mask out everything else\n",
    "    \"\"\"\n",
    "    _, n_classes, _ = u.shape\n",
    "    u_norm = torch.norm(u,dim=2)\n",
    "    #mask = F.one_hot(torch.argmax(u_norm,1), num_classes=n_classes)\n",
    "    mask = torch.nn.functional.one_hot(torch.argmax(u_norm,1), num_classes=n_classes)\n",
    "    return torch.einsum('bnd,bn->bnd',u, mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff2059c-8705-4b8b-8f9d-b48d7194a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squash(nn.Module):\n",
    "    def __init__(self, eps=10e-21):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input:  x(b,n,d)\n",
    "        Output: squash(x(b,n,d))\n",
    "        \"\"\"       \n",
    "        return squash_func(x, self.eps)\n",
    "    \n",
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)\n",
    "\n",
    "\n",
    "class PrimaryCaps(nn.Module):\n",
    "    \"\"\"\n",
    "        Attributes\n",
    "        ----------\n",
    "        F: int depthwise conv number of features\n",
    "        K: int depthwise conv kernel dimension \n",
    "        N: int number of primary capsules\n",
    "        D: int primary capsules dimension (number of properties)\n",
    "        s: int depthwise conv strides\n",
    "    \"\"\"\n",
    "    def __init__(self, F, K, N, D, s=1):\n",
    "        super().__init__()\n",
    "        self.F = F\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "        self.D = D\n",
    "        self.s = s\n",
    "        #\n",
    "        self.dw_conv2d = nn.Conv2d(F, F, kernel_size=K, stride=s, groups=F, padding=\"valid\")\n",
    "        #\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        IN:  (B,C,H,W)\n",
    "        OUT: (B, N, D)\n",
    "        \n",
    "        therefore for x, we have the following constraints:\n",
    "            (B,C,H,W) = (B, F,F,K)\n",
    "        \"\"\"\n",
    "        # (B,C,H,W) -> (B,C,H,W)\n",
    "        x = self.dw_conv2d(x)\n",
    "\n",
    "        # (B,C,H,W) -> (B, N, D)\n",
    "        x = x.view((-1, self.N, self.D))\n",
    "        return x\n",
    "\n",
    "    \n",
    "class PrimaryCaps2(nn.Module):\n",
    "    \"\"\"\n",
    "        Create a primary capsule layer with the methodology described in 'Efficient-CapsNet: Capsule Network with Self-Attention Routing'. \n",
    "        Properties of each capsule s_n are exatracted using a 2D depthwise convolution.\n",
    "\n",
    "        ...\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        F: int depthwise conv number of features\n",
    "        K: int depthwise conv kernel dimension \n",
    "        N: int number of primary capsules\n",
    "        D: int primary capsules dimension (number of properties)\n",
    "        s: int depthwise conv strides\n",
    "    \"\"\"\n",
    "    def __init__(self, F, K, N, D, s=1):\n",
    "        super().__init__()\n",
    "        self.F = F\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "        self.D = D\n",
    "        self.s = s\n",
    "        #\n",
    "        self.dw_conv2d = nn.Conv2d(F, F, kernel_size=K, stride=s, groups=F, padding=\"valid\")\n",
    "        self.squash = Squash()\n",
    "        #\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "         X in (B,C,H,W) = (B,F,K,K)\n",
    "         -> (B, N, D)\n",
    "        \"\"\"\n",
    "        # (B,C,H,W) -> (B,C,H,W)\n",
    "        x = self.dw_conv2d(x)\n",
    "        # (B,C,H,W) -> (B, N, D)\n",
    "        x = x.view((-1, self.N, self.D))\n",
    "        x = self.squash(x)                                     #Difference to PrimaryCaps\n",
    "        #\n",
    "        return x    \n",
    "    \n",
    "\n",
    "class FCCaps(nn.Module):\n",
    "    \"\"\"\n",
    "        Attributes\n",
    "        ----------\n",
    "        n_l ... number of lower layer capsules\n",
    "        d_l ... dimension of lower layer capsules\n",
    "        n_h ... number of higher layer capsules\n",
    "        d_h ... dimension of higher layer capsules\n",
    "\n",
    "        W   (n_l, n_h, d_l, d_h) ... weight tensor\n",
    "        B   (n_l, n_h)           ... bias tensor\n",
    "    \"\"\"\n",
    "    def __init__(self, n_l, n_h, d_l, d_h):\n",
    "        super().__init__()\n",
    "        self.n_l = n_l\n",
    "        self.d_l = d_l\n",
    "        self.n_h = n_h\n",
    "        self.d_h = d_h\n",
    "\n",
    "        self.W = torch.nn.Parameter(torch.rand(n_l, n_h, d_l, d_h), requires_grad=True)\n",
    "        self.B = torch.nn.Parameter(torch.rand(n_l, n_h), requires_grad=True)\n",
    "        self.squash = Squash(eps=1e-20)\n",
    "\n",
    "        # init custom weights\n",
    "        # i'm relly unsure about this initialization scheme\n",
    "        # i don't think it makes sense in our case, but the paper says so ...\n",
    "        torch.nn.init.kaiming_normal_(self.W, a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        torch.nn.init.kaiming_normal_(self.B, a=0, mode=\"fan_in\", nonlinearity=\"leaky_relu\")\n",
    "\n",
    "\n",
    "        self.attention_scaling = np.sqrt(self.d_l)\n",
    "\n",
    "    def forward(self, U_l):\n",
    "        \"\"\"\n",
    "        einsum convenventions:\n",
    "          n_l = i | h\n",
    "          d_l = j\n",
    "          n_h = k\n",
    "          d_h = l\n",
    "        \n",
    "        Data tensors:\n",
    "            IN:  U_l ... lower layer capsules\n",
    "            OUT: U_h ... higher layer capsules\n",
    "            DIMS: \n",
    "                U_l (n_l, d_l)\n",
    "                U_h (n_h, d_h)\n",
    "                W   (n_l, n_h, d_l, d_h)\n",
    "                B   (n_l, n_h)\n",
    "                A   (n_l, n_l, n_h)\n",
    "                C   (n_l, n_h)\n",
    "        \"\"\"\n",
    "        U_hat = torch.einsum('...ij,ikjl->...ikl', U_l, self.W)\n",
    "        A = torch.einsum(\"...ikl, ...hkl -> ...hik\", U_hat, U_hat)\n",
    "        #A = A / torch.sqrt(torch.Tensor([self.d_l]))\n",
    "        A = A / self.attention_scaling\n",
    "        A_sum = torch.einsum(\"...hij->...hj\",A)\n",
    "        C = torch.softmax(A_sum,dim=-1)\n",
    "        CB = C + self.B\n",
    "        U_h = torch.einsum('...ikl,...ik->...kl', U_hat, CB)\n",
    "        return self.squash(U_h)\n",
    "    \n",
    "    \n",
    "class FCCaps2(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully-connected caps layer. It exploites the routing mechanism, explained in 'Efficient-CapsNet: Capsule Network with Self-Attention Routing', \n",
    "    to create a parent layer of capsules. \n",
    "    \n",
    "    nl: number of input capsuls          (nl...i)(nl...h)\n",
    "    dl: dimension of input capsuls       (dl...j)\n",
    "    nh: number of output capsuls         (nh...k)\n",
    "    dh: dimension of output capsuls      (dh...l)\n",
    "    b: batch size                        (...)\n",
    "\n",
    "    W: weigth tensor                     (W->ikjl)\n",
    "    B: bias matrix                       (B->1ik)\n",
    "    U_l: input capsuls matrix            (U->...ij)    \n",
    "    U_hat: weigthed input capsuls matrix (U_hat->...ikl)\n",
    "    A: covariance tensor                 (A->...hik)\n",
    "    C: couplimg coefficients             (C->...ik)\n",
    "    \n",
    "    input: nl, dl, nh, dh\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nl, nh,dl, dh):\n",
    "        super().__init__()\n",
    "        self.nl = nl\n",
    "        self.dl = dl\n",
    "        self.nh = nh\n",
    "        self.dh = dh\n",
    "\n",
    "        self.W = torch.nn.Parameter(torch.rand([self.nl,self.nh,self.dl,self.dh]), requires_grad=True)\n",
    "        self.B = torch.nn.Parameter(torch.rand([1,self.nl,self.nh]), requires_grad=True)                         #Difference in Dimension definition, but shouldnot be a problem\n",
    "        self.squash = Squash()                                                                                   #eps in function predefind\n",
    "\n",
    "        \n",
    "            # init custom weights -> not implemented\n",
    "        \n",
    "        \n",
    "    def forward(self, U_l):\n",
    "        \"\"\"\n",
    "        Data tensors:\n",
    "            Input:  U_l ... lower layer capsules\n",
    "            Ouput: U_h ... higher layer capsules\n",
    "        \"\"\"\n",
    "        U_hat = torch.einsum(\"...ij,ikjl->...ikl\",U_l,self.W)\n",
    "        A = torch.einsum(\"...hkl,...ikl->...hik\",U_hat, U_hat)\n",
    "        A = A / torch.sqrt(torch.Tensor([self.dl]))\n",
    "        A_hat = torch.einsum(\"...hik->...ik\",A)\n",
    "        C = torch.softmax(A_hat,dim=-1)\n",
    "        CB = C+self.B\n",
    "        U_h = torch.einsum(\"...ikl,...ik->...kl\",U_hat,CB)\n",
    "        U_h = self.squash(U_h)\n",
    "        return U_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4588606-babf-4f88-9a85-2989c1208f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.rand((1,128,9,9))\n",
    "print(x.size())\n",
    "S = PrimaryCaps(F=128, K=9, N=16, D=8)(x)\n",
    "print(S.size())\n",
    "\n",
    "F = FCCaps2(16,10,8,16)(S)\n",
    "\n",
    "print(F.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59413dce-59dc-4d1b-9363-1f25b50c27e6",
   "metadata": {},
   "source": [
    "#### mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde56e0-e4ee-4eef-bc05-4878f74eba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistEcnBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "        Backbone model from Efficient-CapsNet for MNIST\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(5, 5), padding=\"valid\"),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=\"valid\"),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=(3, 3), padding=\"valid\"),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=2, padding=\"valid\"),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            IN:\n",
    "                x (b, 1, 28, 28)\n",
    "            OUT:\n",
    "                x (b, 128, 9, 9)\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be833dc-90f5-467b-bf86-2c4ab2ff78b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1,1,28,28))\n",
    "print(x.size())\n",
    "A = MnistEcnBackbone()(x)\n",
    "print(A.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa70702d-55ac-46fb-a1c3-cb8921e0b454",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistEcnDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "        Decoder model from Efficient-CapsNet for MNIST\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(16*10, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 28*28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            IN:\n",
    "                x (b, n, d) with n=10 and d=16\n",
    "            OUT:\n",
    "                x_rec (b, 1, 28, 28)\n",
    "            Notes:\n",
    "                input must be masked!\n",
    "        \"\"\"\n",
    "        x = self.layers(x)\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c36c5d-1fb5-43ca-9f2d-a28fdc3b62fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1,160))\n",
    "print(x.size())\n",
    "A = MnistEcnDecoder()(x)\n",
    "print(A.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0be5b2-371b-43bb-a9ac-0a088bcf3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistEffCapsNet(nn.Module):\n",
    "    \"\"\"\n",
    "        EffCaps Implementation for MNIST\n",
    "        all parameters taken from the paper\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # values from paper, are fixed!\n",
    "        self.n_l = 16  # num of primary capsules\n",
    "        self.d_l = 8   # dim of primary capsules\n",
    "        self.n_h = 10  # num of output capsules\n",
    "        self.d_h = 16  # dim of output capsules\n",
    "        \n",
    "        self.backbone = MnistEcnBackbone()\n",
    "        self.primcaps = PrimaryCaps(F=128, K=9, N=self.n_l, D=self.d_l) # F = n_l * d_l !!!\n",
    "        self.fcncaps = FCCaps(self.n_l, self.n_h, self.d_l, self.d_h)\n",
    "        self.decoder = MnistEcnDecoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            IN:\n",
    "                x (b, 1, 28, 28)                       \n",
    "            OUT:\n",
    "                u_h    \n",
    "                    (b, n_h, d_h)\n",
    "                    output caps\n",
    "                x_rec  \n",
    "                    (b, 1, 28, 28)\n",
    "                    reconstruction of x\n",
    "        \"\"\"\n",
    "        u_l = self.primcaps(self.backbone(x))\n",
    "        u_h = self.fcncaps(u_l)\n",
    "        #\n",
    "        u_h_masked = max_norm_masking(u_h)\n",
    "        u_h_masked = torch.flatten(u_h_masked, start_dim=1)\n",
    "        x_rec = self.decoder(u_h_masked)\n",
    "        return u_h, x_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbefd86-1cc7-4201-b1b7-0a69ba489c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1,1,28,28))\n",
    "print(x.size())\n",
    "u_h, x_rec = MnistEffCapsNet()(x)\n",
    "print(u_h.size())\n",
    "print(x_rec.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dfea27-a484-4b67-a97a-d19fe025cfb4",
   "metadata": {},
   "source": [
    "#### multimnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85256a6-2207-47ab-b506-c86fe34bf57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiMnistEcnBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "        Backbone model from Efficient-CapsNet for MNIST\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(5, 5), padding=\"valid\"),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=\"valid\"),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=(3, 3), stride=2, padding=\"valid\"),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=2, padding=\"valid\"),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            IN:\n",
    "                x (b, 1, 36, 36)\n",
    "            OUT:\n",
    "                x (b, 128, 6, 6)\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374ac8eb-6362-4370-9831-fd8aa5409bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((10,1,36,36))\n",
    "print(x.size())\n",
    "A = MultiMnistEcnBackbone()(x)\n",
    "print(A.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb39e96-8da3-4881-abff-bd5d472d9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiMnistEcnDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "        Decoder model from Efficient-CapsNet for MNIST\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(16*10, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 36*36),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            IN:\n",
    "                x (b, n, d) with n=10 and d=16\n",
    "            OUT:\n",
    "                x_rec (b, 1, 36, 36)\n",
    "            Notes:\n",
    "                input must be masked!\n",
    "        \"\"\"\n",
    "        x = self.layers(x)\n",
    "        x = x.view(-1, 1, 36, 36)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcb6836-ec63-4fc3-a6f8-0df838c25f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1,160))\n",
    "print(x.size())\n",
    "A = MultiMnistEcnDecoder()(x)\n",
    "print(A.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc4fd57-ff83-424a-99b9-aff40c06b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiMnistEffCapsNet(nn.Module):\n",
    "    \"\"\"\n",
    "        EffCaps Implementation for MNIST\n",
    "        all parameters taken from the paper\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # values from paper, are fixed!\n",
    "        self.n_l = 16  # num of primary capsules\n",
    "        self.d_l = 8   # dim of primary capsules\n",
    "        self.n_h = 10  # num of output capsules\n",
    "        self.d_h = 16  # dim of output capsules\n",
    "        \n",
    "        self.backbone = MultiMnistEcnBackbone()\n",
    "        self.primcaps = PrimaryCaps(F=128, K=5, N=self.n_l, D=self.d_l, s=2) # F = n_l * d_l !!!\n",
    "        self.fcncaps = FCCaps2(self.n_l,  self.n_h, self.d_l, self.d_h)\n",
    "        self.decoder = MultiMnistEcnDecoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            IN:\n",
    "                x (b, 1, 36, 36)\n",
    "            OUT:\n",
    "                u_h    \n",
    "                    (b, n_h, d_h)\n",
    "                    output caps\n",
    "                x_rec  \n",
    "                    (b, 1, 36, 36)\n",
    "                    reconstruction of x\n",
    "        \"\"\"\n",
    "        u_l = self.backbone(x)\n",
    "        u_l = self.primcaps(u_l)\n",
    "        u_h = self.fcncaps(u_l)\n",
    "        #\n",
    "        u_h_masked = max_norm_masking(u_h)\n",
    "        u_h_masked = torch.flatten(u_h_masked, start_dim=1)\n",
    "        x_rec = self.decoder(u_h_masked)\n",
    "        return u_h, x_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2a8bf3-d992-4a6c-8be2-fcbbbc76d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.nn.Parameter(torch.rand((1,1,36,36)), requires_grad=True)\n",
    "print(x.size())\n",
    "u_h, x_rec = MultiMnistEffCapsNet()(x)\n",
    "print(x_rec.size())\n",
    "loss_rec = torch.nn.functional.mse_loss(x, x_rec)\n",
    "print(loss_rec)\n",
    "#print(A.size())\n",
    "\n",
    "print(x_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18028c07-639d-438a-83b3-c284c66b7be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = F.one_hot(torch.arange(0, 5) % 3)\n",
    "import torch.nn.functional as F    \n",
    "x = torch.nn.functional.one_hot(torch.arange(0, 5) % 3)\n",
    "y = F.one_hot(torch.arange(0, 5) % 3)\n",
    "print(x,\"\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c7e4e3-4e42-4b9d-84bd-ace2f5eabee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd26c16-7dc2-4b22-a8d7-bd15741c53bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true1 = tf.keras.layers.Input(shape=(10,))\n",
    "\n",
    "print(y_true1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4467d23d-05d0-487f-8700-b85fa07bf500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mask(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Mask operation described in 'Dynamic routinig between capsules'.\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    call(inputs, double_mask)\n",
    "        mask a capsule layer\n",
    "        set double_mask for multimnist dataset\n",
    "    \"\"\"\n",
    "    def call(self, inputs, double_mask=None, **kwargs):\n",
    "        if type(inputs) is list:\n",
    "            if double_mask:\n",
    "                inputs, mask1, mask2 = inputs\n",
    "            else:\n",
    "                inputs, mask = inputs\n",
    "        else:  \n",
    "            x = tf.sqrt(tf.reduce_sum(tf.square(inputs), -1))\n",
    "            if double_mask:\n",
    "                mask1 = tf.keras.backend.one_hot(tf.argsort(x,direction='DESCENDING',axis=-1)[...,0],num_classes=x.get_shape().as_list()[1])\n",
    "                mask2 = tf.keras.backend.one_hot(tf.argsort(x,direction='DESCENDING',axis=-1)[...,1],num_classes=x.get_shape().as_list()[1])\n",
    "            else:\n",
    "                mask = tf.keras.backend.one_hot(indices=tf.argmax(x, 1), num_classes=x.get_shape().as_list()[1])\n",
    "\n",
    "        if double_mask:\n",
    "            masked1 = tf.keras.backend.batch_flatten(inputs * tf.expand_dims(mask1, -1))\n",
    "            masked2 = tf.keras.backend.batch_flatten(inputs * tf.expand_dims(mask2, -1))\n",
    "            return masked1, masked2\n",
    "        else:\n",
    "            masked = tf.keras.backend.batch_flatten(inputs * tf.expand_dims(mask, -1))\n",
    "            return masked\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:  \n",
    "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
    "        else:  # generation step\n",
    "            return tuple([None, input_shape[1] * input_shape[2]])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Mask, self).get_config()\n",
    "        return config\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d15bac-2e63-4088-9a8b-06e6fa092c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input((1,10,16))\n",
    "y_true1 = tf.keras.layers.Input(shape=(10,))\n",
    "y_true2 = tf.keras.layers.Input(shape=(10,))\n",
    "\n",
    "masked_by_y1,masked_by_y2 = Mask()([inputs, y_true1, y_true2],double_mask=True)  \n",
    "masked1,masked2 = Mask()(inputs,double_mask=True)\n",
    "\n",
    "print(masked_by_y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff284b6-9db2-4660-8461-bffdbff83bee",
   "metadata": {},
   "source": [
    "### SmalNorb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9ffde-c76a-4f0b-90fe-e4ae36b29652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmalNorbEcnBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "        Backbone model from Efficient-CapsNet for SmalNorb\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, kernel_size=(7, 7),stride=2, padding=\"valid\"),\n",
    "            nn.LeakyReLU(0.3,inplace=True),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=\"valid\"),\n",
    "            nn.LeakyReLU(0.3,inplace=True),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=(3, 3), padding=\"valid\"),\n",
    "            nn.LeakyReLU(0.3,inplace=True),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=2, padding=\"valid\"),\n",
    "            nn.LeakyReLU(0.3,inplace=True),\n",
    "            nn.InstanceNorm2d(128),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            IN:\n",
    "                x (b, 2, 48, 48)\n",
    "            OUT:\n",
    "                x (b, 128, 8, 8)\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280a2f3-8e76-468d-a190-f6cc39446cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = torch.rand((1,2,48,48))\n",
    "x = torch.nn.Parameter(torch.rand((1,2,48,48)), requires_grad=True)\n",
    "print(x.size())\n",
    "A = SmalNorbEcnBackbone()(x)\n",
    "print(A.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d98fb8-ef47-403c-9596-4eeee90e8e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    x = np.random.rand(1,48,48,2)\n",
    "\n",
    "    print(x.shape)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32,7,2,activation=None, padding='valid', kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64,3, activation=None, padding='valid', kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64,3, activation=None, padding='valid', kernel_initializer='he_normal')(x) \n",
    "\n",
    "    x = tf.keras.layers.Conv2D(128,3,2, activation=None, padding='valid', kernel_initializer='he_normal')(x)   \n",
    "\n",
    "    print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797efeaa-cd82-4292-876f-c81cd3820d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmalNorbEcnDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "        Decoder model from Efficient-CapsNet for MNIST\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(16*5, 64),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            #nn.Linear(512, 1024),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            #nn.Linear(1024, 36*36),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "            nn.Conv2d(1, 64, kernel_size=(3, 3), padding=\"valid\"),\n",
    "            nn.LeakyReLU(0.3,inplace=True),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=\"valid\"),\n",
    "            nn.LeakyReLU(0.3,inplace=True),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, kernel_size=(3, 3), padding=\"valid\"),\n",
    "            nn.LeakyReLU(0.3,inplace=True),\n",
    "            nn.Conv2d(128, 2, kernel_size=(3, 3), padding=\"valid\"),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            IN:\n",
    "                x (b, n, d) with n=10 and d=16\n",
    "            OUT:\n",
    "                x_rec (b, 1, 36, 36)\n",
    "            Notes:\n",
    "                input must be masked!\n",
    "        \"\"\"\n",
    "        x = self.layer1(x)\n",
    "        x = x.view(-1, 1, 8, 8)\n",
    "        x = self.layer2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5b690-3dcd-46f5-b2f0-6fe4e5181e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1,80))\n",
    "print(x.size())\n",
    "A = SmalNorbEcnDecoder()(x)\n",
    "print(A.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c2bbe5-056a-4bff-aaba-aba90e522cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (2, 2, 2, 3)\n",
    "x = np.arange(np.prod(input_shape)).reshape(input_shape)\n",
    "print(x)\n",
    "y = tf.keras.layers.UpSampling2D(size=(1, 2))(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f17bd4f-55cb-46ca-af2d-846b886c383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    inputs = tf.keras.Input(16*5)\n",
    "\n",
    "    x = tf.keras.layers.Dense(64)(inputs)\n",
    "    x = tf.keras.layers.Reshape(target_shape=(8,8,1))(x)\n",
    "    x = tf.keras.layers.UpSampling2D(size=(2,2), interpolation='bilinear')(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"valid\", activation=tf.nn.leaky_relu)(x)\n",
    "    x = tf.keras.layers.UpSampling2D(size=(2,2), interpolation='bilinear')(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"valid\", activation=tf.nn.leaky_relu)(x)\n",
    "    x = tf.keras.layers.UpSampling2D(size=(2,2), interpolation='bilinear')(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"valid\", activation=tf.nn.leaky_relu)(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=2, kernel_size=(3,3), padding=\"valid\", activation=tf.nn.sigmoid)(x)  \n",
    "    \n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4809c763-c134-4c29-8d4e-f8afd0f794d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmalNorbEffCapsNet(nn.Module):\n",
    "    \"\"\"\n",
    "        EffCaps Implementation for SmalNorb\n",
    "        all parameters taken from the paper\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # values from paper, are fixed!\n",
    "        self.n_l = 16  # num of primary capsules\n",
    "        self.d_l = 8   # dim of primary capsules\n",
    "        self.n_h = 5   # num of output capsules\n",
    "        self.d_h = 16  # dim of output capsules\n",
    "        \n",
    "        self.backbone = SmalNorbEcnBackbone()\n",
    "        self.primcaps = PrimaryCaps(F=128, K=8, N=self.n_l, D=self.d_l, s=2) # F = n_l * d_l !!!\n",
    "        self.fcncaps = FCCaps2(self.n_l, self.n_h ,self.d_l, self.d_h)\n",
    "        self.decoder = SmalNorbEcnDecoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            IN:\n",
    "                x (b, 2, 48, 48)\n",
    "            OUT:\n",
    "                u_h    \n",
    "                    (b, n_h, d_h)\n",
    "                    output caps\n",
    "                x_rec  \n",
    "                    (b, 2, 48, 48)\n",
    "                    reconstruction of x\n",
    "        \"\"\"\n",
    "        u_l = self.backbone(x)\n",
    "        u_l = self.primcaps(u_l)\n",
    "        u_h = self.fcncaps(u_l)\n",
    "        #\n",
    "        u_h_masked = max_norm_masking(u_h)\n",
    "        u_h_masked = torch.flatten(u_h_masked, start_dim=1)\n",
    "        x_rec = self.decoder(u_h_masked)\n",
    "        return u_h, x_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9f23c8-8292-4257-ba37-3844d3ac9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.nn.Parameter(torch.rand((1,2,48,48)), requires_grad=True)\n",
    "print(x.size())\n",
    "#SmalNorbEffCapsNet()(x)\n",
    "u_h, x_rec = SmalNorbEffCapsNet()(x)\n",
    "print(x_rec.size())\n",
    "print(x_rec[0,1,:,:])\n",
    "loss_rec = torch.nn.functional.mse_loss(x, x_rec)\n",
    "print(loss_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b12ce6d-d2f8-4055-be5b-18eb60e10528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
